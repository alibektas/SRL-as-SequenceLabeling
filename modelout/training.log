2021-07-14 23:01:10,774 ----------------------------------------------------------------------------------------------------
2021-07-14 23:01:10,776 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): XLMRobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250002, 1024, padding_idx=1)
        (position_embeddings): Embedding(514, 1024, padding_idx=1)
        (token_type_embeddings): Embedding(1, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): RobertaPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=189, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-07-14 23:01:10,777 ----------------------------------------------------------------------------------------------------
2021-07-14 23:01:10,777 Corpus: "Corpus: 12543 train + 2002 dev + 2077 test sentences"
2021-07-14 23:01:10,777 ----------------------------------------------------------------------------------------------------
2021-07-14 23:01:10,777 Parameters:
2021-07-14 23:01:10,777  - learning_rate: "5e-06"
2021-07-14 23:01:10,777  - mini_batch_size: "4"
2021-07-14 23:01:10,778  - patience: "3"
2021-07-14 23:01:10,778  - anneal_factor: "0.5"
2021-07-14 23:01:10,778  - max_epochs: "20"
2021-07-14 23:01:10,778  - shuffle: "True"
2021-07-14 23:01:10,778  - train_with_dev: "False"
2021-07-14 23:01:10,778  - batch_growth_annealing: "False"
2021-07-14 23:01:10,778 ----------------------------------------------------------------------------------------------------
2021-07-14 23:01:10,778 Model training base path: "modelout"
2021-07-14 23:01:10,778 ----------------------------------------------------------------------------------------------------
2021-07-14 23:01:10,778 Device: cuda:1
2021-07-14 23:01:10,778 ----------------------------------------------------------------------------------------------------
2021-07-14 23:01:10,778 Embeddings storage mode: gpu
2021-07-14 23:01:10,821 ----------------------------------------------------------------------------------------------------
2021-07-14 23:02:58,178 epoch 1 - iter 313/3136 - loss 0.35864456 - samples/sec: 11.66 - lr: 0.000005
2021-07-14 23:04:37,615 epoch 1 - iter 626/3136 - loss 0.34186281 - samples/sec: 12.59 - lr: 0.000005
2021-07-14 23:06:16,534 epoch 1 - iter 939/3136 - loss 0.31745840 - samples/sec: 12.66 - lr: 0.000005
2021-07-14 23:07:56,157 epoch 1 - iter 1252/3136 - loss 0.31741286 - samples/sec: 12.57 - lr: 0.000005
2021-07-14 23:09:36,882 epoch 1 - iter 1565/3136 - loss 0.31381366 - samples/sec: 12.43 - lr: 0.000005
2021-07-14 23:11:20,012 epoch 1 - iter 1878/3136 - loss 0.31903781 - samples/sec: 12.14 - lr: 0.000005
2021-07-14 23:13:01,495 epoch 1 - iter 2191/3136 - loss 0.32449640 - samples/sec: 12.34 - lr: 0.000005
2021-07-14 23:14:43,534 epoch 1 - iter 2504/3136 - loss 0.32822081 - samples/sec: 12.27 - lr: 0.000005
2021-07-14 23:16:23,262 epoch 1 - iter 2817/3136 - loss 0.32933230 - samples/sec: 12.56 - lr: 0.000005
2021-07-14 23:18:03,669 epoch 1 - iter 3130/3136 - loss 0.33119387 - samples/sec: 12.47 - lr: 0.000005
2021-07-14 23:18:05,565 ----------------------------------------------------------------------------------------------------
2021-07-14 23:18:05,565 EPOCH 1 done: loss 0.3313 - lr 0.0000050
2021-07-14 23:19:08,180 DEV : loss 0.7160420417785645 - score 0.8789
2021-07-14 23:19:08,191 BAD EPOCHS (no improvement): 4
2021-07-14 23:19:08,194 ----------------------------------------------------------------------------------------------------
2021-07-14 23:20:48,902 epoch 2 - iter 313/3136 - loss 0.33703948 - samples/sec: 12.43 - lr: 0.000005
2021-07-14 23:22:30,622 epoch 2 - iter 626/3136 - loss 0.34306117 - samples/sec: 12.31 - lr: 0.000005
2021-07-14 23:24:12,014 epoch 2 - iter 939/3136 - loss 0.33955927 - samples/sec: 12.35 - lr: 0.000005
2021-07-14 23:25:53,297 epoch 2 - iter 1252/3136 - loss 0.32802067 - samples/sec: 12.36 - lr: 0.000005
2021-07-14 23:27:33,573 epoch 2 - iter 1565/3136 - loss 0.32716277 - samples/sec: 12.49 - lr: 0.000005
2021-07-14 23:29:15,400 epoch 2 - iter 1878/3136 - loss 0.33090008 - samples/sec: 12.30 - lr: 0.000005
2021-07-14 23:30:56,642 epoch 2 - iter 2191/3136 - loss 0.32845374 - samples/sec: 12.37 - lr: 0.000005
2021-07-14 23:32:38,892 epoch 2 - iter 2504/3136 - loss 0.32707741 - samples/sec: 12.25 - lr: 0.000005
2021-07-14 23:34:21,000 epoch 2 - iter 2817/3136 - loss 0.33524341 - samples/sec: 12.26 - lr: 0.000005
2021-07-14 23:36:01,881 epoch 2 - iter 3130/3136 - loss 0.33489947 - samples/sec: 12.41 - lr: 0.000005
2021-07-14 23:36:03,835 ----------------------------------------------------------------------------------------------------
2021-07-14 23:36:03,836 EPOCH 2 done: loss 0.3350 - lr 0.0000049
2021-07-14 23:37:07,738 DEV : loss 0.7159886956214905 - score 0.8789
2021-07-14 23:37:07,748 BAD EPOCHS (no improvement): 4
2021-07-14 23:37:07,754 ----------------------------------------------------------------------------------------------------
2021-07-14 23:38:48,685 epoch 3 - iter 313/3136 - loss 0.36166916 - samples/sec: 12.41 - lr: 0.000005
2021-07-14 23:40:29,865 epoch 3 - iter 626/3136 - loss 0.35378684 - samples/sec: 12.38 - lr: 0.000005
2021-07-14 23:42:10,327 epoch 3 - iter 939/3136 - loss 0.33515101 - samples/sec: 12.46 - lr: 0.000005
2021-07-14 23:43:51,318 epoch 3 - iter 1252/3136 - loss 0.34674476 - samples/sec: 12.40 - lr: 0.000005
2021-07-14 23:45:31,988 epoch 3 - iter 1565/3136 - loss 0.35019058 - samples/sec: 12.44 - lr: 0.000005
2021-07-14 23:47:12,986 epoch 3 - iter 1878/3136 - loss 0.34258091 - samples/sec: 12.40 - lr: 0.000005
2021-07-14 23:48:54,328 epoch 3 - iter 2191/3136 - loss 0.34230118 - samples/sec: 12.36 - lr: 0.000005
2021-07-14 23:50:36,177 epoch 3 - iter 2504/3136 - loss 0.34393456 - samples/sec: 12.29 - lr: 0.000005
2021-07-14 23:52:18,083 epoch 3 - iter 2817/3136 - loss 0.34087475 - samples/sec: 12.29 - lr: 0.000005
2021-07-14 23:53:59,220 epoch 3 - iter 3130/3136 - loss 0.33863148 - samples/sec: 12.38 - lr: 0.000005
2021-07-14 23:54:01,160 ----------------------------------------------------------------------------------------------------
2021-07-14 23:54:01,160 EPOCH 3 done: loss 0.3389 - lr 0.0000047
2021-07-14 23:55:04,166 DEV : loss 0.7158181071281433 - score 0.8788
2021-07-14 23:55:04,176 BAD EPOCHS (no improvement): 4
2021-07-14 23:55:04,202 ----------------------------------------------------------------------------------------------------
2021-07-14 23:56:44,982 epoch 4 - iter 313/3136 - loss 0.31691345 - samples/sec: 12.42 - lr: 0.000005
2021-07-14 23:58:26,225 epoch 4 - iter 626/3136 - loss 0.31306300 - samples/sec: 12.37 - lr: 0.000005
2021-07-15 00:00:08,157 epoch 4 - iter 939/3136 - loss 0.31649600 - samples/sec: 12.28 - lr: 0.000005
2021-07-15 00:01:48,648 epoch 4 - iter 1252/3136 - loss 0.32773849 - samples/sec: 12.46 - lr: 0.000005
2021-07-15 00:03:28,591 epoch 4 - iter 1565/3136 - loss 0.31909468 - samples/sec: 12.53 - lr: 0.000005
2021-07-15 00:05:08,787 epoch 4 - iter 1878/3136 - loss 0.32628051 - samples/sec: 12.50 - lr: 0.000005
2021-07-15 00:06:49,261 epoch 4 - iter 2191/3136 - loss 0.32736920 - samples/sec: 12.46 - lr: 0.000005
2021-07-15 00:08:29,959 epoch 4 - iter 2504/3136 - loss 0.32730151 - samples/sec: 12.43 - lr: 0.000005
2021-07-15 00:10:10,621 epoch 4 - iter 2817/3136 - loss 0.32850185 - samples/sec: 12.44 - lr: 0.000005
2021-07-15 00:11:51,279 epoch 4 - iter 3130/3136 - loss 0.32924635 - samples/sec: 12.44 - lr: 0.000005
2021-07-15 00:11:53,058 ----------------------------------------------------------------------------------------------------
2021-07-15 00:11:53,059 EPOCH 4 done: loss 0.3295 - lr 0.0000045
2021-07-15 00:12:56,654 DEV : loss 0.7152830958366394 - score 0.8789
2021-07-15 00:12:56,664 BAD EPOCHS (no improvement): 4
2021-07-15 00:12:56,668 ----------------------------------------------------------------------------------------------------
2021-07-15 00:13:36,437 ----------------------------------------------------------------------------------------------------
2021-07-15 00:13:36,437 Exiting from training early.
2021-07-15 00:13:36,437 Saving model ...
2021-07-15 00:14:21,329 Done.
2021-07-15 00:14:21,331 ----------------------------------------------------------------------------------------------------
2021-07-15 00:14:21,331 Testing using best model ...
2021-07-15 00:15:27,051 	0.8858
2021-07-15 00:15:27,051 
Results:
- F-score (micro): 0.8787
- F-score (macro): 0.4697
- Accuracy (incl. no class): 0.8858

By class:
              precision    recall  f1-score   support

          >>     0.8532    0.8464    0.8498       625
       >ARG1     0.8187    0.8110    0.8148       529
       >ARG0     0.9102    0.9219    0.9160       781
         V01     0.9427    0.9513    0.9470      3410
           >     0.9384    0.9379    0.9382      5412
       <ARG2     0.8189    0.7827    0.8004       543
       <ARG1     0.8882    0.8983    0.8932       964
          <<     0.6635    0.6116    0.6365       345
           <     0.8564    0.8491    0.8527      2902
       <ARG4     0.7818    0.8113    0.7963        53
  >>ARGM-DIS     0.7818    0.8600    0.8190        50
   >ARGM-EXT     0.7292    0.8140    0.7692        86
   >ARGM-TMP     0.8598    0.9338    0.8952       151
 >R-ARGM-TMP     0.5000    0.5000    0.5000         2
       >ARG2     0.6438    0.6620    0.6528        71
   >ARGM-MOD     0.9655    0.9825    0.9739       285
      >>ARG1     0.8533    0.8722    0.8626       360
      >>ARG0     0.9100    0.9143    0.9121       420
   >ARGM-ADV     0.8095    0.7628    0.7855       156
   <ARGM-TMP     0.8684    0.8800    0.8742       225
         V03     0.8964    0.9464    0.9207       448
   <ARGM-ADV     0.5077    0.5000    0.5038        66
   >ARGM-MNR     0.4821    0.4909    0.4865        55
         V02     0.8550    0.8635    0.8592       608
       <ARG3     0.6667    0.5652    0.6118        46
     >R-ARG0     0.9355    0.8788    0.9062        33
   >ARGM-NEG     0.9643    0.9818    0.9730       165
         V21     1.0000    0.0000    0.0000         3
         V12     0.3846    0.5556    0.4545         9
          V_     0.3333    0.2500    0.2857         4
         V10     0.7500    0.3333    0.4615         9
   >ARGM-ADJ     0.7423    0.7701    0.7559       187
       >ARG3     0.0000    0.0000    0.0000         7
         V04     0.8273    0.8750    0.8505       104
   >>XARG1-0     0.5510    0.7105    0.6207        38
  >>ARGM-ADV     0.7692    0.7547    0.7619        53
     <C-ARG1     0.8571    0.7059    0.7742        17
   <ARGM-LOC     0.6974    0.7910    0.7413       134
   <ARGM-GOL     0.5000    0.4000    0.4444        15
         VLV     0.7826    0.7826    0.7826        69
    >>R-ARG1     0.8000    0.9412    0.8649        17
  >>ARGM-LOC     0.5000    0.3333    0.4000        12
  >>ARGM-TMP     0.8261    0.8444    0.8352        45
      <<ARG1     0.5000    0.3333    0.4000        15
    >XARG1-0     0.7241    0.7778    0.7500        54
   >ARGM-LOC     0.5429    0.7308    0.6230        26
         V11     0.8462    0.6875    0.7586        16
       <ARG0     0.7164    0.7500    0.7328        64
    <XARG1-0     0.7213    0.8000    0.7586        55
    >>R-ARG0     0.8571    0.9000    0.8780        20
         V18     0.5000    0.5000    0.5000         2
   <ARGM-PRD     0.2500    0.1538    0.1905        13
   >ARGM-DIS     0.7901    0.7805    0.7853        82
   <ARGM-MNR     0.6200    0.7209    0.6667        43
   <ARGM-PRP     0.4762    0.6667    0.5556        15
         V09     0.8889    0.8000    0.8421        10
      <<ARG2     0.5714    0.4211    0.4848        19
      <<ARG0     1.0000    0.0000    0.0000         2
  <<ARGM-TMP     0.6296    0.8947    0.7391        19
 >R-ARGM-MNR     1.0000    0.0000    0.0000         8
     >R-ARG1     0.7308    0.8261    0.7755        23
  >>ARGM-MNR     0.8571    0.5000    0.6316        12
 >R-ARGM-LOC     0.3000    1.0000    0.4615         3
   <ARGM-EXT     0.4500    0.7500    0.5625        12
   <ARGM-ADJ     0.0000    0.0000    0.0000         8
   <ARGM-NEG     0.8333    0.8333    0.8333        12
>>R-ARGM-LOC     0.6000    0.7500    0.6667         4
  >>ARGM-MOD     0.9412    0.9505    0.9458       101
  <<ARGM-LOC     0.4286    0.2500    0.3158        12
   <ARGM-DIR     0.5366    0.5789    0.5570        38
  <<ARGM-ADV     0.3750    0.2727    0.3158        11
   >ARGM-PRP     1.0000    0.2000    0.3333         5
   <ARGM-CAU     0.5714    1.0000    0.7273         4
        <C-V     0.8125    0.8667    0.8387        15
   >ARGM-PRD     0.4545    0.5556    0.5000         9
         V07     0.6667    0.7500    0.7059        16
   >ARGM-CAU     0.4000    0.8000    0.5333         5
         V15     1.0000    0.4000    0.5714         5
  <<ARGM-COM     1.0000    0.0000    0.0000         2
   <ARGM-COM     0.5714    0.8000    0.6667        10
     <R-ARG1     1.0000    0.0000    0.0000         1
  <<ARGM-DIS     0.0000    0.0000    0.0000         5
         V06     0.6190    0.5417    0.5778        24
  >>ARGM-CAU     1.0000    0.6250    0.7692         8
      >>ARG2     0.5882    0.4348    0.5000        23
   >ARGM-GOL     1.0000    0.0000    0.0000         2
   >ARGM-CXN     0.5000    1.0000    0.6667         5
         V19     1.0000    0.6667    0.8000         3
  >>ARGM-NEG     0.8889    0.8889    0.8889        18
  >>ARGM-ADJ     1.0000    0.0000    0.0000         3
>>R-ARGM-ADJ     1.0000    0.0000    0.0000         1
         V08     1.0000    0.3333    0.5000        15
         V05     0.6667    0.5806    0.6207        31
  <<ARGM-MNR     1.0000    0.2000    0.3333         5
  >>ARGM-PRP     1.0000    0.0000    0.0000         1
    <<C-ARG1     1.0000    0.0000    0.0000         2
   <ARGM-DIS     0.5417    0.5652    0.5532        23
      >>ARG3     1.0000    0.0000    0.0000         2
     <C-ARG3     0.5000    0.5000    0.5000         2
     <C-ARG2     0.5455    0.8571    0.6667         7
         V25     1.0000    1.0000    1.0000         1
  <<ARGM-DIR     1.0000    0.0000    0.0000         5
  <<ARGM-EXT     1.0000    0.0000    0.0000         1
         V14     1.0000    1.0000    1.0000         4
       <ARG5     1.0000    0.0000    0.0000         1
         V30     1.0000    0.0000    0.0000         2
   <<XARG1-0     1.0000    0.0000    0.0000         1
  <<ARGM-PRP     1.0000    0.0000    0.0000         2
  >>ARGM-PRD     1.0000    0.0000    0.0000         1
    >>R-ARG2     1.0000    0.0000    0.0000         1
   <ARGM-MOD     1.0000    0.0000    0.0000         1
  <<ARGM-GOL     1.0000    0.0000    0.0000         1
>>R-ARGM-ADV     0.0000    0.0000    0.0000         1
  >>ARGM-DIR     1.0000    0.0000    0.0000         1
>>R-ARGM-DIR     1.0000    0.0000    0.0000         1
  >>ARGM-GOL     1.0000    0.0000    0.0000         1
  <<ARGM-ADJ     1.0000    0.0000    0.0000         1
         V24     1.0000    0.0000    0.0000         1
         V13     1.0000    0.0000    0.0000         2
         V22     1.0000    0.0000    0.0000         1
   >ARG1-DSP     1.0000    0.0000    0.0000         3
         V16     1.0000    0.0000    0.0000         2
       <<C-V     1.0000    0.0000    0.0000         1
 <C-ARGM-CXN     0.3333    1.0000    0.5000         1
   <ARGM-CXN     1.0000    0.5000    0.6667         6
         V28     1.0000    0.0000    0.0000         2
      <<ARG3     1.0000    0.0000    0.0000         1
 <C-ARGM-LOC     1.0000    1.0000    1.0000         1
       >ARGA     1.0000    0.0000    0.0000         2
       >ARG4     0.0000    1.0000    0.0000         0
         V17     1.0000    0.0000    0.0000         1
   >ARGM-DIR     1.0000    0.0000    0.0000         2
  >>ARGM-EXT     1.0000    0.0000    0.0000         2
    >>C-ARG0     1.0000    0.0000    0.0000         1

   micro avg     0.8786    0.8788    0.8787     20520
   macro avg     0.7714    0.4893    0.4697     20520
weighted avg     0.8793    0.8788    0.8766     20520

2021-07-15 00:15:27,051 ----------------------------------------------------------------------------------------------------
