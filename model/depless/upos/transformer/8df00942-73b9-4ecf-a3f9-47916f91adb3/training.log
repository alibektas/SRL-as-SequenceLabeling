2021-10-13 23:00:19,006 ----------------------------------------------------------------------------------------------------
2021-10-13 23:00:19,009 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 1024, padding_idx=0)
        (position_embeddings): Embedding(512, 1024)
        (token_type_embeddings): Embedding(2, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=464, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-10-13 23:00:19,013 ----------------------------------------------------------------------------------------------------
2021-10-13 23:00:19,013 Corpus: "Corpus: 12543 train + 2002 dev + 2077 test sentences"
2021-10-13 23:00:19,013 ----------------------------------------------------------------------------------------------------
2021-10-13 23:00:19,013 Parameters:
2021-10-13 23:00:19,013  - learning_rate: "5e-06"
2021-10-13 23:00:19,013  - mini_batch_size: "4"
2021-10-13 23:00:19,013  - patience: "3"
2021-10-13 23:00:19,013  - anneal_factor: "0.5"
2021-10-13 23:00:19,013  - max_epochs: "20"
2021-10-13 23:00:19,013  - shuffle: "True"
2021-10-13 23:00:19,013  - train_with_dev: "False"
2021-10-13 23:00:19,013  - batch_growth_annealing: "False"
2021-10-13 23:00:19,013 ----------------------------------------------------------------------------------------------------
2021-10-13 23:00:19,026 Model training base path: "model/depless/upos/transformer/8df00942-73b9-4ecf-a3f9-47916f91adb3"
2021-10-13 23:00:19,026 ----------------------------------------------------------------------------------------------------
2021-10-13 23:00:19,026 Device: cuda:1
2021-10-13 23:00:19,026 ----------------------------------------------------------------------------------------------------
2021-10-13 23:00:19,026 Embeddings storage mode: gpu
2021-10-13 23:00:19,047 ----------------------------------------------------------------------------------------------------
2021-10-13 23:02:14,930 epoch 1 - iter 313/3136 - loss 4.51265911 - samples/sec: 10.80 - lr: 0.000005
2021-10-13 23:04:02,622 epoch 1 - iter 626/3136 - loss 3.95742883 - samples/sec: 11.63 - lr: 0.000005
2021-10-13 23:05:51,339 epoch 1 - iter 939/3136 - loss 3.51186728 - samples/sec: 11.52 - lr: 0.000005
2021-10-13 23:07:39,385 epoch 1 - iter 1252/3136 - loss 3.21248025 - samples/sec: 11.59 - lr: 0.000005
2021-10-13 23:09:28,666 epoch 1 - iter 1565/3136 - loss 3.03176625 - samples/sec: 11.46 - lr: 0.000005
2021-10-13 23:11:20,261 epoch 1 - iter 1878/3136 - loss 2.93645558 - samples/sec: 11.22 - lr: 0.000005
2021-10-13 23:13:08,237 epoch 1 - iter 2191/3136 - loss 2.82434211 - samples/sec: 11.60 - lr: 0.000005
2021-10-13 23:14:55,904 epoch 1 - iter 2504/3136 - loss 2.71689502 - samples/sec: 11.63 - lr: 0.000005
2021-10-13 23:16:44,008 epoch 1 - iter 2817/3136 - loss 2.61988912 - samples/sec: 11.58 - lr: 0.000005
2021-10-13 23:18:31,280 epoch 1 - iter 3130/3136 - loss 2.54227569 - samples/sec: 11.67 - lr: 0.000005
2021-10-13 23:18:33,259 ----------------------------------------------------------------------------------------------------
2021-10-13 23:18:33,259 EPOCH 1 done: loss 2.5413 - lr 0.0000050
2021-10-13 23:19:38,947 DEV : loss 1.5210949182510376 - score 0.6554
2021-10-13 23:19:38,958 BAD EPOCHS (no improvement): 4
2021-10-13 23:19:38,963 ----------------------------------------------------------------------------------------------------
2021-10-13 23:21:27,284 epoch 2 - iter 313/3136 - loss 1.74211939 - samples/sec: 11.56 - lr: 0.000005
2021-10-13 23:23:15,592 epoch 2 - iter 626/3136 - loss 1.65943799 - samples/sec: 11.56 - lr: 0.000005
2021-10-13 23:25:04,028 epoch 2 - iter 939/3136 - loss 1.62227104 - samples/sec: 11.55 - lr: 0.000005
2021-10-13 23:26:53,096 epoch 2 - iter 1252/3136 - loss 1.62238667 - samples/sec: 11.48 - lr: 0.000005
2021-10-13 23:28:41,315 epoch 2 - iter 1565/3136 - loss 1.61391974 - samples/sec: 11.57 - lr: 0.000005
2021-10-13 23:30:30,125 epoch 2 - iter 1878/3136 - loss 1.58147006 - samples/sec: 11.51 - lr: 0.000005
2021-10-13 23:32:20,229 epoch 2 - iter 2191/3136 - loss 1.55929500 - samples/sec: 11.37 - lr: 0.000005
2021-10-13 23:34:08,170 epoch 2 - iter 2504/3136 - loss 1.55377982 - samples/sec: 11.60 - lr: 0.000005
2021-10-13 23:35:57,104 epoch 2 - iter 2817/3136 - loss 1.53457479 - samples/sec: 11.49 - lr: 0.000005
2021-10-13 23:37:46,274 epoch 2 - iter 3130/3136 - loss 1.51797298 - samples/sec: 11.47 - lr: 0.000005
2021-10-13 23:37:48,400 ----------------------------------------------------------------------------------------------------
2021-10-13 23:37:48,400 EPOCH 2 done: loss 1.5179 - lr 0.0000049
2021-10-13 23:38:53,383 DEV : loss 1.1563252210617065 - score 0.7175
2021-10-13 23:38:53,393 BAD EPOCHS (no improvement): 4
2021-10-13 23:38:53,395 ----------------------------------------------------------------------------------------------------
2021-10-13 23:40:41,075 epoch 3 - iter 313/3136 - loss 1.29959335 - samples/sec: 11.63 - lr: 0.000005
2021-10-13 23:42:28,901 epoch 3 - iter 626/3136 - loss 1.28207820 - samples/sec: 11.61 - lr: 0.000005
2021-10-13 23:44:17,119 epoch 3 - iter 939/3136 - loss 1.26808472 - samples/sec: 11.57 - lr: 0.000005
2021-10-13 23:46:05,511 epoch 3 - iter 1252/3136 - loss 1.25482679 - samples/sec: 11.55 - lr: 0.000005
2021-10-13 23:47:54,813 epoch 3 - iter 1565/3136 - loss 1.25254363 - samples/sec: 11.46 - lr: 0.000005
2021-10-13 23:49:42,245 epoch 3 - iter 1878/3136 - loss 1.24673238 - samples/sec: 11.65 - lr: 0.000005
2021-10-13 23:51:34,707 epoch 3 - iter 2191/3136 - loss 1.23915715 - samples/sec: 11.13 - lr: 0.000005
2021-10-13 23:53:34,810 epoch 3 - iter 2504/3136 - loss 1.23196959 - samples/sec: 10.43 - lr: 0.000005
2021-10-13 23:55:33,566 epoch 3 - iter 2817/3136 - loss 1.21846175 - samples/sec: 10.54 - lr: 0.000005
2021-10-13 23:57:29,112 epoch 3 - iter 3130/3136 - loss 1.22545436 - samples/sec: 10.84 - lr: 0.000005
2021-10-13 23:57:31,169 ----------------------------------------------------------------------------------------------------
2021-10-13 23:57:31,170 EPOCH 3 done: loss 1.2250 - lr 0.0000047
2021-10-13 23:58:36,554 DEV : loss 1.0173779726028442 - score 0.7545
2021-10-13 23:58:36,563 BAD EPOCHS (no improvement): 4
2021-10-13 23:58:36,569 ----------------------------------------------------------------------------------------------------
2021-10-14 00:00:26,823 epoch 4 - iter 313/3136 - loss 1.02388562 - samples/sec: 11.36 - lr: 0.000005
2021-10-14 00:02:14,690 epoch 4 - iter 626/3136 - loss 1.07327634 - samples/sec: 11.61 - lr: 0.000005
2021-10-14 00:04:02,903 epoch 4 - iter 939/3136 - loss 1.06266141 - samples/sec: 11.57 - lr: 0.000005
2021-10-14 00:05:51,645 epoch 4 - iter 1252/3136 - loss 1.05422674 - samples/sec: 11.51 - lr: 0.000005
2021-10-14 00:07:40,462 epoch 4 - iter 1565/3136 - loss 1.05513744 - samples/sec: 11.51 - lr: 0.000005
2021-10-14 00:09:28,438 epoch 4 - iter 1878/3136 - loss 1.05450856 - samples/sec: 11.60 - lr: 0.000005
2021-10-14 00:11:15,928 epoch 4 - iter 2191/3136 - loss 1.04191078 - samples/sec: 11.65 - lr: 0.000005
2021-10-14 00:13:04,888 epoch 4 - iter 2504/3136 - loss 1.04451701 - samples/sec: 11.49 - lr: 0.000005
2021-10-14 00:14:52,817 epoch 4 - iter 2817/3136 - loss 1.04253501 - samples/sec: 11.60 - lr: 0.000005
2021-10-14 00:16:40,398 epoch 4 - iter 3130/3136 - loss 1.04184425 - samples/sec: 11.64 - lr: 0.000005
2021-10-14 00:16:42,371 ----------------------------------------------------------------------------------------------------
2021-10-14 00:16:42,372 EPOCH 4 done: loss 1.0412 - lr 0.0000045
2021-10-14 00:17:47,585 DEV : loss 0.9599972367286682 - score 0.7733
2021-10-14 00:17:47,593 BAD EPOCHS (no improvement): 4
2021-10-14 00:17:47,596 ----------------------------------------------------------------------------------------------------
2021-10-14 00:19:37,879 epoch 5 - iter 313/3136 - loss 1.05158970 - samples/sec: 11.35 - lr: 0.000004
2021-10-14 00:21:25,770 epoch 5 - iter 626/3136 - loss 1.01263906 - samples/sec: 11.61 - lr: 0.000004
2021-10-14 00:23:13,928 epoch 5 - iter 939/3136 - loss 1.02250930 - samples/sec: 11.58 - lr: 0.000004
2021-10-14 00:25:01,446 epoch 5 - iter 1252/3136 - loss 1.00061723 - samples/sec: 11.65 - lr: 0.000004
2021-10-14 00:26:49,493 epoch 5 - iter 1565/3136 - loss 0.99299370 - samples/sec: 11.59 - lr: 0.000004
2021-10-14 00:28:37,443 epoch 5 - iter 1878/3136 - loss 0.98576995 - samples/sec: 11.60 - lr: 0.000004
2021-10-14 00:30:25,906 epoch 5 - iter 2191/3136 - loss 0.96941735 - samples/sec: 11.54 - lr: 0.000004
2021-10-14 00:32:13,905 epoch 5 - iter 2504/3136 - loss 0.97047967 - samples/sec: 11.59 - lr: 0.000004
2021-10-14 00:34:02,942 epoch 5 - iter 2817/3136 - loss 0.96609080 - samples/sec: 11.48 - lr: 0.000004
2021-10-14 00:35:52,025 epoch 5 - iter 3130/3136 - loss 0.96023708 - samples/sec: 11.48 - lr: 0.000004
2021-10-14 00:35:53,858 ----------------------------------------------------------------------------------------------------
2021-10-14 00:35:53,858 EPOCH 5 done: loss 0.9596 - lr 0.0000043
2021-10-14 00:36:58,805 DEV : loss 0.9001528024673462 - score 0.7879
2021-10-14 00:36:58,814 BAD EPOCHS (no improvement): 4
2021-10-14 00:36:58,820 ----------------------------------------------------------------------------------------------------
2021-10-14 00:38:47,083 epoch 6 - iter 313/3136 - loss 0.82495999 - samples/sec: 11.57 - lr: 0.000004
2021-10-14 00:40:37,085 epoch 6 - iter 626/3136 - loss 0.81839443 - samples/sec: 11.38 - lr: 0.000004
2021-10-14 00:42:24,565 epoch 6 - iter 939/3136 - loss 0.82330052 - samples/sec: 11.65 - lr: 0.000004
2021-10-14 00:44:12,757 epoch 6 - iter 1252/3136 - loss 0.80954249 - samples/sec: 11.57 - lr: 0.000004
2021-10-14 00:45:59,958 epoch 6 - iter 1565/3136 - loss 0.80960925 - samples/sec: 11.68 - lr: 0.000004
2021-10-14 00:47:47,990 epoch 6 - iter 1878/3136 - loss 0.81762464 - samples/sec: 11.59 - lr: 0.000004
2021-10-14 00:49:36,459 epoch 6 - iter 2191/3136 - loss 0.82895157 - samples/sec: 11.54 - lr: 0.000004
2021-10-14 00:51:25,014 epoch 6 - iter 2504/3136 - loss 0.82917634 - samples/sec: 11.53 - lr: 0.000004
2021-10-14 00:53:13,963 epoch 6 - iter 2817/3136 - loss 0.82287302 - samples/sec: 11.49 - lr: 0.000004
2021-10-14 00:55:02,001 epoch 6 - iter 3130/3136 - loss 0.82240950 - samples/sec: 11.59 - lr: 0.000004
2021-10-14 00:55:03,948 ----------------------------------------------------------------------------------------------------
2021-10-14 00:55:03,949 EPOCH 6 done: loss 0.8226 - lr 0.0000040
2021-10-14 00:56:08,863 DEV : loss 0.914989173412323 - score 0.7978
2021-10-14 00:56:08,872 BAD EPOCHS (no improvement): 4
2021-10-14 00:56:08,875 ----------------------------------------------------------------------------------------------------
2021-10-14 00:57:56,793 epoch 7 - iter 313/3136 - loss 0.78621433 - samples/sec: 11.60 - lr: 0.000004
2021-10-14 00:59:45,485 epoch 7 - iter 626/3136 - loss 0.78674556 - samples/sec: 11.52 - lr: 0.000004
2021-10-14 01:01:32,662 epoch 7 - iter 939/3136 - loss 0.77864913 - samples/sec: 11.68 - lr: 0.000004
2021-10-14 01:03:20,786 epoch 7 - iter 1252/3136 - loss 0.74996004 - samples/sec: 11.58 - lr: 0.000004
2021-10-14 01:05:07,729 epoch 7 - iter 1565/3136 - loss 0.74378219 - samples/sec: 11.71 - lr: 0.000004
2021-10-14 01:06:56,048 epoch 7 - iter 1878/3136 - loss 0.73789632 - samples/sec: 11.56 - lr: 0.000004
2021-10-14 01:08:44,604 epoch 7 - iter 2191/3136 - loss 0.73509493 - samples/sec: 11.53 - lr: 0.000004
2021-10-14 01:10:33,361 epoch 7 - iter 2504/3136 - loss 0.73256443 - samples/sec: 11.51 - lr: 0.000004
2021-10-14 01:12:21,650 epoch 7 - iter 2817/3136 - loss 0.72993432 - samples/sec: 11.56 - lr: 0.000004
2021-10-14 01:14:09,786 epoch 7 - iter 3130/3136 - loss 0.73659843 - samples/sec: 11.58 - lr: 0.000004
2021-10-14 01:14:11,767 ----------------------------------------------------------------------------------------------------
2021-10-14 01:14:11,768 EPOCH 7 done: loss 0.7368 - lr 0.0000036
2021-10-14 01:15:16,661 DEV : loss 0.9202439785003662 - score 0.8044
2021-10-14 01:15:16,669 BAD EPOCHS (no improvement): 4
2021-10-14 01:15:16,673 ----------------------------------------------------------------------------------------------------
2021-10-14 01:17:05,621 epoch 8 - iter 313/3136 - loss 0.72130369 - samples/sec: 11.49 - lr: 0.000004
2021-10-14 01:18:54,916 epoch 8 - iter 626/3136 - loss 0.67095347 - samples/sec: 11.46 - lr: 0.000004
2021-10-14 01:20:43,625 epoch 8 - iter 939/3136 - loss 0.68155504 - samples/sec: 11.52 - lr: 0.000004
2021-10-14 01:22:35,967 epoch 8 - iter 1252/3136 - loss 0.67596186 - samples/sec: 11.15 - lr: 0.000003
2021-10-14 01:24:24,091 epoch 8 - iter 1565/3136 - loss 0.68396486 - samples/sec: 11.58 - lr: 0.000003
2021-10-14 01:26:11,249 epoch 8 - iter 1878/3136 - loss 0.69428243 - samples/sec: 11.68 - lr: 0.000003
2021-10-14 01:27:59,625 epoch 8 - iter 2191/3136 - loss 0.69539579 - samples/sec: 11.55 - lr: 0.000003
2021-10-14 01:29:47,263 epoch 8 - iter 2504/3136 - loss 0.69628679 - samples/sec: 11.63 - lr: 0.000003
2021-10-14 01:31:35,334 epoch 8 - iter 2817/3136 - loss 0.69149653 - samples/sec: 11.59 - lr: 0.000003
2021-10-14 01:33:24,380 epoch 8 - iter 3130/3136 - loss 0.68887565 - samples/sec: 11.48 - lr: 0.000003
2021-10-14 01:33:26,331 ----------------------------------------------------------------------------------------------------
2021-10-14 01:33:26,331 EPOCH 8 done: loss 0.6915 - lr 0.0000033
2021-10-14 01:34:31,558 DEV : loss 0.9291534423828125 - score 0.81
2021-10-14 01:34:31,569 BAD EPOCHS (no improvement): 4
2021-10-14 01:34:31,573 ----------------------------------------------------------------------------------------------------
2021-10-14 01:36:30,908 epoch 9 - iter 313/3136 - loss 0.61435587 - samples/sec: 10.49 - lr: 0.000003
2021-10-14 01:38:21,700 epoch 9 - iter 626/3136 - loss 0.65206302 - samples/sec: 11.30 - lr: 0.000003
2021-10-14 01:40:09,249 epoch 9 - iter 939/3136 - loss 0.64809182 - samples/sec: 11.64 - lr: 0.000003
2021-10-14 01:41:57,545 epoch 9 - iter 1252/3136 - loss 0.65567368 - samples/sec: 11.56 - lr: 0.000003
2021-10-14 01:43:45,478 epoch 9 - iter 1565/3136 - loss 0.65388925 - samples/sec: 11.60 - lr: 0.000003
2021-10-14 01:45:36,489 epoch 9 - iter 1878/3136 - loss 0.65107218 - samples/sec: 11.28 - lr: 0.000003
2021-10-14 01:47:25,070 epoch 9 - iter 2191/3136 - loss 0.65551870 - samples/sec: 11.53 - lr: 0.000003
2021-10-14 01:49:13,204 epoch 9 - iter 2504/3136 - loss 0.65529887 - samples/sec: 11.58 - lr: 0.000003
2021-10-14 01:51:01,121 epoch 9 - iter 2817/3136 - loss 0.65672409 - samples/sec: 11.60 - lr: 0.000003
2021-10-14 01:52:58,990 epoch 9 - iter 3130/3136 - loss 0.65258487 - samples/sec: 10.62 - lr: 0.000003
2021-10-14 01:53:01,265 ----------------------------------------------------------------------------------------------------
2021-10-14 01:53:01,265 EPOCH 9 done: loss 0.6533 - lr 0.0000029
2021-10-14 01:54:17,304 DEV : loss 0.9267628788948059 - score 0.8156
2021-10-14 01:54:17,313 BAD EPOCHS (no improvement): 4
2021-10-14 01:54:17,316 ----------------------------------------------------------------------------------------------------
2021-10-14 01:56:05,982 epoch 10 - iter 313/3136 - loss 0.60474562 - samples/sec: 11.52 - lr: 0.000003
2021-10-14 01:57:54,568 epoch 10 - iter 626/3136 - loss 0.60879175 - samples/sec: 11.53 - lr: 0.000003
2021-10-14 01:59:42,314 epoch 10 - iter 939/3136 - loss 0.61453411 - samples/sec: 11.62 - lr: 0.000003
2021-10-14 02:01:30,550 epoch 10 - iter 1252/3136 - loss 0.62157892 - samples/sec: 11.57 - lr: 0.000003
2021-10-14 02:03:18,194 epoch 10 - iter 1565/3136 - loss 0.63846826 - samples/sec: 11.63 - lr: 0.000003
2021-10-14 02:05:06,284 epoch 10 - iter 1878/3136 - loss 0.63272979 - samples/sec: 11.58 - lr: 0.000003
2021-10-14 02:06:54,061 epoch 10 - iter 2191/3136 - loss 0.63123183 - samples/sec: 11.62 - lr: 0.000003
2021-10-14 02:08:41,936 epoch 10 - iter 2504/3136 - loss 0.62693580 - samples/sec: 11.61 - lr: 0.000003
2021-10-14 02:10:30,289 epoch 10 - iter 2817/3136 - loss 0.62784605 - samples/sec: 11.56 - lr: 0.000003
2021-10-14 02:12:21,300 epoch 10 - iter 3130/3136 - loss 0.62688143 - samples/sec: 11.28 - lr: 0.000003
2021-10-14 02:12:23,348 ----------------------------------------------------------------------------------------------------
2021-10-14 02:12:23,348 EPOCH 10 done: loss 0.6266 - lr 0.0000025
2021-10-14 02:13:28,183 DEV : loss 0.9407778978347778 - score 0.816
2021-10-14 02:13:28,191 BAD EPOCHS (no improvement): 4
2021-10-14 02:13:28,194 ----------------------------------------------------------------------------------------------------
2021-10-14 02:15:16,140 epoch 11 - iter 313/3136 - loss 0.61487723 - samples/sec: 11.60 - lr: 0.000002
2021-10-14 02:17:04,201 epoch 11 - iter 626/3136 - loss 0.59176353 - samples/sec: 11.59 - lr: 0.000002
2021-10-14 02:18:52,822 epoch 11 - iter 939/3136 - loss 0.56763287 - samples/sec: 11.53 - lr: 0.000002
2021-10-14 02:20:40,561 epoch 11 - iter 1252/3136 - loss 0.57852651 - samples/sec: 11.62 - lr: 0.000002
2021-10-14 02:22:29,814 epoch 11 - iter 1565/3136 - loss 0.57733270 - samples/sec: 11.46 - lr: 0.000002
2021-10-14 02:24:18,604 epoch 11 - iter 1878/3136 - loss 0.57637946 - samples/sec: 11.51 - lr: 0.000002
2021-10-14 02:26:07,216 epoch 11 - iter 2191/3136 - loss 0.57434618 - samples/sec: 11.53 - lr: 0.000002
2021-10-14 02:27:55,832 epoch 11 - iter 2504/3136 - loss 0.57471589 - samples/sec: 11.53 - lr: 0.000002
2021-10-14 02:29:52,300 epoch 11 - iter 2817/3136 - loss 0.57594974 - samples/sec: 10.75 - lr: 0.000002
2021-10-14 02:31:41,069 epoch 11 - iter 3130/3136 - loss 0.57588900 - samples/sec: 11.51 - lr: 0.000002
2021-10-14 02:31:43,187 ----------------------------------------------------------------------------------------------------
2021-10-14 02:31:43,187 EPOCH 11 done: loss 0.5760 - lr 0.0000021
2021-10-14 02:32:48,497 DEV : loss 0.9542363286018372 - score 0.8177
2021-10-14 02:32:48,505 BAD EPOCHS (no improvement): 4
2021-10-14 02:32:48,511 ----------------------------------------------------------------------------------------------------
2021-10-14 02:34:36,971 epoch 12 - iter 313/3136 - loss 0.56698182 - samples/sec: 11.54 - lr: 0.000002
2021-10-14 02:36:25,147 epoch 12 - iter 626/3136 - loss 0.60082181 - samples/sec: 11.57 - lr: 0.000002
2021-10-14 02:38:13,921 epoch 12 - iter 939/3136 - loss 0.58725972 - samples/sec: 11.51 - lr: 0.000002
2021-10-14 02:40:00,730 epoch 12 - iter 1252/3136 - loss 0.57827036 - samples/sec: 11.72 - lr: 0.000002
2021-10-14 02:41:52,119 epoch 12 - iter 1565/3136 - loss 0.57970300 - samples/sec: 11.24 - lr: 0.000002
2021-10-14 02:43:50,752 epoch 12 - iter 1878/3136 - loss 0.57617542 - samples/sec: 10.55 - lr: 0.000002
2021-10-14 02:45:41,054 epoch 12 - iter 2191/3136 - loss 0.57482794 - samples/sec: 11.35 - lr: 0.000002
2021-10-14 02:47:29,794 epoch 12 - iter 2504/3136 - loss 0.57978749 - samples/sec: 11.51 - lr: 0.000002
2021-10-14 02:49:17,567 epoch 12 - iter 2817/3136 - loss 0.57432559 - samples/sec: 11.62 - lr: 0.000002
2021-10-14 02:51:06,696 epoch 12 - iter 3130/3136 - loss 0.57510715 - samples/sec: 11.47 - lr: 0.000002
2021-10-14 02:51:08,696 ----------------------------------------------------------------------------------------------------
2021-10-14 02:51:08,697 EPOCH 12 done: loss 0.5756 - lr 0.0000017
2021-10-14 02:52:13,595 DEV : loss 0.9708715081214905 - score 0.8184
2021-10-14 02:52:13,604 BAD EPOCHS (no improvement): 4
2021-10-14 02:52:13,606 ----------------------------------------------------------------------------------------------------
2021-10-14 02:54:02,096 epoch 13 - iter 313/3136 - loss 0.56414318 - samples/sec: 11.54 - lr: 0.000002
2021-10-14 02:55:50,986 epoch 13 - iter 626/3136 - loss 0.56681160 - samples/sec: 11.50 - lr: 0.000002
2021-10-14 02:57:40,911 epoch 13 - iter 939/3136 - loss 0.55550171 - samples/sec: 11.39 - lr: 0.000002
2021-10-14 02:59:32,412 epoch 13 - iter 1252/3136 - loss 0.55402303 - samples/sec: 11.23 - lr: 0.000002
2021-10-14 03:01:20,457 epoch 13 - iter 1565/3136 - loss 0.55332869 - samples/sec: 11.59 - lr: 0.000002
2021-10-14 03:03:08,759 epoch 13 - iter 1878/3136 - loss 0.55420034 - samples/sec: 11.56 - lr: 0.000002
2021-10-14 03:04:57,720 epoch 13 - iter 2191/3136 - loss 0.54803087 - samples/sec: 11.49 - lr: 0.000001
2021-10-14 03:06:46,765 epoch 13 - iter 2504/3136 - loss 0.54272242 - samples/sec: 11.48 - lr: 0.000001
2021-10-14 03:08:34,963 epoch 13 - iter 2817/3136 - loss 0.54226930 - samples/sec: 11.57 - lr: 0.000001
2021-10-14 03:10:22,880 epoch 13 - iter 3130/3136 - loss 0.54299852 - samples/sec: 11.60 - lr: 0.000001
2021-10-14 03:10:24,858 ----------------------------------------------------------------------------------------------------
2021-10-14 03:10:24,858 EPOCH 13 done: loss 0.5426 - lr 0.0000014
2021-10-14 03:11:30,479 DEV : loss 0.9702442288398743 - score 0.8212
2021-10-14 03:11:30,488 BAD EPOCHS (no improvement): 4
2021-10-14 03:11:30,491 ----------------------------------------------------------------------------------------------------
2021-10-14 03:13:19,377 epoch 14 - iter 313/3136 - loss 0.51621028 - samples/sec: 11.50 - lr: 0.000001
2021-10-14 03:15:08,030 epoch 14 - iter 626/3136 - loss 0.51740776 - samples/sec: 11.52 - lr: 0.000001
2021-10-14 03:16:57,488 epoch 14 - iter 939/3136 - loss 0.52994289 - samples/sec: 11.44 - lr: 0.000001
2021-10-14 03:18:45,939 epoch 14 - iter 1252/3136 - loss 0.53180611 - samples/sec: 11.55 - lr: 0.000001
2021-10-14 03:20:34,911 epoch 14 - iter 1565/3136 - loss 0.52911617 - samples/sec: 11.49 - lr: 0.000001
2021-10-14 03:22:22,106 epoch 14 - iter 1878/3136 - loss 0.53214367 - samples/sec: 11.68 - lr: 0.000001
2021-10-14 03:24:09,594 epoch 14 - iter 2191/3136 - loss 0.54275492 - samples/sec: 11.65 - lr: 0.000001
2021-10-14 03:25:57,054 epoch 14 - iter 2504/3136 - loss 0.54019205 - samples/sec: 11.65 - lr: 0.000001
2021-10-14 03:27:44,154 epoch 14 - iter 2817/3136 - loss 0.53649206 - samples/sec: 11.69 - lr: 0.000001
2021-10-14 03:29:31,842 epoch 14 - iter 3130/3136 - loss 0.53498830 - samples/sec: 11.63 - lr: 0.000001
2021-10-14 03:29:34,031 ----------------------------------------------------------------------------------------------------
2021-10-14 03:29:34,031 EPOCH 14 done: loss 0.5352 - lr 0.0000010
2021-10-14 03:30:38,974 DEV : loss 0.9977230429649353 - score 0.821
2021-10-14 03:30:38,982 BAD EPOCHS (no improvement): 4
2021-10-14 03:30:38,998 ----------------------------------------------------------------------------------------------------
2021-10-14 03:32:26,023 epoch 15 - iter 313/3136 - loss 0.60750382 - samples/sec: 11.70 - lr: 0.000001
2021-10-14 03:34:15,476 epoch 15 - iter 626/3136 - loss 0.58801652 - samples/sec: 11.44 - lr: 0.000001
2021-10-14 03:36:11,422 epoch 15 - iter 939/3136 - loss 0.54892984 - samples/sec: 10.80 - lr: 0.000001
2021-10-14 03:38:10,024 epoch 15 - iter 1252/3136 - loss 0.53716294 - samples/sec: 10.56 - lr: 0.000001
2021-10-14 03:39:59,228 epoch 15 - iter 1565/3136 - loss 0.53353977 - samples/sec: 11.47 - lr: 0.000001
2021-10-14 03:41:47,675 epoch 15 - iter 1878/3136 - loss 0.53600619 - samples/sec: 11.55 - lr: 0.000001
2021-10-14 03:43:36,096 epoch 15 - iter 2191/3136 - loss 0.53482001 - samples/sec: 11.55 - lr: 0.000001
2021-10-14 03:45:24,217 epoch 15 - iter 2504/3136 - loss 0.53284053 - samples/sec: 11.58 - lr: 0.000001
2021-10-14 03:47:11,559 epoch 15 - iter 2817/3136 - loss 0.52801018 - samples/sec: 11.66 - lr: 0.000001
2021-10-14 03:49:00,178 epoch 15 - iter 3130/3136 - loss 0.53161333 - samples/sec: 11.53 - lr: 0.000001
2021-10-14 03:49:02,146 ----------------------------------------------------------------------------------------------------
2021-10-14 03:49:02,147 EPOCH 15 done: loss 0.5313 - lr 0.0000007
2021-10-14 03:50:07,351 DEV : loss 0.9958623051643372 - score 0.8225
2021-10-14 03:50:07,360 BAD EPOCHS (no improvement): 4
2021-10-14 03:50:07,363 ----------------------------------------------------------------------------------------------------
2021-10-14 03:51:57,072 epoch 16 - iter 313/3136 - loss 0.51365222 - samples/sec: 11.41 - lr: 0.000001
2021-10-14 03:53:44,826 epoch 16 - iter 626/3136 - loss 0.50880028 - samples/sec: 11.62 - lr: 0.000001
2021-10-14 03:55:32,475 epoch 16 - iter 939/3136 - loss 0.52589112 - samples/sec: 11.63 - lr: 0.000001
2021-10-14 03:57:21,086 epoch 16 - iter 1252/3136 - loss 0.53204579 - samples/sec: 11.53 - lr: 0.000001
2021-10-14 03:59:08,445 epoch 16 - iter 1565/3136 - loss 0.52660990 - samples/sec: 11.66 - lr: 0.000001
2021-10-14 04:00:55,323 epoch 16 - iter 1878/3136 - loss 0.51597621 - samples/sec: 11.72 - lr: 0.000001
2021-10-14 04:02:42,576 epoch 16 - iter 2191/3136 - loss 0.51317321 - samples/sec: 11.67 - lr: 0.000001
2021-10-14 04:04:30,283 epoch 16 - iter 2504/3136 - loss 0.51258168 - samples/sec: 11.62 - lr: 0.000001
2021-10-14 04:06:17,861 epoch 16 - iter 2817/3136 - loss 0.50836182 - samples/sec: 11.64 - lr: 0.000001
2021-10-14 04:08:10,845 epoch 16 - iter 3130/3136 - loss 0.50857361 - samples/sec: 11.08 - lr: 0.000000
2021-10-14 04:08:12,889 ----------------------------------------------------------------------------------------------------
2021-10-14 04:08:12,889 EPOCH 16 done: loss 0.5083 - lr 0.0000005
2021-10-14 04:09:18,423 DEV : loss 1.0039139986038208 - score 0.8229
2021-10-14 04:09:18,432 BAD EPOCHS (no improvement): 4
2021-10-14 04:09:18,435 ----------------------------------------------------------------------------------------------------
2021-10-14 04:11:07,722 epoch 17 - iter 313/3136 - loss 0.56263963 - samples/sec: 11.46 - lr: 0.000000
2021-10-14 04:12:55,606 epoch 17 - iter 626/3136 - loss 0.54564569 - samples/sec: 11.61 - lr: 0.000000
2021-10-14 04:14:43,554 epoch 17 - iter 939/3136 - loss 0.53068324 - samples/sec: 11.60 - lr: 0.000000
2021-10-14 04:16:33,035 epoch 17 - iter 1252/3136 - loss 0.52390386 - samples/sec: 11.44 - lr: 0.000000
2021-10-14 04:18:20,407 epoch 17 - iter 1565/3136 - loss 0.52591071 - samples/sec: 11.66 - lr: 0.000000
2021-10-14 04:20:09,500 epoch 17 - iter 1878/3136 - loss 0.51905411 - samples/sec: 11.48 - lr: 0.000000
2021-10-14 04:21:59,803 epoch 17 - iter 2191/3136 - loss 0.51736321 - samples/sec: 11.35 - lr: 0.000000
2021-10-14 04:23:47,790 epoch 17 - iter 2504/3136 - loss 0.51535452 - samples/sec: 11.59 - lr: 0.000000
2021-10-14 04:25:36,144 epoch 17 - iter 2817/3136 - loss 0.51113843 - samples/sec: 11.56 - lr: 0.000000
2021-10-14 04:27:24,780 epoch 17 - iter 3130/3136 - loss 0.50606329 - samples/sec: 11.53 - lr: 0.000000
2021-10-14 04:27:26,753 ----------------------------------------------------------------------------------------------------
2021-10-14 04:27:26,753 EPOCH 17 done: loss 0.5058 - lr 0.0000003
2021-10-14 04:28:32,206 DEV : loss 1.0060402154922485 - score 0.8225
2021-10-14 04:28:32,215 BAD EPOCHS (no improvement): 4
2021-10-14 04:28:32,217 ----------------------------------------------------------------------------------------------------
2021-10-14 04:30:21,243 epoch 18 - iter 313/3136 - loss 0.49126752 - samples/sec: 11.48 - lr: 0.000000
2021-10-14 04:32:09,036 epoch 18 - iter 626/3136 - loss 0.49374799 - samples/sec: 11.62 - lr: 0.000000
2021-10-14 04:33:56,523 epoch 18 - iter 939/3136 - loss 0.49309295 - samples/sec: 11.65 - lr: 0.000000
2021-10-14 04:35:47,283 epoch 18 - iter 1252/3136 - loss 0.50253615 - samples/sec: 11.30 - lr: 0.000000
2021-10-14 04:37:35,276 epoch 18 - iter 1565/3136 - loss 0.49822961 - samples/sec: 11.59 - lr: 0.000000
2021-10-14 04:39:23,044 epoch 18 - iter 1878/3136 - loss 0.50103757 - samples/sec: 11.62 - lr: 0.000000
2021-10-14 04:41:14,600 epoch 18 - iter 2191/3136 - loss 0.50777001 - samples/sec: 11.22 - lr: 0.000000
2021-10-14 04:43:02,675 epoch 18 - iter 2504/3136 - loss 0.50861819 - samples/sec: 11.59 - lr: 0.000000
2021-10-14 04:44:50,322 epoch 18 - iter 2817/3136 - loss 0.50466293 - samples/sec: 11.63 - lr: 0.000000
2021-10-14 04:46:38,596 epoch 18 - iter 3130/3136 - loss 0.49836073 - samples/sec: 11.56 - lr: 0.000000
2021-10-14 04:46:40,623 ----------------------------------------------------------------------------------------------------
2021-10-14 04:46:40,624 EPOCH 18 done: loss 0.4998 - lr 0.0000001
2021-10-14 04:47:46,232 DEV : loss 1.0087331533432007 - score 0.8227
2021-10-14 04:47:46,240 BAD EPOCHS (no improvement): 4
2021-10-14 04:47:46,243 ----------------------------------------------------------------------------------------------------
2021-10-14 04:49:33,999 epoch 19 - iter 313/3136 - loss 0.47633857 - samples/sec: 11.62 - lr: 0.000000
2021-10-14 04:51:22,527 epoch 19 - iter 626/3136 - loss 0.45879867 - samples/sec: 11.54 - lr: 0.000000
2021-10-14 04:53:13,022 epoch 19 - iter 939/3136 - loss 0.46662481 - samples/sec: 11.33 - lr: 0.000000
2021-10-14 04:55:02,036 epoch 19 - iter 1252/3136 - loss 0.48004070 - samples/sec: 11.49 - lr: 0.000000
2021-10-14 04:56:50,659 epoch 19 - iter 1565/3136 - loss 0.47758686 - samples/sec: 11.53 - lr: 0.000000
2021-10-14 04:58:38,517 epoch 19 - iter 1878/3136 - loss 0.47229384 - samples/sec: 11.61 - lr: 0.000000
2021-10-14 05:00:28,559 epoch 19 - iter 2191/3136 - loss 0.47546932 - samples/sec: 11.38 - lr: 0.000000
2021-10-14 05:02:16,692 epoch 19 - iter 2504/3136 - loss 0.48449171 - samples/sec: 11.58 - lr: 0.000000
2021-10-14 05:04:05,180 epoch 19 - iter 2817/3136 - loss 0.48532110 - samples/sec: 11.54 - lr: 0.000000
2021-10-14 05:05:54,648 epoch 19 - iter 3130/3136 - loss 0.48648792 - samples/sec: 11.44 - lr: 0.000000
2021-10-14 05:05:56,733 ----------------------------------------------------------------------------------------------------
2021-10-14 05:05:56,733 EPOCH 19 done: loss 0.4860 - lr 0.0000000
2021-10-14 05:07:01,884 DEV : loss 1.0088868141174316 - score 0.8221
2021-10-14 05:07:01,893 BAD EPOCHS (no improvement): 4
2021-10-14 05:07:01,917 ----------------------------------------------------------------------------------------------------
2021-10-14 05:08:52,621 epoch 20 - iter 313/3136 - loss 0.48843094 - samples/sec: 11.31 - lr: 0.000000
2021-10-14 05:10:40,479 epoch 20 - iter 626/3136 - loss 0.47657694 - samples/sec: 11.61 - lr: 0.000000
2021-10-14 05:12:29,303 epoch 20 - iter 939/3136 - loss 0.47776182 - samples/sec: 11.51 - lr: 0.000000
2021-10-14 05:14:17,296 epoch 20 - iter 1252/3136 - loss 0.47606125 - samples/sec: 11.59 - lr: 0.000000
2021-10-14 05:16:04,511 epoch 20 - iter 1565/3136 - loss 0.47329774 - samples/sec: 11.68 - lr: 0.000000
2021-10-14 05:17:53,783 epoch 20 - iter 1878/3136 - loss 0.48004950 - samples/sec: 11.46 - lr: 0.000000
2021-10-14 05:19:42,536 epoch 20 - iter 2191/3136 - loss 0.47761194 - samples/sec: 11.51 - lr: 0.000000
2021-10-14 05:21:31,199 epoch 20 - iter 2504/3136 - loss 0.48877158 - samples/sec: 11.52 - lr: 0.000000
2021-10-14 05:23:20,627 epoch 20 - iter 2817/3136 - loss 0.48520064 - samples/sec: 11.44 - lr: 0.000000
2021-10-14 05:25:14,015 epoch 20 - iter 3130/3136 - loss 0.48188844 - samples/sec: 11.04 - lr: 0.000000
2021-10-14 05:25:16,061 ----------------------------------------------------------------------------------------------------
2021-10-14 05:25:16,061 EPOCH 20 done: loss 0.4812 - lr 0.0000000
2021-10-14 05:26:21,820 DEV : loss 1.0086164474487305 - score 0.8222
2021-10-14 05:26:21,828 BAD EPOCHS (no improvement): 4
2021-10-14 05:26:51,563 ----------------------------------------------------------------------------------------------------
2021-10-14 05:26:51,563 Testing using best model ...
2021-10-14 05:28:07,476 	0.8153
2021-10-14 05:28:07,476 
Results:
- F-score (micro): 0.8153
- F-score (macro): 0.2736
- Accuracy (incl. no class): 0.8153

By class:
                    precision    recall  f1-score   support

            1,NOUN     0.9183    0.9316    0.9249      4666
   1,NOUN,ARGM-ADJ     0.7616    0.6886    0.7233       167
        1,AUX,ARG1     0.9107    0.9244    0.9175       397
             2,ADJ     0.1111    0.0400    0.0588        25
             1,ADJ     0.8669    0.8829    0.8748       760
    1,ADJ,ARGM-EXT     0.7500    0.8276    0.7869        58
       -1,AUX,ARG2     0.9053    0.9138    0.9095       429
            -1,ADJ     0.7433    0.7591    0.7511       328
           -1,ROOT     0.8984    0.8847    0.8915      1709
            1,VERB     0.9362    0.9475    0.9418      1828
       1,VERB,ARG1     0.7703    0.7982    0.7840       332
           -1,PRON     0.7500    0.6848    0.7159        92
           1,PROPN     0.8457    0.8087    0.8268       983
      -1,VERB,ARG2     0.6941    0.7248    0.7091       407
           -1,VERB     0.8195    0.8448    0.8320       844
           -2,PRON     0.5897    0.7188    0.6479        32
       1,VERB,ARG0     0.9443    0.9568    0.9505      1064
            4,NOUN     0.8889    0.3333    0.4848        24
            3,NOUN     0.6296    0.5862    0.6071       116
            2,NOUN     0.7520    0.7767    0.7641       609
           -1,NOUN     0.7566    0.8034    0.7793      1180
      -1,VERB,ARG1     0.9180    0.9309    0.9244      1563
      -1,VERB,ARG4     0.7255    0.6727    0.6981        55
            -3,ADJ     0.3913    0.3750    0.3830        24
           2,PROPN     0.7070    0.7005    0.7037       217
               1,X     0.8049    0.7333    0.7674        45
  -1,VERB,ARGM-ADV     0.4554    0.5258    0.4880        97
          -1,PROPN     0.7921    0.7581    0.7748       583
          -2,PROPN     0.5912    0.7297    0.6532       222
          -4,PROPN     0.0735    0.0862    0.0794        58
              -1,X     0.7692    0.5455    0.6383        55
    1,AUX,ARGM-DIS     0.4762    0.5882    0.5263        17
    1,AUX,ARGM-ADV     0.6364    0.6364    0.6364        22
       -1,AUX,ARG1     0.8269    0.8431    0.8350        51
   -1,AUX,ARGM-ADV     0.5192    0.6923    0.5934        39
    1,AUX,ARGM-TMP     0.7778    0.7368    0.7568        19
   -1,AUX,ARGM-TMP     0.6774    0.9130    0.7778        23
  1,AUX,R-ARGM-TMP     1.0000    0.0000    0.0000         1
           -3,NOUN     0.2617    0.3256    0.2902        86
       1,NOUN,ARG2     0.2353    0.2000    0.2162        20
       1,NOUN,ARG0     0.8155    0.7500    0.7814       112
       1,NOUN,ARG1     0.5789    0.6226    0.6000       106
      -1,NOUN,ARG2     0.4688    0.5000    0.4839        30
   1,VERB,ARGM-MOD     0.9620    0.9838    0.9728       309
   1,VERB,ARGM-ADV     0.8035    0.7394    0.7701       188
  -1,VERB,ARGM-TMP     0.8264    0.8902    0.8571       246
   1,VERB,ARGM-MNR     0.6250    0.6897    0.6557        29
   -1,AUX,ARGM-LOC     0.6000    0.5000    0.5455         6
           -4,VERB     0.5465    0.5732    0.5595        82
            1,PRON     0.9038    0.9527    0.9276       148
            -1,ADV     0.6917    0.7345    0.7124       113
      -1,VERB,ARG3     0.5778    0.5200    0.5474        50
     1,VERB,R-ARG0     0.9600    0.9600    0.9600        50
   1,VERB,ARGM-NEG     0.9517    0.9787    0.9650       141
      -2,VERB,ARG1     0.5897    0.6389    0.6133        36
         -1,VERB,V     1.0000    0.0000    0.0000         3
           -3,VERB     0.6540    0.7005    0.6765       197
             1,ADV     0.7357    0.8110    0.7715       127
           -5,VERB     0.3000    0.4000    0.3429        30
   2,NOUN,ARGM-ADJ     0.5385    0.8235    0.6512        17
       1,NOUN,ARG3     1.0000    0.0000    0.0000         4
   1,NOUN,ARGM-MNR     0.4839    0.4839    0.4839        31
      -1,NOUN,ARG1     0.7222    0.7647    0.7429       136
    -1,VERB,C-ARG1     0.6154    0.5455    0.5783        44
  -2,VERB,ARGM-LOC     1.0000    0.0000    0.0000         8
  -1,VERB,ARGM-LOC     0.6620    0.7402    0.6989       127
  -1,VERB,ARGM-GOL     0.0000    0.0000    0.0000        13
  -1,VERB,ARGM-PRP     0.6349    0.7407    0.6838        54
           -2,VERB     0.7551    0.7688    0.7619       385
   1,NOUN,ARGM-LVB     0.8095    0.8095    0.8095        42
  -1,VERB,ARGM-PRR     0.7465    0.8548    0.7970        62
            -2,DET     0.6667    0.6667    0.6667         9
            -1,NUM     0.7143    0.5294    0.6081        85
     1,VERB,R-ARG1     0.7632    0.9667    0.8529        30
   1,NOUN,ARGM-LOC     1.0000    0.0000    0.0000         3
   1,VERB,ARGM-LOC     0.6190    0.6190    0.6190        21
      -2,NOUN,ARG1     0.4483    0.7222    0.5532        18
       2,VERB,ARG0     0.7826    0.8571    0.8182        21
  -1,NOUN,ARGM-PRD     0.0000    0.0000    0.0000         8
  -1,NOUN,ARGM-LVB     0.2500    0.2500    0.2500         4
      -1,VERB,ARG0     0.7255    0.7400    0.7327        50
      -1,NOUN,ARG0     0.3750    0.5625    0.4500        16
       3,VERB,ARG0     1.0000    0.0000    0.0000         5
         <UNKNOWN>     0.0000    1.0000    0.0000         0
           -2,NOUN     0.5101    0.5650    0.5362       223
     3,VERB,R-ARG0     1.0000    0.0000    0.0000         1
            3,VERB     0.2222    0.1429    0.1739        14
   2,VERB,ARGM-ADV     0.4667    0.6364    0.5385        11
             1,DET     0.8235    0.8235    0.8235        17
           -7,VERB     1.0000    0.0000    0.0000         4
   -1,AUX,ARGM-NEG     0.8750    0.9130    0.8936        23
       2,NOUN,ARG1     0.5000    0.2857    0.3636         7
       -1,ADJ,ARG1     0.6667    0.6154    0.6400        26
       -1,ADJ,ARG0     0.3750    0.4000    0.3871        15
  -1,VERB,ARGM-PRD     0.0000    0.0000    0.0000        16
            4,VERB     1.0000    0.0000    0.0000         4
  -1,NOUN,ARGM-ADJ     0.4643    0.4333    0.4483        30
   1,VERB,ARGM-TMP     0.8280    0.9774    0.8966       133
           -9,VERB     1.0000    0.0000    0.0000         2
          -10,VERB     1.0000    0.0000    0.0000         2
    1,ADJ,ARGM-ADV     0.1667    0.1667    0.1667         6
  -1,VERB,ARGM-MNR     0.5185    0.7000    0.5957        60
   1,VERB,ARGM-DIS     0.7739    0.9082    0.8357        98
   1,NOUN,ARGM-NEG     0.7368    0.8235    0.7778        17
        2,AUX,ARG1     0.8625    0.9583    0.9079        72
           4,PROPN     0.6250    0.5556    0.5882        18
       -1,ADJ,ARG2     0.5778    0.7027    0.6341        37
      -2,NOUN,ARG2     1.0000    0.0000    0.0000         3
            -2,ADJ     0.4955    0.5789    0.5340        95
  -2,VERB,ARGM-ADV     0.0000    0.0000    0.0000        13
           -6,VERB     1.0000    0.0000    0.0000        17
            -1,ADP     0.7895    0.7143    0.7500        21
   1,NOUN,ARGM-TMP     0.6800    0.6071    0.6415        28
            2,VERB     0.4688    0.4286    0.4478        35
  -2,VERB,ARGM-TMP     1.0000    0.0000    0.0000        14
  -2,NOUN,ARGM-PRD     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-TMP     0.4500    0.6923    0.5455        13
  -1,VERB,ARGM-CAU     0.4138    0.8571    0.5581        14
       -2,ADJ,ARG2     1.0000    0.0000    0.0000         2
  -1,NOUN,ARGM-LOC     0.5455    0.6667    0.6000        18
             1,NUM     0.8125    0.8432    0.8276       185
  -4,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
      1,AUX,R-ARG1     0.7500    0.6000    0.6667        10
        1,ADJ,ARG2     1.0000    1.0000    1.0000         1
  -2,NOUN,ARGM-ADJ     1.0000    0.0000    0.0000         3
            -2,NUM     1.0000    0.5938    0.7451        32
             2,NUM     0.9500    0.6786    0.7917        28
            -6,NUM     1.0000    0.0000    0.0000         1
            -7,NUM     1.0000    0.0000    0.0000         1
            -8,NUM     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-MNR     1.0000    0.0000    0.0000         8
     1,VERB,R-ARG2     0.0000    0.0000    0.0000         1
          -3,PROPN     0.3832    0.3565    0.3694       115
      -4,NOUN,ARG1     1.0000    0.0000    0.0000         1
           -4,NOUN     0.2353    0.1194    0.1584        67
  -1,VERB,ARGM-EXT     0.4615    0.6667    0.5455         9
            -1,DET     0.9286    0.6190    0.7429        21
            -3,NUM     1.0000    0.0000    0.0000         1
       2,NOUN,ARG0     0.3077    0.4000    0.3478        10
 1,VERB,R-ARGM-LOC     0.8750    1.0000    0.9333         7
           -5,NOUN     0.2000    0.3125    0.2439        16
       -2,ADJ,ARG1     1.0000    0.0000    0.0000         3
       -3,AUX,ARG1     1.0000    0.0000    0.0000         1
       -2,AUX,ARG1     0.0000    0.0000    0.0000         2
            -6,ADJ     1.0000    0.0000    0.0000         3
       2,VERB,ARG1     0.6471    0.6111    0.6286        18
   1,VERB,ARGM-PRR     1.0000    0.0000    0.0000         3
  -1,NOUN,ARGM-PRP     1.0000    0.3333    0.5000         3
   -1,AUX,ARGM-PRD     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-ADV     1.0000    0.0000    0.0000         2
       1,VERB,ARG2     0.4762    0.3846    0.4255        26
   1,NOUN,ARGM-PRP     1.0000    0.0000    0.0000         2
  -2,NOUN,ARGM-TMP     1.0000    0.0000    0.0000         3
      -2,NOUN,ARG0     1.0000    0.0000    0.0000         4
             1,SYM     0.6486    0.7273    0.6857        33
            -1,SYM     0.5000    0.9565    0.6567        46
           3,PROPN     0.5000    0.4600    0.4792        50
   1,VERB,ARGM-EXT     0.6667    0.6667    0.6667        24
           -9,NOUN     1.0000    0.0000    0.0000         2
           -7,NOUN     0.0000    0.0000    0.0000         4
     -1,ADJ,C-ARG1     0.6000    1.0000    0.7500         3
            -2,ADV     0.6000    0.1875    0.2857        16
            -4,ADJ     0.0000    0.0000    0.0000        11
      -2,VERB,ARG2     0.3333    0.0833    0.1333        12
    1,AUX,ARGM-MOD     0.9848    0.9559    0.9701        68
    2,AUX,ARGM-ADV     0.7273    0.6667    0.6957        12
       -1,VERB,C-V     0.7647    0.8667    0.8125        15
   1,NOUN,ARGM-PRD     0.5556    0.5000    0.5263        10
   1,NOUN,ARGM-EXT     0.3333    0.4000    0.3636         5
      -3,NOUN,ARG3     1.0000    0.0000    0.0000         2
   3,VERB,ARGM-TMP     1.0000    0.0000    0.0000         2
   3,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
      -3,NOUN,ARG0     1.0000    0.0000    0.0000         1
       4,VERB,ARG0     1.0000    0.0000    0.0000         1
        1,AUX,ARG2     0.8077    0.7500    0.7778        28
             3,ADV     1.0000    0.0000    0.0000         2
             2,ADV     1.0000    0.0000    0.0000         7
      -3,NOUN,ARG1     1.0000    0.2000    0.3333         5
          -5,PROPN     1.0000    0.0000    0.0000        41
  -1,VERB,ARGM-COM     0.5000    0.5455    0.5217        11
          -8,PROPN     1.0000    0.0000    0.0000        13
  -2,VERB,ARGM-MNR     1.0000    0.0000    0.0000         2
    -1,NOUN,R-ARG1     1.0000    0.0000    0.0000         1
      -3,VERB,ARG1     1.0000    0.0000    0.0000         6
  -1,VERB,ARGM-DIS     0.4545    0.3846    0.4167        13
           -8,VERB     1.0000    0.0000    0.0000         3
       5,VERB,ARG0     1.0000    0.0000    0.0000         1
      -3,VERB,ARG2     1.0000    0.0000    0.0000         3
            5,VERB     1.0000    0.0000    0.0000         4
       4,VERB,ARG1     1.0000    0.0000    0.0000         2
   3,VERB,ARGM-DIS     1.0000    0.0000    0.0000         2
   2,VERB,ARGM-DIS     0.2500    0.3333    0.2857         3
  -1,VERB,ARGM-NEG     0.5000    0.5000    0.5000         2
   1,VERB,ARGM-PRD     1.0000    0.0000    0.0000         1
       -1,ADJ,ARG3     1.0000    0.0000    0.0000         7
          -6,PROPN     0.0952    0.1333    0.1111        15
   2,NOUN,ARGM-GOL     1.0000    0.0000    0.0000         2
    2,AUX,ARGM-DIS     0.3333    0.3333    0.3333         3
   2,VERB,ARGM-CAU     1.0000    0.0000    0.0000         3
    1,ADJ,ARGM-CXN     0.8333    1.0000    0.9091         5
             2,AUX     1.0000    0.0000    0.0000         1
             1,AUX     0.2500    0.2727    0.2609        11
 -1,ADJ,C-ARGM-CXN     0.6000    0.6000    0.6000         5
           5,PROPN     1.0000    0.0000    0.0000         1
          -7,PROPN     1.0000    0.0000    0.0000         6
  -1,VERB,ARGM-DIR     0.4103    0.4103    0.4103        39
   -1,AUX,ARGM-MNR     1.0000    0.0000    0.0000         2
   1,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         2
  -2,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         5
   -1,AUX,ARGM-EXT     1.0000    0.0000    0.0000         3
   2,NOUN,ARGM-LOC     1.0000    0.0000    0.0000         2
 1,NOUN,R-ARGM-ADJ     1.0000    0.0000    0.0000         1
            1,INTJ     1.0000    0.0000    0.0000         7
           -1,INTJ     0.6957    0.5714    0.6275        28
   4,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
      -5,NOUN,ARG1     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
          -1,SCONJ     1.0000    0.5714    0.7273         7
   2,NOUN,ARGM-DIS     1.0000    0.0000    0.0000         2
       2,VERB,ARG2     1.0000    0.0000    0.0000         2
    -3,VERB,C-ARG1     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-CAU     0.5455    0.5455    0.5455        11
    -1,NOUN,C-ARG3     1.0000    0.0000    0.0000         1
    1,AUX,ARGM-NEG     0.8889    1.0000    0.9412         8
   -1,AUX,ARGM-CAU     1.0000    0.0000    0.0000         1
   -2,AUX,ARGM-DIS     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-DIS     1.0000    0.0000    0.0000         3
   1,NOUN,ARGM-MOD     0.8333    0.7143    0.7692         7
  -4,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
          -9,PROPN     1.0000    0.0000    0.0000         3
         -13,PROPN     1.0000    0.0000    0.0000         1
         -15,PROPN     1.0000    0.0000    0.0000         1
         -16,PROPN     1.0000    0.0000    0.0000         2
         -18,PROPN     1.0000    0.0000    0.0000         1
            -2,SYM     1.0000    0.0000    0.0000         3
              -2,X     0.0000    0.0000    0.0000        12
   -1,AUX,ARGM-DIS     1.0000    0.1667    0.2857         6
   1,NOUN,ARGM-DIS     0.5000    0.6667    0.5714         3
   2,VERB,ARGM-MNR     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-DIS     0.4545    1.0000    0.6250         5
  -2,NOUN,ARGM-PRP     1.0000    0.0000    0.0000         1
        2,AUX,ARG2     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-TMP     0.5000    0.4000    0.4444         5
  -1,NOUN,ARGM-EXT     1.0000    0.5000    0.6667         2
      -1,VERB,ARG5     1.0000    0.0000    0.0000         1
              -3,X     1.0000    0.0000    0.0000         5
       3,NOUN,ARG0     1.0000    0.0000    0.0000         2
               2,X     1.0000    0.0000    0.0000         3
   3,NOUN,ARGM-TMP     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         4
   -1,ADJ,ARGM-TMP     1.0000    0.0000    0.0000         1
      -4,NOUN,ARG4     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-GOL     1.0000    0.0000    0.0000         1
      -2,VERB,ARG0     1.0000    0.0000    0.0000         1
           -6,NOUN     0.5000    0.2500    0.3333         4
      -2,NOUN,ARG3     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-TMP     1.0000    0.0000    0.0000         2
            2,PRON     1.0000    0.0000    0.0000         4
   2,VERB,ARGM-PRR     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-MOD     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-LVB     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-PRD     1.0000    1.0000    1.0000         1
  -1,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         8
           7,PROPN     1.0000    0.0000    0.0000         2
  -3,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         1
      1,ADJ,R-ARG1     1.0000    0.0000    0.0000         1
      2,AUX,R-ARG1     0.0000    1.0000    0.0000         0
    1,ADJ,ARGM-MOD     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-MOD     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-GOL     1.0000    0.0000    0.0000         2
            -1,AUX     0.0000    0.0000    0.0000         8
 3,VERB,R-ARGM-ADV     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-COM     1.0000    0.0000    0.0000         2
   3,NOUN,ARGM-ADJ     1.0000    0.0000    0.0000         5
       3,VERB,ARG1     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-DIR     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-CAU     1.0000    0.0000    0.0000         2
  -3,VERB,ARGM-PRP     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         2
           -3,PRON     1.0000    0.0000    0.0000         4
   -1,AUX,ARGM-PRP     1.0000    0.0000    0.0000         4
    -1,VERB,C-ARG2     0.4286    0.5000    0.4615         6
        1,ADJ,ARG1     1.0000    0.0000    0.0000         2
         -1,NOUN,V     1.0000    0.0000    0.0000         1
           -8,NOUN     1.0000    0.0000    0.0000         4
   2,VERB,ARGM-TMP     1.0000    0.6667    0.8000         3
    -1,VERB,C-ARG0     1.0000    0.0000    0.0000         2
              -4,X     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-LVB     1.0000    0.0000    0.0000         5
   -2,AUX,ARGM-ADV     1.0000    0.0000    0.0000         3
            -5,ADJ     1.0000    0.0000    0.0000         2
    1,ADJ,ARGM-ADJ     1.0000    0.0000    0.0000         2
           1,PUNCT     1.0000    0.0000    0.0000         1
          -1,PUNCT     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-LOC     1.0000    0.0000    0.0000         1
   5,VERB,ARGM-ADV     1.0000    0.0000    0.0000         1
            -2,AUX     1.0000    0.0000    0.0000         5
   9,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         1
       3,NOUN,ARG1     1.0000    0.0000    0.0000         1
             1,ADP     1.0000    0.0000    0.0000         9
     1,NOUN,R-ARG0     1.0000    0.0000    0.0000         2
            5,NOUN     1.0000    0.0000    0.0000         5
      -5,VERB,ARG1     1.0000    0.0000    0.0000         1
   1,VERB,ARG1-DSP     1.0000    0.0000    0.0000         4
-1,VERB,C-ARG1-DSP     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-NEG     1.0000    0.0000    0.0000         1
      -4,VERB,ARG1     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-GOL     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-PRP     1.0000    0.0000    0.0000         2
       -3,ADJ,ARG0     1.0000    0.0000    0.0000         1
      -1,NOUN,ARG3     1.0000    0.0000    0.0000         2
   -2,AUX,ARGM-CAU     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-DIR     1.0000    0.0000    0.0000         4
   -1,ADJ,ARGM-EXT     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-LOC     1.0000    0.0000    0.0000         1
   -3,ADJ,ARGM-CXN     1.0000    0.0000    0.0000         1
           1,CCONJ     0.6667    1.0000    0.8000         2
   2,NOUN,ARGM-NEG     1.0000    0.0000    0.0000         1
   1,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-CXN     1.0000    0.5000    0.6667         6
      -2,VERB,ARG3     1.0000    0.0000    0.0000         2
             2,DET     1.0000    0.0000    0.0000         2
   4,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-DIR     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-PRP     1.0000    0.0000    0.0000         1
-1,VERB,C-ARGM-LOC     1.0000    0.0000    0.0000         1
       1,VERB,ARGA     1.0000    0.0000    0.0000         2
  -3,VERB,ARGM-CAU     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-DIS     1.0000    0.0000    0.0000         1
            1,PART     1.0000    0.5000    0.6667         6
   1,VERB,ARGM-PRP     1.0000    0.0000    0.0000         3
            -3,ADV     1.0000    0.0000    0.0000         2
            -4,ADV     1.0000    0.0000    0.0000         1
          -10,NOUN     1.0000    0.0000    0.0000         2
          -13,NOUN     1.0000    0.0000    0.0000         2
          -15,NOUN     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
       1,VERB,ARG3     1.0000    0.0000    0.0000         2
   -1,AUX,ARGM-GOL     1.0000    0.0000    0.0000         2
           -2,INTJ     1.0000    0.0000    0.0000         2
           -3,INTJ     1.0000    0.0000    0.0000         1
    1,ADJ,ARGM-LVB     1.0000    0.0000    0.0000         1
    -2,VERB,C-ARG1     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
          -14,NOUN     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-TMP     1.0000    0.0000    0.0000         1
     -1,AUX,C-ARG2     1.0000    0.0000    0.0000         1
    -1,VERB,C-ARG3     1.0000    0.0000    0.0000         1
    1,ADJ,ARGM-NEG     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-DIR     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-CAU     1.0000    0.0000    0.0000         3
       -2,ADJ,ARG0     1.0000    0.0000    0.0000         2
           -1,PART     1.0000    0.0000    0.0000         2
    -1,NOUN,C-ARG1     1.0000    0.0000    0.0000         1
       -2,AUX,ARG2     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-DIR     1.0000    0.0000    0.0000         1
     -2,ADJ,C-ARG1     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-PRR     1.0000    0.0000    0.0000         1
        3,AUX,ARG1     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-MOD     1.0000    1.0000    1.0000         1
    2,AUX,ARGM-NEG     1.0000    0.0000    0.0000         1

          accuracy                         0.8153     25096
         macro avg     0.8169    0.2836    0.2736     25096
      weighted avg     0.8225    0.8153    0.8076     25096

2021-10-14 05:28:07,477 ----------------------------------------------------------------------------------------------------
