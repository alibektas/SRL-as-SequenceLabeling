2021-10-19 12:01:43,507 ----------------------------------------------------------------------------------------------------
2021-10-19 12:01:43,510 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 1024, padding_idx=0)
        (position_embeddings): Embedding(512, 1024)
        (token_type_embeddings): Embedding(2, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=465, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-10-19 12:01:43,515 ----------------------------------------------------------------------------------------------------
2021-10-19 12:01:43,515 Corpus: "Corpus: 12543 train + 2002 dev + 2077 test sentences"
2021-10-19 12:01:43,515 ----------------------------------------------------------------------------------------------------
2021-10-19 12:01:43,515 Parameters:
2021-10-19 12:01:43,515  - learning_rate: "5e-06"
2021-10-19 12:01:43,515  - mini_batch_size: "4"
2021-10-19 12:01:43,515  - patience: "3"
2021-10-19 12:01:43,515  - anneal_factor: "0.5"
2021-10-19 12:01:43,515  - max_epochs: "20"
2021-10-19 12:01:43,515  - shuffle: "True"
2021-10-19 12:01:43,515  - train_with_dev: "False"
2021-10-19 12:01:43,522  - batch_growth_annealing: "False"
2021-10-19 12:01:43,522 ----------------------------------------------------------------------------------------------------
2021-10-19 12:01:43,522 Model training base path: "model/flattened/upos/transformer/2c96f99c-5503-4c42-91a8-ac8e5581946d"
2021-10-19 12:01:43,522 ----------------------------------------------------------------------------------------------------
2021-10-19 12:01:43,522 Device: cuda:1
2021-10-19 12:01:43,522 ----------------------------------------------------------------------------------------------------
2021-10-19 12:01:43,522 Embeddings storage mode: gpu
2021-10-19 12:01:43,539 ----------------------------------------------------------------------------------------------------
2021-10-19 12:03:37,334 epoch 1 - iter 313/3136 - loss 4.51593965 - samples/sec: 11.00 - lr: 0.000005
2021-10-19 12:05:29,304 epoch 1 - iter 626/3136 - loss 3.88018768 - samples/sec: 11.18 - lr: 0.000005
2021-10-19 12:07:20,008 epoch 1 - iter 939/3136 - loss 3.45383314 - samples/sec: 11.31 - lr: 0.000005
2021-10-19 12:09:08,330 epoch 1 - iter 1252/3136 - loss 3.15937151 - samples/sec: 11.56 - lr: 0.000005
2021-10-19 12:10:56,926 epoch 1 - iter 1565/3136 - loss 3.02432360 - samples/sec: 11.53 - lr: 0.000005
2021-10-19 12:12:47,663 epoch 1 - iter 1878/3136 - loss 2.95041662 - samples/sec: 11.31 - lr: 0.000005
2021-10-19 12:14:37,499 epoch 1 - iter 2191/3136 - loss 2.85003018 - samples/sec: 11.40 - lr: 0.000005
2021-10-19 12:16:26,100 epoch 1 - iter 2504/3136 - loss 2.75972735 - samples/sec: 11.53 - lr: 0.000005
2021-10-19 12:18:14,033 epoch 1 - iter 2817/3136 - loss 2.67286836 - samples/sec: 11.60 - lr: 0.000005
2021-10-19 12:20:01,871 epoch 1 - iter 3130/3136 - loss 2.60146933 - samples/sec: 11.61 - lr: 0.000005
2021-10-19 12:20:03,979 ----------------------------------------------------------------------------------------------------
2021-10-19 12:20:03,979 EPOCH 1 done: loss 2.6004 - lr 0.0000050
2021-10-19 12:21:11,881 DEV : loss 1.6463255882263184 - score 0.5882
2021-10-19 12:21:11,893 BAD EPOCHS (no improvement): 4
2021-10-19 12:21:11,915 ----------------------------------------------------------------------------------------------------
2021-10-19 12:22:59,790 epoch 2 - iter 313/3136 - loss 1.84808203 - samples/sec: 11.61 - lr: 0.000005
2021-10-19 12:24:50,788 epoch 2 - iter 626/3136 - loss 1.83037257 - samples/sec: 11.28 - lr: 0.000005
2021-10-19 12:26:40,660 epoch 2 - iter 939/3136 - loss 1.78440316 - samples/sec: 11.40 - lr: 0.000005
2021-10-19 12:28:34,126 epoch 2 - iter 1252/3136 - loss 1.74924129 - samples/sec: 11.04 - lr: 0.000005
2021-10-19 12:30:27,934 epoch 2 - iter 1565/3136 - loss 1.71752478 - samples/sec: 11.00 - lr: 0.000005
2021-10-19 12:32:17,392 epoch 2 - iter 1878/3136 - loss 1.69086094 - samples/sec: 11.44 - lr: 0.000005
2021-10-19 12:34:06,639 epoch 2 - iter 2191/3136 - loss 1.67281094 - samples/sec: 11.46 - lr: 0.000005
2021-10-19 12:35:54,741 epoch 2 - iter 2504/3136 - loss 1.67851204 - samples/sec: 11.58 - lr: 0.000005
2021-10-19 12:37:44,689 epoch 2 - iter 2817/3136 - loss 1.65663500 - samples/sec: 11.39 - lr: 0.000005
2021-10-19 12:39:35,954 epoch 2 - iter 3130/3136 - loss 1.63188639 - samples/sec: 11.25 - lr: 0.000005
2021-10-19 12:39:38,109 ----------------------------------------------------------------------------------------------------
2021-10-19 12:39:38,109 EPOCH 2 done: loss 1.6313 - lr 0.0000049
2021-10-19 12:40:44,155 DEV : loss 1.2584542036056519 - score 0.6734
2021-10-19 12:40:44,163 BAD EPOCHS (no improvement): 4
2021-10-19 12:40:44,168 ----------------------------------------------------------------------------------------------------
2021-10-19 12:42:34,497 epoch 3 - iter 313/3136 - loss 1.33580372 - samples/sec: 11.35 - lr: 0.000005
2021-10-19 12:44:22,883 epoch 3 - iter 626/3136 - loss 1.38419928 - samples/sec: 11.55 - lr: 0.000005
2021-10-19 12:46:11,571 epoch 3 - iter 939/3136 - loss 1.39057228 - samples/sec: 11.52 - lr: 0.000005
2021-10-19 12:47:59,826 epoch 3 - iter 1252/3136 - loss 1.36753213 - samples/sec: 11.57 - lr: 0.000005
2021-10-19 12:49:50,039 epoch 3 - iter 1565/3136 - loss 1.37057924 - samples/sec: 11.36 - lr: 0.000005
2021-10-19 12:51:39,007 epoch 3 - iter 1878/3136 - loss 1.35295378 - samples/sec: 11.49 - lr: 0.000005
2021-10-19 12:53:28,235 epoch 3 - iter 2191/3136 - loss 1.34062218 - samples/sec: 11.46 - lr: 0.000005
2021-10-19 12:55:19,657 epoch 3 - iter 2504/3136 - loss 1.32909153 - samples/sec: 11.24 - lr: 0.000005
2021-10-19 12:57:07,904 epoch 3 - iter 2817/3136 - loss 1.32225857 - samples/sec: 11.57 - lr: 0.000005
2021-10-19 12:58:56,505 epoch 3 - iter 3130/3136 - loss 1.31503967 - samples/sec: 11.53 - lr: 0.000005
2021-10-19 12:58:58,570 ----------------------------------------------------------------------------------------------------
2021-10-19 12:58:58,570 EPOCH 3 done: loss 1.3147 - lr 0.0000047
2021-10-19 13:00:04,902 DEV : loss 1.1374144554138184 - score 0.7082
2021-10-19 13:00:04,910 BAD EPOCHS (no improvement): 4
2021-10-19 13:00:04,912 ----------------------------------------------------------------------------------------------------
2021-10-19 13:01:52,996 epoch 4 - iter 313/3136 - loss 1.05979796 - samples/sec: 11.58 - lr: 0.000005
2021-10-19 13:03:41,329 epoch 4 - iter 626/3136 - loss 1.09961217 - samples/sec: 11.56 - lr: 0.000005
2021-10-19 13:05:30,013 epoch 4 - iter 939/3136 - loss 1.12770307 - samples/sec: 11.52 - lr: 0.000005
2021-10-19 13:07:19,494 epoch 4 - iter 1252/3136 - loss 1.12871295 - samples/sec: 11.44 - lr: 0.000005
2021-10-19 13:09:08,933 epoch 4 - iter 1565/3136 - loss 1.12588823 - samples/sec: 11.44 - lr: 0.000005
2021-10-19 13:10:58,251 epoch 4 - iter 1878/3136 - loss 1.10909897 - samples/sec: 11.45 - lr: 0.000005
2021-10-19 13:12:46,622 epoch 4 - iter 2191/3136 - loss 1.09485835 - samples/sec: 11.55 - lr: 0.000005
2021-10-19 13:14:36,650 epoch 4 - iter 2504/3136 - loss 1.09460400 - samples/sec: 11.38 - lr: 0.000005
2021-10-19 13:16:26,570 epoch 4 - iter 2817/3136 - loss 1.09545763 - samples/sec: 11.39 - lr: 0.000005
2021-10-19 13:18:20,411 epoch 4 - iter 3130/3136 - loss 1.08239742 - samples/sec: 11.00 - lr: 0.000005
2021-10-19 13:18:22,535 ----------------------------------------------------------------------------------------------------
2021-10-19 13:18:22,536 EPOCH 4 done: loss 1.0829 - lr 0.0000045
2021-10-19 13:19:28,370 DEV : loss 1.0719887018203735 - score 0.7337
2021-10-19 13:19:28,378 BAD EPOCHS (no improvement): 4
2021-10-19 13:19:28,385 ----------------------------------------------------------------------------------------------------
2021-10-19 13:21:17,260 epoch 5 - iter 313/3136 - loss 0.94015285 - samples/sec: 11.50 - lr: 0.000004
2021-10-19 13:23:06,225 epoch 5 - iter 626/3136 - loss 0.93656852 - samples/sec: 11.49 - lr: 0.000004
2021-10-19 13:24:55,022 epoch 5 - iter 939/3136 - loss 0.92945288 - samples/sec: 11.51 - lr: 0.000004
2021-10-19 13:26:45,001 epoch 5 - iter 1252/3136 - loss 0.93537875 - samples/sec: 11.39 - lr: 0.000004
2021-10-19 13:28:34,459 epoch 5 - iter 1565/3136 - loss 0.93704428 - samples/sec: 11.44 - lr: 0.000004
2021-10-19 13:30:23,438 epoch 5 - iter 1878/3136 - loss 0.96243148 - samples/sec: 11.49 - lr: 0.000004
2021-10-19 13:32:12,222 epoch 5 - iter 2191/3136 - loss 0.95074705 - samples/sec: 11.51 - lr: 0.000004
2021-10-19 13:34:09,247 epoch 5 - iter 2504/3136 - loss 0.94916453 - samples/sec: 10.70 - lr: 0.000004
2021-10-19 13:35:59,051 epoch 5 - iter 2817/3136 - loss 0.94320580 - samples/sec: 11.40 - lr: 0.000004
2021-10-19 13:37:47,991 epoch 5 - iter 3130/3136 - loss 0.94755160 - samples/sec: 11.49 - lr: 0.000004
2021-10-19 13:37:49,994 ----------------------------------------------------------------------------------------------------
2021-10-19 13:37:49,995 EPOCH 5 done: loss 0.9461 - lr 0.0000043
2021-10-19 13:38:55,380 DEV : loss 1.0760302543640137 - score 0.7435
2021-10-19 13:38:55,388 BAD EPOCHS (no improvement): 4
2021-10-19 13:38:55,392 ----------------------------------------------------------------------------------------------------
2021-10-19 13:40:45,226 epoch 6 - iter 313/3136 - loss 0.91580150 - samples/sec: 11.40 - lr: 0.000004
2021-10-19 13:42:34,328 epoch 6 - iter 626/3136 - loss 0.88040698 - samples/sec: 11.48 - lr: 0.000004
2021-10-19 13:44:27,249 epoch 6 - iter 939/3136 - loss 0.85789156 - samples/sec: 11.09 - lr: 0.000004
2021-10-19 13:46:18,757 epoch 6 - iter 1252/3136 - loss 0.85659319 - samples/sec: 11.23 - lr: 0.000004
2021-10-19 13:48:06,924 epoch 6 - iter 1565/3136 - loss 0.85429898 - samples/sec: 11.58 - lr: 0.000004
2021-10-19 13:49:55,147 epoch 6 - iter 1878/3136 - loss 0.84641658 - samples/sec: 11.57 - lr: 0.000004
2021-10-19 13:51:44,056 epoch 6 - iter 2191/3136 - loss 0.84151202 - samples/sec: 11.50 - lr: 0.000004
2021-10-19 13:53:33,019 epoch 6 - iter 2504/3136 - loss 0.85029317 - samples/sec: 11.49 - lr: 0.000004
2021-10-19 13:55:22,008 epoch 6 - iter 2817/3136 - loss 0.85582504 - samples/sec: 11.49 - lr: 0.000004
2021-10-19 13:57:11,258 epoch 6 - iter 3130/3136 - loss 0.85185450 - samples/sec: 11.46 - lr: 0.000004
2021-10-19 13:57:13,251 ----------------------------------------------------------------------------------------------------
2021-10-19 13:57:13,252 EPOCH 6 done: loss 0.8519 - lr 0.0000040
2021-10-19 13:58:18,125 DEV : loss 1.0767654180526733 - score 0.7533
2021-10-19 13:58:18,134 BAD EPOCHS (no improvement): 4
2021-10-19 13:58:18,137 ----------------------------------------------------------------------------------------------------
2021-10-19 14:00:09,811 epoch 7 - iter 313/3136 - loss 0.77307146 - samples/sec: 11.21 - lr: 0.000004
2021-10-19 14:01:58,775 epoch 7 - iter 626/3136 - loss 0.81157839 - samples/sec: 11.49 - lr: 0.000004
2021-10-19 14:03:47,745 epoch 7 - iter 939/3136 - loss 0.78939104 - samples/sec: 11.49 - lr: 0.000004
2021-10-19 14:05:37,056 epoch 7 - iter 1252/3136 - loss 0.79130826 - samples/sec: 11.45 - lr: 0.000004
2021-10-19 14:07:25,792 epoch 7 - iter 1565/3136 - loss 0.78768143 - samples/sec: 11.52 - lr: 0.000004
2021-10-19 14:09:19,025 epoch 7 - iter 1878/3136 - loss 0.79046350 - samples/sec: 11.06 - lr: 0.000004
2021-10-19 14:11:09,764 epoch 7 - iter 2191/3136 - loss 0.78349920 - samples/sec: 11.31 - lr: 0.000004
2021-10-19 14:12:58,850 epoch 7 - iter 2504/3136 - loss 0.77716884 - samples/sec: 11.48 - lr: 0.000004
2021-10-19 14:14:47,545 epoch 7 - iter 2817/3136 - loss 0.77494590 - samples/sec: 11.52 - lr: 0.000004
2021-10-19 14:16:38,404 epoch 7 - iter 3130/3136 - loss 0.77602157 - samples/sec: 11.29 - lr: 0.000004
2021-10-19 14:16:40,395 ----------------------------------------------------------------------------------------------------
2021-10-19 14:16:40,395 EPOCH 7 done: loss 0.7755 - lr 0.0000036
2021-10-19 14:17:47,303 DEV : loss 1.1279035806655884 - score 0.7605
2021-10-19 14:17:47,311 BAD EPOCHS (no improvement): 4
2021-10-19 14:17:47,314 ----------------------------------------------------------------------------------------------------
2021-10-19 14:19:35,674 epoch 8 - iter 313/3136 - loss 0.75954571 - samples/sec: 11.55 - lr: 0.000004
2021-10-19 14:21:23,700 epoch 8 - iter 626/3136 - loss 0.74557186 - samples/sec: 11.59 - lr: 0.000004
2021-10-19 14:23:15,737 epoch 8 - iter 939/3136 - loss 0.70635414 - samples/sec: 11.18 - lr: 0.000004
2021-10-19 14:25:06,946 epoch 8 - iter 1252/3136 - loss 0.71264747 - samples/sec: 11.26 - lr: 0.000003
2021-10-19 14:26:55,860 epoch 8 - iter 1565/3136 - loss 0.71416868 - samples/sec: 11.50 - lr: 0.000003
2021-10-19 14:28:45,314 epoch 8 - iter 1878/3136 - loss 0.71562927 - samples/sec: 11.44 - lr: 0.000003
2021-10-19 14:30:35,192 epoch 8 - iter 2191/3136 - loss 0.72265991 - samples/sec: 11.40 - lr: 0.000003
2021-10-19 14:32:23,592 epoch 8 - iter 2504/3136 - loss 0.72601125 - samples/sec: 11.55 - lr: 0.000003
2021-10-19 14:34:14,320 epoch 8 - iter 2817/3136 - loss 0.72878143 - samples/sec: 11.31 - lr: 0.000003
2021-10-19 14:36:05,228 epoch 8 - iter 3130/3136 - loss 0.73056068 - samples/sec: 11.29 - lr: 0.000003
2021-10-19 14:36:07,476 ----------------------------------------------------------------------------------------------------
2021-10-19 14:36:07,476 EPOCH 8 done: loss 0.7299 - lr 0.0000033
2021-10-19 14:37:12,646 DEV : loss 1.1165168285369873 - score 0.7672
2021-10-19 14:37:12,654 BAD EPOCHS (no improvement): 4
2021-10-19 14:37:12,657 ----------------------------------------------------------------------------------------------------
2021-10-19 14:39:00,958 epoch 9 - iter 313/3136 - loss 0.61252416 - samples/sec: 11.56 - lr: 0.000003
2021-10-19 14:40:48,963 epoch 9 - iter 626/3136 - loss 0.61557962 - samples/sec: 11.59 - lr: 0.000003
2021-10-19 14:42:38,567 epoch 9 - iter 939/3136 - loss 0.63763907 - samples/sec: 11.42 - lr: 0.000003
2021-10-19 14:44:27,835 epoch 9 - iter 1252/3136 - loss 0.65210372 - samples/sec: 11.46 - lr: 0.000003
2021-10-19 14:46:17,130 epoch 9 - iter 1565/3136 - loss 0.65582704 - samples/sec: 11.46 - lr: 0.000003
2021-10-19 14:48:05,362 epoch 9 - iter 1878/3136 - loss 0.65049666 - samples/sec: 11.57 - lr: 0.000003
2021-10-19 14:49:54,876 epoch 9 - iter 2191/3136 - loss 0.65054391 - samples/sec: 11.43 - lr: 0.000003
2021-10-19 14:51:43,760 epoch 9 - iter 2504/3136 - loss 0.65130829 - samples/sec: 11.50 - lr: 0.000003
2021-10-19 14:53:32,408 epoch 9 - iter 2817/3136 - loss 0.66470161 - samples/sec: 11.52 - lr: 0.000003
2021-10-19 14:55:22,562 epoch 9 - iter 3130/3136 - loss 0.65779040 - samples/sec: 11.37 - lr: 0.000003
2021-10-19 14:55:24,657 ----------------------------------------------------------------------------------------------------
2021-10-19 14:55:24,657 EPOCH 9 done: loss 0.6580 - lr 0.0000029
2021-10-19 14:56:30,206 DEV : loss 1.120398759841919 - score 0.7726
2021-10-19 14:56:30,215 BAD EPOCHS (no improvement): 4
2021-10-19 14:56:30,231 ----------------------------------------------------------------------------------------------------
2021-10-19 14:58:19,165 epoch 10 - iter 313/3136 - loss 0.59319327 - samples/sec: 11.49 - lr: 0.000003
2021-10-19 15:00:07,138 epoch 10 - iter 626/3136 - loss 0.60232540 - samples/sec: 11.60 - lr: 0.000003
2021-10-19 15:01:56,012 epoch 10 - iter 939/3136 - loss 0.60300348 - samples/sec: 11.50 - lr: 0.000003
2021-10-19 15:03:45,008 epoch 10 - iter 1252/3136 - loss 0.61746573 - samples/sec: 11.49 - lr: 0.000003
2021-10-19 15:05:34,874 epoch 10 - iter 1565/3136 - loss 0.61282132 - samples/sec: 11.40 - lr: 0.000003
2021-10-19 15:07:23,777 epoch 10 - iter 1878/3136 - loss 0.62476071 - samples/sec: 11.50 - lr: 0.000003
2021-10-19 15:09:12,510 epoch 10 - iter 2191/3136 - loss 0.62306651 - samples/sec: 11.52 - lr: 0.000003
2021-10-19 15:11:01,336 epoch 10 - iter 2504/3136 - loss 0.63084384 - samples/sec: 11.51 - lr: 0.000003
2021-10-19 15:12:52,654 epoch 10 - iter 2817/3136 - loss 0.63502896 - samples/sec: 11.25 - lr: 0.000003
2021-10-19 15:14:41,487 epoch 10 - iter 3130/3136 - loss 0.63704817 - samples/sec: 11.50 - lr: 0.000003
2021-10-19 15:14:43,493 ----------------------------------------------------------------------------------------------------
2021-10-19 15:14:43,494 EPOCH 10 done: loss 0.6363 - lr 0.0000025
2021-10-19 15:15:49,013 DEV : loss 1.1451799869537354 - score 0.7727
2021-10-19 15:15:49,021 BAD EPOCHS (no improvement): 4
2021-10-19 15:15:49,069 ----------------------------------------------------------------------------------------------------
2021-10-19 15:17:41,248 epoch 11 - iter 313/3136 - loss 0.57314319 - samples/sec: 11.16 - lr: 0.000002
2021-10-19 15:19:33,990 epoch 11 - iter 626/3136 - loss 0.57950709 - samples/sec: 11.11 - lr: 0.000002
2021-10-19 15:21:23,290 epoch 11 - iter 939/3136 - loss 0.57939093 - samples/sec: 11.46 - lr: 0.000002
2021-10-19 15:23:16,668 epoch 11 - iter 1252/3136 - loss 0.58388687 - samples/sec: 11.04 - lr: 0.000002
2021-10-19 15:25:05,234 epoch 11 - iter 1565/3136 - loss 0.58734669 - samples/sec: 11.53 - lr: 0.000002
2021-10-19 15:26:54,145 epoch 11 - iter 1878/3136 - loss 0.59725692 - samples/sec: 11.50 - lr: 0.000002
2021-10-19 15:28:41,975 epoch 11 - iter 2191/3136 - loss 0.59690343 - samples/sec: 11.61 - lr: 0.000002
2021-10-19 15:30:30,487 epoch 11 - iter 2504/3136 - loss 0.59713616 - samples/sec: 11.54 - lr: 0.000002
2021-10-19 15:32:19,364 epoch 11 - iter 2817/3136 - loss 0.59423647 - samples/sec: 11.50 - lr: 0.000002
2021-10-19 15:34:09,103 epoch 11 - iter 3130/3136 - loss 0.59340589 - samples/sec: 11.41 - lr: 0.000002
2021-10-19 15:34:11,189 ----------------------------------------------------------------------------------------------------
2021-10-19 15:34:11,189 EPOCH 11 done: loss 0.5940 - lr 0.0000021
2021-10-19 15:35:16,130 DEV : loss 1.1441454887390137 - score 0.7774
2021-10-19 15:35:16,139 BAD EPOCHS (no improvement): 4
2021-10-19 15:35:16,141 ----------------------------------------------------------------------------------------------------
2021-10-19 15:37:06,229 epoch 12 - iter 313/3136 - loss 0.57310292 - samples/sec: 11.37 - lr: 0.000002
2021-10-19 15:38:55,092 epoch 12 - iter 626/3136 - loss 0.54613128 - samples/sec: 11.50 - lr: 0.000002
2021-10-19 15:40:43,940 epoch 12 - iter 939/3136 - loss 0.57856051 - samples/sec: 11.50 - lr: 0.000002
2021-10-19 15:42:37,760 epoch 12 - iter 1252/3136 - loss 0.57999485 - samples/sec: 11.00 - lr: 0.000002
2021-10-19 15:44:36,556 epoch 12 - iter 1565/3136 - loss 0.59760308 - samples/sec: 10.54 - lr: 0.000002
2021-10-19 15:46:25,347 epoch 12 - iter 1878/3136 - loss 0.58610449 - samples/sec: 11.51 - lr: 0.000002
2021-10-19 15:48:13,290 epoch 12 - iter 2191/3136 - loss 0.58182412 - samples/sec: 11.60 - lr: 0.000002
2021-10-19 15:50:01,402 epoch 12 - iter 2504/3136 - loss 0.58516974 - samples/sec: 11.58 - lr: 0.000002
2021-10-19 15:51:56,470 epoch 12 - iter 2817/3136 - loss 0.59195675 - samples/sec: 10.88 - lr: 0.000002
2021-10-19 15:53:51,578 epoch 12 - iter 3130/3136 - loss 0.59043599 - samples/sec: 10.88 - lr: 0.000002
2021-10-19 15:53:53,536 ----------------------------------------------------------------------------------------------------
2021-10-19 15:53:53,536 EPOCH 12 done: loss 0.5905 - lr 0.0000017
2021-10-19 15:54:58,656 DEV : loss 1.181410789489746 - score 0.7778
2021-10-19 15:54:58,664 BAD EPOCHS (no improvement): 4
2021-10-19 15:54:58,668 ----------------------------------------------------------------------------------------------------
2021-10-19 15:56:48,958 epoch 13 - iter 313/3136 - loss 0.49057499 - samples/sec: 11.35 - lr: 0.000002
2021-10-19 15:58:37,252 epoch 13 - iter 626/3136 - loss 0.51022287 - samples/sec: 11.56 - lr: 0.000002
2021-10-19 16:00:26,007 epoch 13 - iter 939/3136 - loss 0.51519188 - samples/sec: 11.51 - lr: 0.000002
2021-10-19 16:02:15,325 epoch 13 - iter 1252/3136 - loss 0.51797793 - samples/sec: 11.45 - lr: 0.000002
2021-10-19 16:04:04,353 epoch 13 - iter 1565/3136 - loss 0.52966261 - samples/sec: 11.48 - lr: 0.000002
2021-10-19 16:05:53,252 epoch 13 - iter 1878/3136 - loss 0.53636769 - samples/sec: 11.50 - lr: 0.000002
2021-10-19 16:07:41,733 epoch 13 - iter 2191/3136 - loss 0.53664734 - samples/sec: 11.54 - lr: 0.000001
2021-10-19 16:09:30,927 epoch 13 - iter 2504/3136 - loss 0.53587633 - samples/sec: 11.47 - lr: 0.000001
2021-10-19 16:11:19,158 epoch 13 - iter 2817/3136 - loss 0.53724617 - samples/sec: 11.57 - lr: 0.000001
2021-10-19 16:13:09,735 epoch 13 - iter 3130/3136 - loss 0.54006514 - samples/sec: 11.32 - lr: 0.000001
2021-10-19 16:13:11,781 ----------------------------------------------------------------------------------------------------
2021-10-19 16:13:11,781 EPOCH 13 done: loss 0.5415 - lr 0.0000014
2021-10-19 16:14:16,923 DEV : loss 1.1777759790420532 - score 0.7807
2021-10-19 16:14:16,931 BAD EPOCHS (no improvement): 4
2021-10-19 16:14:16,959 ----------------------------------------------------------------------------------------------------
2021-10-19 16:16:04,542 epoch 14 - iter 313/3136 - loss 0.50705330 - samples/sec: 11.64 - lr: 0.000001
2021-10-19 16:17:54,609 epoch 14 - iter 626/3136 - loss 0.53738986 - samples/sec: 11.38 - lr: 0.000001
2021-10-19 16:19:43,206 epoch 14 - iter 939/3136 - loss 0.51432112 - samples/sec: 11.53 - lr: 0.000001
2021-10-19 16:21:32,345 epoch 14 - iter 1252/3136 - loss 0.50331723 - samples/sec: 11.47 - lr: 0.000001
2021-10-19 16:23:20,722 epoch 14 - iter 1565/3136 - loss 0.50504932 - samples/sec: 11.55 - lr: 0.000001
2021-10-19 16:25:11,135 epoch 14 - iter 1878/3136 - loss 0.50371073 - samples/sec: 11.34 - lr: 0.000001
2021-10-19 16:26:59,014 epoch 14 - iter 2191/3136 - loss 0.50495277 - samples/sec: 11.61 - lr: 0.000001
2021-10-19 16:28:47,250 epoch 14 - iter 2504/3136 - loss 0.50959429 - samples/sec: 11.57 - lr: 0.000001
2021-10-19 16:30:39,754 epoch 14 - iter 2817/3136 - loss 0.51266317 - samples/sec: 11.13 - lr: 0.000001
2021-10-19 16:32:29,140 epoch 14 - iter 3130/3136 - loss 0.51278078 - samples/sec: 11.45 - lr: 0.000001
2021-10-19 16:32:31,197 ----------------------------------------------------------------------------------------------------
2021-10-19 16:32:31,198 EPOCH 14 done: loss 0.5135 - lr 0.0000010
2021-10-19 16:33:36,200 DEV : loss 1.2026926279067993 - score 0.7803
2021-10-19 16:33:36,209 BAD EPOCHS (no improvement): 4
2021-10-19 16:33:36,214 ----------------------------------------------------------------------------------------------------
2021-10-19 16:35:24,385 epoch 15 - iter 313/3136 - loss 0.53411693 - samples/sec: 11.58 - lr: 0.000001
2021-10-19 16:37:17,100 epoch 15 - iter 626/3136 - loss 0.52443551 - samples/sec: 11.11 - lr: 0.000001
2021-10-19 16:39:06,628 epoch 15 - iter 939/3136 - loss 0.52140448 - samples/sec: 11.43 - lr: 0.000001
2021-10-19 16:40:54,624 epoch 15 - iter 1252/3136 - loss 0.51681837 - samples/sec: 11.59 - lr: 0.000001
2021-10-19 16:42:42,931 epoch 15 - iter 1565/3136 - loss 0.50978400 - samples/sec: 11.56 - lr: 0.000001
2021-10-19 16:44:33,542 epoch 15 - iter 1878/3136 - loss 0.50723413 - samples/sec: 11.32 - lr: 0.000001
2021-10-19 16:46:24,468 epoch 15 - iter 2191/3136 - loss 0.50143176 - samples/sec: 11.29 - lr: 0.000001
2021-10-19 16:48:15,069 epoch 15 - iter 2504/3136 - loss 0.50841671 - samples/sec: 11.32 - lr: 0.000001
2021-10-19 16:50:12,887 epoch 15 - iter 2817/3136 - loss 0.51112055 - samples/sec: 10.63 - lr: 0.000001
2021-10-19 16:52:02,290 epoch 15 - iter 3130/3136 - loss 0.51295940 - samples/sec: 11.44 - lr: 0.000001
2021-10-19 16:52:04,280 ----------------------------------------------------------------------------------------------------
2021-10-19 16:52:04,281 EPOCH 15 done: loss 0.5135 - lr 0.0000007
2021-10-19 16:53:09,482 DEV : loss 1.2040660381317139 - score 0.7826
2021-10-19 16:53:09,490 BAD EPOCHS (no improvement): 4
2021-10-19 16:53:09,494 ----------------------------------------------------------------------------------------------------
2021-10-19 16:54:56,343 epoch 16 - iter 313/3136 - loss 0.58573253 - samples/sec: 11.72 - lr: 0.000001
2021-10-19 16:56:45,229 epoch 16 - iter 626/3136 - loss 0.51827828 - samples/sec: 11.50 - lr: 0.000001
2021-10-19 16:58:33,435 epoch 16 - iter 939/3136 - loss 0.51680811 - samples/sec: 11.57 - lr: 0.000001
2021-10-19 17:00:22,690 epoch 16 - iter 1252/3136 - loss 0.50513786 - samples/sec: 11.46 - lr: 0.000001
2021-10-19 17:02:11,492 epoch 16 - iter 1565/3136 - loss 0.50885042 - samples/sec: 11.51 - lr: 0.000001
2021-10-19 17:03:59,707 epoch 16 - iter 1878/3136 - loss 0.50316887 - samples/sec: 11.57 - lr: 0.000001
2021-10-19 17:05:47,113 epoch 16 - iter 2191/3136 - loss 0.49958362 - samples/sec: 11.66 - lr: 0.000001
2021-10-19 17:07:37,626 epoch 16 - iter 2504/3136 - loss 0.50383128 - samples/sec: 11.33 - lr: 0.000001
2021-10-19 17:09:26,903 epoch 16 - iter 2817/3136 - loss 0.49994214 - samples/sec: 11.46 - lr: 0.000001
2021-10-19 17:11:16,285 epoch 16 - iter 3130/3136 - loss 0.49406595 - samples/sec: 11.45 - lr: 0.000000
2021-10-19 17:11:18,519 ----------------------------------------------------------------------------------------------------
2021-10-19 17:11:18,519 EPOCH 16 done: loss 0.4935 - lr 0.0000005
2021-10-19 17:12:35,347 DEV : loss 1.2206780910491943 - score 0.7815
2021-10-19 17:12:35,355 BAD EPOCHS (no improvement): 4
2021-10-19 17:12:35,359 ----------------------------------------------------------------------------------------------------
2021-10-19 17:14:25,678 epoch 17 - iter 313/3136 - loss 0.53820454 - samples/sec: 11.35 - lr: 0.000000
2021-10-19 17:16:21,171 epoch 17 - iter 626/3136 - loss 0.52175674 - samples/sec: 10.84 - lr: 0.000000
2021-10-19 17:18:08,811 epoch 17 - iter 939/3136 - loss 0.53485034 - samples/sec: 11.63 - lr: 0.000000
2021-10-19 17:19:57,298 epoch 17 - iter 1252/3136 - loss 0.53024880 - samples/sec: 11.54 - lr: 0.000000
2021-10-19 17:21:47,760 epoch 17 - iter 1565/3136 - loss 0.52701228 - samples/sec: 11.34 - lr: 0.000000
2021-10-19 17:23:39,056 epoch 17 - iter 1878/3136 - loss 0.52664675 - samples/sec: 11.25 - lr: 0.000000
2021-10-19 17:25:28,208 epoch 17 - iter 2191/3136 - loss 0.52189134 - samples/sec: 11.47 - lr: 0.000000
2021-10-19 17:27:17,887 epoch 17 - iter 2504/3136 - loss 0.52272720 - samples/sec: 11.42 - lr: 0.000000
2021-10-19 17:29:08,051 epoch 17 - iter 2817/3136 - loss 0.51657672 - samples/sec: 11.37 - lr: 0.000000
2021-10-19 17:31:03,164 epoch 17 - iter 3130/3136 - loss 0.51902216 - samples/sec: 10.88 - lr: 0.000000
2021-10-19 17:31:05,235 ----------------------------------------------------------------------------------------------------
2021-10-19 17:31:05,235 EPOCH 17 done: loss 0.5188 - lr 0.0000003
2021-10-19 17:32:10,715 DEV : loss 1.2250354290008545 - score 0.7819
2021-10-19 17:32:10,723 BAD EPOCHS (no improvement): 4
2021-10-19 17:32:10,729 ----------------------------------------------------------------------------------------------------
2021-10-19 17:33:58,795 epoch 18 - iter 313/3136 - loss 0.53548519 - samples/sec: 11.59 - lr: 0.000000
2021-10-19 17:35:47,810 epoch 18 - iter 626/3136 - loss 0.52301208 - samples/sec: 11.49 - lr: 0.000000
2021-10-19 17:37:37,488 epoch 18 - iter 939/3136 - loss 0.50545174 - samples/sec: 11.42 - lr: 0.000000
2021-10-19 17:40:19,251 epoch 18 - iter 1252/3136 - loss 0.50133605 - samples/sec: 7.74 - lr: 0.000000
2021-10-19 17:42:59,047 epoch 18 - iter 1565/3136 - loss 0.50838808 - samples/sec: 7.84 - lr: 0.000000
2021-10-19 17:46:04,551 epoch 18 - iter 1878/3136 - loss 0.49839976 - samples/sec: 6.75 - lr: 0.000000
2021-10-19 17:49:03,468 epoch 18 - iter 2191/3136 - loss 0.49082145 - samples/sec: 7.00 - lr: 0.000000
2021-10-19 17:51:51,579 epoch 18 - iter 2504/3136 - loss 0.48823537 - samples/sec: 7.45 - lr: 0.000000
2021-10-19 17:53:58,474 epoch 18 - iter 2817/3136 - loss 0.48743949 - samples/sec: 9.87 - lr: 0.000000
2021-10-19 17:55:55,884 epoch 18 - iter 3130/3136 - loss 0.48389656 - samples/sec: 10.66 - lr: 0.000000
2021-10-19 17:55:58,092 ----------------------------------------------------------------------------------------------------
2021-10-19 17:55:58,092 EPOCH 18 done: loss 0.4839 - lr 0.0000001
2021-10-19 17:57:10,983 DEV : loss 1.225695013999939 - score 0.783
2021-10-19 17:57:10,991 BAD EPOCHS (no improvement): 4
2021-10-19 17:57:10,996 ----------------------------------------------------------------------------------------------------
2021-10-19 17:59:09,407 epoch 19 - iter 313/3136 - loss 0.46479846 - samples/sec: 10.57 - lr: 0.000000
2021-10-19 18:01:02,074 epoch 19 - iter 626/3136 - loss 0.45656571 - samples/sec: 11.11 - lr: 0.000000
2021-10-19 18:03:03,570 epoch 19 - iter 939/3136 - loss 0.49870407 - samples/sec: 10.31 - lr: 0.000000
2021-10-19 18:05:26,821 epoch 19 - iter 1252/3136 - loss 0.49930007 - samples/sec: 8.74 - lr: 0.000000
2021-10-19 18:07:42,096 epoch 19 - iter 1565/3136 - loss 0.48806497 - samples/sec: 9.26 - lr: 0.000000
2021-10-19 18:09:44,766 epoch 19 - iter 1878/3136 - loss 0.48464517 - samples/sec: 10.21 - lr: 0.000000
2021-10-19 18:11:46,322 epoch 19 - iter 2191/3136 - loss 0.48976799 - samples/sec: 10.30 - lr: 0.000000
2021-10-19 18:13:47,529 epoch 19 - iter 2504/3136 - loss 0.49018363 - samples/sec: 10.33 - lr: 0.000000
2021-10-19 18:15:48,238 epoch 19 - iter 2817/3136 - loss 0.49665736 - samples/sec: 10.37 - lr: 0.000000
2021-10-19 18:17:49,219 epoch 19 - iter 3130/3136 - loss 0.49815185 - samples/sec: 10.35 - lr: 0.000000
2021-10-19 18:17:51,686 ----------------------------------------------------------------------------------------------------
2021-10-19 18:17:51,686 EPOCH 19 done: loss 0.4991 - lr 0.0000000
2021-10-19 18:19:03,318 DEV : loss 1.2262133359909058 - score 0.7833
2021-10-19 18:19:03,327 BAD EPOCHS (no improvement): 4
2021-10-19 18:19:03,347 ----------------------------------------------------------------------------------------------------
2021-10-19 18:20:57,719 epoch 20 - iter 313/3136 - loss 0.46569044 - samples/sec: 10.95 - lr: 0.000000
2021-10-19 18:22:52,510 epoch 20 - iter 626/3136 - loss 0.48987185 - samples/sec: 10.91 - lr: 0.000000
2021-10-19 18:24:46,433 epoch 20 - iter 939/3136 - loss 0.49162425 - samples/sec: 10.99 - lr: 0.000000
2021-10-19 18:26:40,552 epoch 20 - iter 1252/3136 - loss 0.48345433 - samples/sec: 10.97 - lr: 0.000000
2021-10-19 18:28:34,754 epoch 20 - iter 1565/3136 - loss 0.49233310 - samples/sec: 10.96 - lr: 0.000000
2021-10-19 18:31:02,249 epoch 20 - iter 1878/3136 - loss 0.49407599 - samples/sec: 8.49 - lr: 0.000000
2021-10-19 18:33:31,273 epoch 20 - iter 2191/3136 - loss 0.49004038 - samples/sec: 8.40 - lr: 0.000000
2021-10-19 18:35:32,808 epoch 20 - iter 2504/3136 - loss 0.49364530 - samples/sec: 10.30 - lr: 0.000000
2021-10-19 18:37:35,295 epoch 20 - iter 2817/3136 - loss 0.49476049 - samples/sec: 10.22 - lr: 0.000000
2021-10-19 18:39:36,086 epoch 20 - iter 3130/3136 - loss 0.49111563 - samples/sec: 10.37 - lr: 0.000000
2021-10-19 18:39:38,209 ----------------------------------------------------------------------------------------------------
2021-10-19 18:39:38,210 EPOCH 20 done: loss 0.4911 - lr 0.0000000
2021-10-19 18:40:44,035 DEV : loss 1.2254611253738403 - score 0.783
2021-10-19 18:40:44,043 BAD EPOCHS (no improvement): 4
2021-10-19 18:41:16,005 ----------------------------------------------------------------------------------------------------
2021-10-19 18:41:17,767 Testing using best model ...
2021-10-19 18:42:57,028 	0.7848
2021-10-19 18:42:57,029 
Results:
- F-score (micro): 0.7848
- F-score (macro): 0.2741
- Accuracy (incl. no class): 0.7848

By class:
                    precision    recall  f1-score   support

            1,NOUN     0.9121    0.9168    0.9144      3497
   1,NOUN,ARGM-ADJ     0.7095    0.7605    0.7341       167
        1,AUX,ARG1     0.9318    0.9295    0.9306       397
             2,ADJ     0.0000    0.0000    0.0000        23
             1,ADJ     0.8587    0.8977    0.8778       528
    1,ADJ,ARGM-EXT     0.7463    0.8621    0.8000        58
       -1,AUX,ARG2     0.9138    0.9138    0.9138       429
            -1,ADJ     0.7466    0.6995    0.7223       396
           -1,ROOT     0.8931    0.8754    0.8842      1709
          -1,PUNCT     0.8671    0.8746    0.8708      2081
       1,VERB,ARG1     0.7977    0.8193    0.8083       332
           1,PROPN     0.8192    0.7505    0.7833       501
      -1,VERB,ARG2     0.7260    0.7420    0.7339       407
   1,VERB,ARGM-DIS     0.7521    0.8980    0.8186        98
       1,VERB,ARG0     0.9473    0.9624    0.9548      1064
            4,NOUN     0.7143    0.5556    0.6250        27
            3,NOUN     0.5258    0.5604    0.5426        91
            2,NOUN     0.7343    0.8077    0.7692       520
           -1,NOUN     0.7542    0.7851    0.7694      1810
      -1,VERB,ARG1     0.9171    0.9271    0.9220      1563
      -1,VERB,ARG4     0.7018    0.7273    0.7143        55
          -5,PUNCT     0.3571    0.2985    0.3252        67
            -3,ADJ     0.3659    0.4839    0.4167        31
          -2,PUNCT     0.7376    0.8013    0.7681       614
    1,AUX,ARGM-DIS     0.4762    0.5882    0.5263        17
    1,AUX,ARGM-ADV     0.6667    0.6364    0.6512        22
       -1,AUX,ARG1     0.7736    0.8039    0.7885        51
           -1,PRON     0.8033    0.7424    0.7717       132
   -1,AUX,ARGM-ADV     0.5306    0.6667    0.5909        39
    1,AUX,ARGM-TMP     0.8947    0.8947    0.8947        19
  1,AUX,R-ARGM-TMP     1.0000    0.0000    0.0000         1
           -3,NOUN     0.3430    0.2980    0.3189       198
          -6,PUNCT     0.1389    0.3226    0.1942        31
       1,NOUN,ARG2     0.3158    0.3000    0.3077        20
       1,NOUN,ARG0     0.7798    0.7589    0.7692       112
            1,VERB     0.8925    0.8949    0.8937       761
       1,NOUN,ARG1     0.6571    0.6509    0.6540       106
          -1,PROPN     0.7618    0.6821    0.7197       497
      -1,NOUN,ARG2     0.4000    0.4667    0.4308        30
      -1,NOUN,ARG1     0.7424    0.7206    0.7313       136
   1,VERB,ARGM-MOD     0.9621    0.9871    0.9744       309
           -1,VERB     0.5922    0.6387    0.6146       191
   1,VERB,ARGM-ADV     0.8258    0.7819    0.8033       188
  -1,VERB,ARGM-ADV     0.4538    0.5567    0.5000        97
  -1,VERB,ARGM-TMP     0.8560    0.8943    0.8748       246
    1,ADJ,ARGM-ADV     0.0000    0.0000    0.0000         6
   -1,AUX,ARGM-LOC     0.5714    0.6667    0.6154         6
          -4,PUNCT     0.3312    0.4595    0.3849       111
            1,PRON     0.8889    0.9720    0.9286       107
            -1,ADV     0.6781    0.5893    0.6306       168
      -1,VERB,ARG3     0.4902    0.5000    0.4950        50
     1,VERB,R-ARG0     0.9423    0.9800    0.9608        50
   1,VERB,ARGM-NEG     0.9517    0.9787    0.9650       141
      -2,VERB,ARG1     0.5833    0.5833    0.5833        36
         -1,VERB,V     1.0000    0.0000    0.0000         3
             1,ADV     0.8318    0.8641    0.8476       103
          -2,PROPN     0.6652    0.6743    0.6697       218
   2,NOUN,ARGM-ADJ     0.5455    0.7059    0.6154        17
       1,NOUN,ARG3     1.0000    0.0000    0.0000         4
   1,NOUN,ARGM-MNR     0.5455    0.1935    0.2857        31
    -1,VERB,C-ARG1     0.7692    0.6818    0.7229        44
  -2,VERB,ARGM-LOC     1.0000    0.0000    0.0000         8
  -1,VERB,ARGM-LOC     0.6203    0.7717    0.6877       127
  -1,VERB,ARGM-GOL     0.4000    0.1538    0.2222        13
  -1,VERB,ARGM-PRP     0.6393    0.7222    0.6783        54
   1,NOUN,ARGM-LVB     0.7317    0.7143    0.7229        42
  -1,VERB,ARGM-PRR     0.7273    0.7742    0.7500        62
            -1,NUM     0.7322    0.5852    0.6505       229
     1,VERB,R-ARG1     0.6667    0.9333    0.7778        30
   1,NOUN,ARGM-LOC     1.0000    0.0000    0.0000         3
   1,VERB,ARGM-LOC     0.6842    0.6190    0.6500        21
      -2,NOUN,ARG1     0.4500    0.5000    0.4737        18
           -2,NOUN     0.5115    0.5645    0.5367       434
             1,NUM     0.7313    0.6901    0.7101        71
       2,VERB,ARG0     0.7308    0.9048    0.8085        21
  -1,NOUN,ARGM-PRD     0.3333    0.3750    0.3529         8
      -1,VERB,ARG0     0.7800    0.7800    0.7800        50
       3,VERB,ARG0     1.0000    0.0000    0.0000         5
     3,VERB,R-ARG0     1.0000    0.0000    0.0000         1
   2,VERB,ARGM-ADV     0.4000    0.5455    0.4615        11
           -4,NOUN     0.3529    0.2308    0.2791       104
   -1,AUX,ARGM-NEG     0.9130    0.9130    0.9130        23
       2,NOUN,ARG1     0.2857    0.2857    0.2857         7
       -1,ADJ,ARG1     0.6250    0.5769    0.6000        26
          -3,PUNCT     0.5780    0.6198    0.5982       263
  -1,VERB,ARGM-PRD     0.2500    0.1250    0.1667        16
           -2,PRON     0.6216    0.6389    0.6301        36
  -1,NOUN,ARGM-ADJ     0.4839    0.5000    0.4918        30
       -1,ADJ,ARG0     0.5000    0.4667    0.4828        15
   1,VERB,ARGM-TMP     0.8187    0.9850    0.8942       133
          -8,PUNCT     1.0000    0.0000    0.0000        14
          -9,PUNCT     1.0000    0.0000    0.0000         6
         <UNKNOWN>     0.0000    1.0000    0.0000         0
         -10,PUNCT     1.0000    0.0000    0.0000         4
         -11,PUNCT     1.0000    0.0000    0.0000         4
  -1,VERB,ARGM-MNR     0.5714    0.6000    0.5854        60
           2,PROPN     0.6789    0.5827    0.6271       127
   1,NOUN,ARGM-NEG     0.6842    0.7647    0.7222        17
        2,AUX,ARG1     0.8571    0.9167    0.8859        72
           4,PROPN     0.7143    0.3846    0.5000        13
           3,PROPN     0.6400    0.4444    0.5246        36
       -1,ADJ,ARG2     0.5417    0.7027    0.6118        37
      -2,NOUN,ARG2     0.0000    0.0000    0.0000         3
            -2,ADJ     0.5052    0.4414    0.4712       111
  -2,VERB,ARGM-ADV     0.0000    0.0000    0.0000        13
  -1,VERB,ARGM-COM     0.6154    0.7273    0.6667        11
   1,NOUN,ARGM-TMP     0.6250    0.5357    0.5769        28
  -1,VERB,ARGM-CAU     0.3667    0.7857    0.5000        14
            2,VERB     0.4286    0.1667    0.2400        18
  -2,VERB,ARGM-TMP     0.3333    0.0714    0.1176        14
      -1,NOUN,ARG0     0.5000    0.6250    0.5556        16
  -2,NOUN,ARGM-PRD     1.0000    0.0000    0.0000         1
           -2,VERB     0.3492    0.4151    0.3793        53
  -1,NOUN,ARGM-TMP     0.4545    0.7692    0.5714        13
          -7,PUNCT     1.0000    0.0000    0.0000        24
  -1,VERB,ARGM-DIR     0.5333    0.4103    0.4638        39
       -2,ADJ,ARG2     1.0000    0.0000    0.0000         2
          -3,PROPN     0.5556    0.6475    0.5980       139
  -1,NOUN,ARGM-LOC     0.3750    0.5000    0.4286        18
             1,SYM     0.7742    0.9600    0.8571        25
  -4,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
      1,AUX,R-ARG1     0.5455    0.6000    0.5714        10
        1,ADJ,ARG2     1.0000    1.0000    1.0000         1
  -2,NOUN,ARGM-ADJ     1.0000    0.0000    0.0000         3
           -5,NOUN     0.1724    0.1136    0.1370        44
           -6,NOUN     0.0000    0.0000    0.0000        20
 1,VERB,R-ARGM-MNR     1.0000    0.0000    0.0000         8
          -4,PROPN     0.5354    0.7067    0.6092        75
      -4,NOUN,ARG1     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-MNR     0.6552    0.6552    0.6552        29
         -19,PUNCT     1.0000    0.0000    0.0000         1
  -1,VERB,ARGM-EXT     0.5385    0.7778    0.6364         9
             1,DET     0.7333    0.6875    0.7097        16
            -1,DET     0.8864    0.5652    0.6903        69
 1,VERB,R-ARGM-LOC     0.7500    0.8571    0.8000         7
       2,NOUN,ARG0     0.2000    0.2000    0.2000        10
           -9,NOUN     1.0000    0.0000    0.0000         8
       -2,ADJ,ARG1     1.0000    0.0000    0.0000         3
       -3,AUX,ARG1     1.0000    0.0000    0.0000         1
            -6,ADJ     1.0000    0.0000    0.0000         3
            -4,ADJ     0.0000    0.0000    0.0000        11
       2,VERB,ARG1     0.5238    0.6111    0.5641        18
   1,VERB,ARGM-PRR     1.0000    0.3333    0.5000         3
  -1,NOUN,ARGM-PRP     1.0000    0.3333    0.5000         3
   -1,AUX,ARGM-PRD     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-ADV     1.0000    0.0000    0.0000         2
       1,VERB,ARG2     0.4000    0.2308    0.2927        26
   1,NOUN,ARGM-PRP     1.0000    0.0000    0.0000         2
  -2,NOUN,ARGM-TMP     1.0000    0.0000    0.0000         3
      -2,NOUN,ARG0     1.0000    0.0000    0.0000         4
            -1,SYM     0.5758    0.6230    0.5984        61
   1,VERB,ARGM-EXT     0.7895    0.6250    0.6977        24
     -1,ADJ,C-ARG1     0.3333    1.0000    0.5000         3
      -2,VERB,ARG2     1.0000    0.0000    0.0000        12
    2,AUX,ARGM-DIS     0.3333    0.3333    0.3333         3
    1,AUX,ARGM-MOD     0.9697    0.9412    0.9552        68
       -1,VERB,C-V     0.7647    0.8667    0.8125        15
   1,NOUN,ARGM-PRD     0.6364    0.7000    0.6667        10
      -3,NOUN,ARG3     1.0000    0.0000    0.0000         2
             2,NUM     1.0000    0.0000    0.0000         4
   3,VERB,ARGM-TMP     1.0000    0.0000    0.0000         2
   3,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
      -3,NOUN,ARG0     1.0000    0.0000    0.0000         1
       4,VERB,ARG0     1.0000    0.0000    0.0000         1
        1,AUX,ARG2     0.6970    0.8214    0.7541        28
      -3,NOUN,ARG1     0.0000    0.0000    0.0000         5
            3,VERB     1.0000    0.0000    0.0000         4
  -2,VERB,ARGM-MNR     1.0000    0.0000    0.0000         2
    -1,NOUN,R-ARG1     1.0000    0.0000    0.0000         1
      -3,VERB,ARG1     1.0000    0.0000    0.0000         6
  -1,VERB,ARGM-DIS     0.6000    0.4615    0.5217        13
           -5,VERB     1.0000    0.0000    0.0000         4
   -1,AUX,ARGM-TMP     0.6765    1.0000    0.8070        23
       5,VERB,ARG0     1.0000    0.0000    0.0000         1
      -3,VERB,ARG2     1.0000    0.0000    0.0000         3
       4,VERB,ARG1     1.0000    0.0000    0.0000         2
          -5,PROPN     0.3182    0.4667    0.3784        45
          -6,PROPN     0.1333    0.2000    0.1600        10
   3,VERB,ARGM-DIS     1.0000    0.0000    0.0000         2
   2,VERB,ARGM-DIS     0.3333    0.3333    0.3333         3
  -1,VERB,ARGM-NEG     0.5000    0.5000    0.5000         2
   1,VERB,ARGM-PRD     1.0000    0.0000    0.0000         1
       -1,ADJ,ARG3     1.0000    0.0000    0.0000         7
          -7,PROPN     0.0000    0.0000    0.0000        11
          -8,PROPN     0.0000    0.0000    0.0000         2
   2,NOUN,ARGM-GOL     1.0000    0.0000    0.0000         2
   2,VERB,ARGM-CAU     1.0000    0.0000    0.0000         3
    1,ADJ,ARGM-CXN     0.7143    1.0000    0.8333         5
             2,AUX     1.0000    0.0000    0.0000         1
             1,AUX     0.0000    0.0000    0.0000         1
 -1,ADJ,C-ARGM-CXN     0.5714    0.8000    0.6667         5
   1,NOUN,ARGM-DIS     0.5000    0.6667    0.5714         3
   -1,AUX,ARGM-MNR     1.0000    0.0000    0.0000         2
   1,NOUN,ARGM-ADV     1.0000    0.5000    0.6667         2
  -2,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         5
           -3,VERB     0.0000    0.0000    0.0000        15
   -1,AUX,ARGM-EXT     1.0000    0.0000    0.0000         3
   2,NOUN,ARGM-LOC     1.0000    0.0000    0.0000         2
 1,NOUN,R-ARGM-ADJ     1.0000    0.0000    0.0000         1
            1,INTJ     1.0000    0.0000    0.0000         4
           -1,INTJ     1.0000    0.0000    0.0000        10
   2,NOUN,ARGM-TMP     0.2857    0.4000    0.3333         5
   4,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
   1,NOUN,ARGM-EXT     0.6667    0.4000    0.5000         5
      -5,NOUN,ARG1     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-DIS     1.0000    0.0000    0.0000         2
       2,VERB,ARG2     1.0000    0.0000    0.0000         2
    -3,VERB,C-ARG1     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-CAU     0.6000    0.5455    0.5714        11
    -1,NOUN,C-ARG3     1.0000    0.0000    0.0000         1
    1,AUX,ARGM-NEG     0.8750    0.8750    0.8750         8
   -1,AUX,ARGM-CAU     1.0000    0.0000    0.0000         1
   -2,AUX,ARGM-DIS     1.0000    0.0000    0.0000         1
            -2,NUM     0.6410    0.3521    0.4545        71
  -1,NOUN,ARGM-LVB     0.3333    0.2500    0.2857         4
  -2,VERB,ARGM-DIS     1.0000    0.0000    0.0000         3
              -1,X     0.5844    0.7337    0.6506       184
            -2,DET     0.5714    0.9231    0.7059        13
            -2,ADV     1.0000    0.3000    0.4615        20
  -4,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
              -2,X     0.3000    0.0968    0.1463        31
            -2,SYM     1.0000    0.0000    0.0000        14
            -3,SYM     1.0000    0.0000    0.0000         1
            -3,NUM     0.2500    0.3333    0.2857        24
              -3,X     0.0000    1.0000    0.0000         0
   -1,AUX,ARGM-DIS     1.0000    0.0000    0.0000         6
   2,VERB,ARGM-MNR     1.0000    0.0000    0.0000         1
            -4,NUM     1.0000    0.0000    0.0000         7
            -5,NUM     1.0000    0.0000    0.0000         2
            -6,NUM     1.0000    0.0000    0.0000         4
            -7,NUM     1.0000    0.0000    0.0000         2
  -2,NOUN,ARGM-PRP     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-ADV     0.7692    0.8333    0.8000        12
             2,ADV     1.0000    0.0000    0.0000         5
        2,AUX,ARG2     1.0000    0.0000    0.0000         2
  -1,NOUN,ARGM-EXT     1.0000    0.5000    0.6667         2
      -1,VERB,ARG5     1.0000    0.0000    0.0000         1
               1,X     0.2000    0.5000    0.2857         4
       3,NOUN,ARG0     1.0000    0.0000    0.0000         2
               2,X     1.0000    0.0000    0.0000         2
   3,NOUN,ARGM-TMP     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         4
   -1,ADJ,ARGM-TMP     1.0000    0.0000    0.0000         1
   1,NOUN,ARGM-MOD     1.0000    0.5714    0.7273         7
      -4,NOUN,ARG4     1.0000    0.0000    0.0000         1
            -1,ADP     1.0000    0.0000    0.0000         7
            -2,ADP     1.0000    0.0000    0.0000         3
  -2,VERB,ARGM-GOL     1.0000    0.0000    0.0000         1
      -2,VERB,ARG0     1.0000    0.0000    0.0000         1
         -12,PUNCT     1.0000    0.0000    0.0000         1
      -2,NOUN,ARG3     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-TMP     1.0000    0.0000    0.0000         2
   2,VERB,ARGM-PRR     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-MOD     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-LVB     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-PRD     0.5000    1.0000    0.6667         1
  -1,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         8
           7,PROPN     1.0000    0.0000    0.0000         2
           6,PROPN     1.0000    0.0000    0.0000         1
           5,PROPN     1.0000    0.0000    0.0000         1
  -3,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         1
      1,ADJ,R-ARG1     1.0000    0.0000    0.0000         1
    1,ADJ,ARGM-MOD     1.0000    0.0000    0.0000         1
     1,VERB,R-ARG2     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-MOD     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-GOL     1.0000    0.0000    0.0000         2
          -9,PROPN     1.0000    0.0000    0.0000         6
 3,VERB,R-ARGM-ADV     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-COM     1.0000    0.0000    0.0000         2
   3,NOUN,ARGM-ADJ     1.0000    0.0000    0.0000         5
       3,VERB,ARG1     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-DIR     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-CAU     1.0000    0.0000    0.0000         2
  -3,VERB,ARGM-PRP     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         2
           -3,PRON     1.0000    0.0000    0.0000         7
          -1,SCONJ     1.0000    0.0000    0.0000         1
      2,AUX,R-ARG1     0.0000    1.0000    0.0000         0
   -1,AUX,ARGM-PRP     1.0000    0.0000    0.0000         4
           -4,VERB     1.0000    0.0000    0.0000         2
        1,ADJ,ARG1     1.0000    0.0000    0.0000         2
         -1,NOUN,V     1.0000    0.0000    0.0000         1
           -7,NOUN     0.0000    0.0000    0.0000         7
           -8,NOUN     0.0000    0.0000    0.0000         8
   2,VERB,ARGM-TMP     1.0000    0.3333    0.5000         3
    -1,VERB,C-ARG0     1.0000    0.0000    0.0000         2
         -10,PROPN     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-LVB     1.0000    0.0000    0.0000         5
   -2,AUX,ARGM-ADV     1.0000    0.0000    0.0000         3
            -5,ADJ     1.0000    0.0000    0.0000         4
    1,ADJ,ARGM-ADJ     1.0000    0.0000    0.0000         2
    -1,VERB,C-ARG2     0.8333    0.8333    0.8333         6
  -3,VERB,ARGM-LOC     1.0000    0.0000    0.0000         1
   5,VERB,ARGM-ADV     1.0000    0.0000    0.0000         1
            -1,AUX     1.0000    0.0000    0.0000         3
            -2,AUX     1.0000    0.0000    0.0000         1
   9,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         1
       3,NOUN,ARG1     1.0000    0.0000    0.0000         1
     1,NOUN,R-ARG0     1.0000    0.0000    0.0000         2
       -2,AUX,ARG1     1.0000    0.0000    0.0000         2
            5,NOUN     1.0000    0.0000    0.0000         4
      -5,VERB,ARG1     1.0000    0.0000    0.0000         1
   1,VERB,ARG1-DSP     1.0000    0.0000    0.0000         4
-1,VERB,C-ARG1-DSP     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-NEG     1.0000    0.0000    0.0000         1
            2,PRON     1.0000    0.3333    0.5000         3
      -4,VERB,ARG1     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-GOL     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-PRP     1.0000    0.0000    0.0000         2
       -3,ADJ,ARG0     1.0000    0.0000    0.0000         1
      -1,NOUN,ARG3     1.0000    0.0000    0.0000         2
   -2,AUX,ARGM-CAU     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-DIR     1.0000    0.0000    0.0000         4
   -1,ADJ,ARGM-EXT     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-CXN     0.8000    0.6667    0.7273         6
    2,AUX,ARGM-LOC     1.0000    0.0000    0.0000         1
   -3,ADJ,ARGM-CXN     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-DIS     0.6000    0.6000    0.6000         5
   2,NOUN,ARGM-NEG     1.0000    0.0000    0.0000         1
   1,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
             3,ADV     1.0000    0.0000    0.0000         1
      -2,VERB,ARG3     1.0000    0.0000    0.0000         2
             2,DET     1.0000    0.0000    0.0000         2
   4,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-DIR     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-PRP     1.0000    0.0000    0.0000         1
            -3,ADV     1.0000    0.0000    0.0000         1
-1,VERB,C-ARGM-LOC     1.0000    0.0000    0.0000         1
       1,VERB,ARGA     1.0000    0.0000    0.0000         2
  -3,VERB,ARGM-CAU     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-DIS     1.0000    0.0000    0.0000         1
            1,PART     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-PRP     1.0000    0.3333    0.5000         3
  -1,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
       1,VERB,ARG3     1.0000    0.0000    0.0000         2
   -1,AUX,ARGM-GOL     1.0000    0.0000    0.0000         2
    1,ADJ,ARGM-LVB     1.0000    0.0000    0.0000         1
    -2,VERB,C-ARG1     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-TMP     1.0000    0.0000    0.0000         1
             1,ADP     1.0000    0.2500    0.4000         4
     -1,AUX,C-ARG2     0.5000    1.0000    0.6667         1
    -1,VERB,C-ARG3     1.0000    0.0000    0.0000         1
    1,ADJ,ARGM-NEG     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-DIR     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-CAU     1.0000    0.0000    0.0000         3
       -2,ADJ,ARG0     1.0000    0.0000    0.0000         2
    -1,NOUN,C-ARG1     1.0000    0.0000    0.0000         1
       -2,AUX,ARG2     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-DIR     1.0000    0.0000    0.0000         1
     -2,ADJ,C-ARG1     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-PRR     1.0000    0.0000    0.0000         1
        3,AUX,ARG1     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-MOD     1.0000    1.0000    1.0000         1
    2,AUX,ARGM-NEG     1.0000    0.0000    0.0000         1

          accuracy                         0.7848     25096
         macro avg     0.7962    0.2890    0.2741     25096
      weighted avg     0.7955    0.7848    0.7788     25096

2021-10-19 18:42:57,029 ----------------------------------------------------------------------------------------------------
