2021-10-08 01:24:04,459 ----------------------------------------------------------------------------------------------------
2021-10-08 01:24:04,464 Model: "SequenceTagger(
  (embeddings): TransformerWordEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(28996, 1024, padding_idx=0)
        (position_embeddings): Embedding(512, 1024)
        (token_type_embeddings): Embedding(2, 1024)
        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=1024, out_features=1024, bias=True)
                (key): Linear(in_features=1024, out_features=1024, bias=True)
                (value): Linear(in_features=1024, out_features=1024, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=1024, out_features=1024, bias=True)
                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=1024, out_features=4096, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=4096, out_features=1024, bias=True)
              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=1024, out_features=1024, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (linear): Linear(in_features=1024, out_features=465, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-10-08 01:24:04,466 ----------------------------------------------------------------------------------------------------
2021-10-08 01:24:04,466 Corpus: "Corpus: 12543 train + 2002 dev + 2077 test sentences"
2021-10-08 01:24:04,466 ----------------------------------------------------------------------------------------------------
2021-10-08 01:24:04,466 Parameters:
2021-10-08 01:24:04,469  - learning_rate: "5e-06"
2021-10-08 01:24:04,469  - mini_batch_size: "4"
2021-10-08 01:24:04,469  - patience: "3"
2021-10-08 01:24:04,469  - anneal_factor: "0.5"
2021-10-08 01:24:04,469  - max_epochs: "120"
2021-10-08 01:24:04,469  - shuffle: "True"
2021-10-08 01:24:04,469  - train_with_dev: "False"
2021-10-08 01:24:04,469  - batch_growth_annealing: "False"
2021-10-08 01:24:04,469 ----------------------------------------------------------------------------------------------------
2021-10-08 01:24:04,469 Model training base path: "model/flattened/upos/transformer/39f161e0-6d79-4069-abdf-aa7d65b1a266"
2021-10-08 01:24:04,469 ----------------------------------------------------------------------------------------------------
2021-10-08 01:24:04,469 Device: cuda:1
2021-10-08 01:24:04,470 ----------------------------------------------------------------------------------------------------
2021-10-08 01:24:04,470 Embeddings storage mode: gpu
2021-10-08 01:24:04,485 ----------------------------------------------------------------------------------------------------
2021-10-08 01:26:36,076 epoch 1 - iter 313/3136 - loss 4.72452151 - samples/sec: 8.26 - lr: 0.000005
2021-10-08 01:29:02,871 epoch 1 - iter 626/3136 - loss 4.19754075 - samples/sec: 8.53 - lr: 0.000005
2021-10-08 01:31:28,354 epoch 1 - iter 939/3136 - loss 3.75644904 - samples/sec: 8.61 - lr: 0.000005
2021-10-08 01:33:55,334 epoch 1 - iter 1252/3136 - loss 3.44793777 - samples/sec: 8.52 - lr: 0.000005
2021-10-08 01:36:23,930 epoch 1 - iter 1565/3136 - loss 3.29371214 - samples/sec: 8.43 - lr: 0.000005
2021-10-08 01:38:53,624 epoch 1 - iter 1878/3136 - loss 3.20709398 - samples/sec: 8.36 - lr: 0.000005
2021-10-08 01:41:22,233 epoch 1 - iter 2191/3136 - loss 3.09376318 - samples/sec: 8.43 - lr: 0.000005
2021-10-08 01:43:49,562 epoch 1 - iter 2504/3136 - loss 2.99601829 - samples/sec: 8.50 - lr: 0.000005
2021-10-08 01:46:15,652 epoch 1 - iter 2817/3136 - loss 2.90382036 - samples/sec: 8.57 - lr: 0.000005
2021-10-08 01:48:41,627 epoch 1 - iter 3130/3136 - loss 2.82738193 - samples/sec: 8.58 - lr: 0.000005
2021-10-08 01:48:44,523 ----------------------------------------------------------------------------------------------------
2021-10-08 01:48:44,523 EPOCH 1 done: loss 2.8267 - lr 0.0000050
2021-10-08 01:50:28,252 DEV : loss 1.812050461769104 - score 0.5562
2021-10-08 01:50:28,265 BAD EPOCHS (no improvement): 4
2021-10-08 01:50:28,269 ----------------------------------------------------------------------------------------------------
2021-10-08 01:52:50,608 epoch 2 - iter 313/3136 - loss 2.03941953 - samples/sec: 8.80 - lr: 0.000005
2021-10-08 01:55:10,208 epoch 2 - iter 626/3136 - loss 1.93688473 - samples/sec: 8.97 - lr: 0.000005
2021-10-08 01:57:30,821 epoch 2 - iter 939/3136 - loss 1.91128751 - samples/sec: 8.90 - lr: 0.000005
2021-10-08 01:59:45,778 epoch 2 - iter 1252/3136 - loss 1.89595529 - samples/sec: 9.28 - lr: 0.000005
2021-10-08 02:01:59,770 epoch 2 - iter 1565/3136 - loss 1.87182207 - samples/sec: 9.34 - lr: 0.000005
2021-10-08 02:04:11,884 epoch 2 - iter 1878/3136 - loss 1.85043779 - samples/sec: 9.48 - lr: 0.000005
2021-10-08 02:06:25,155 epoch 2 - iter 2191/3136 - loss 1.83671039 - samples/sec: 9.40 - lr: 0.000005
2021-10-08 02:08:34,795 epoch 2 - iter 2504/3136 - loss 1.81084306 - samples/sec: 9.66 - lr: 0.000005
2021-10-08 02:10:42,227 epoch 2 - iter 2817/3136 - loss 1.78909184 - samples/sec: 9.83 - lr: 0.000005
2021-10-08 02:12:47,147 epoch 2 - iter 3130/3136 - loss 1.76465136 - samples/sec: 10.02 - lr: 0.000005
2021-10-08 02:12:49,184 ----------------------------------------------------------------------------------------------------
2021-10-08 02:12:49,184 EPOCH 2 done: loss 1.7645 - lr 0.0000050
2021-10-08 02:14:46,585 DEV : loss 1.378911018371582 - score 0.645
2021-10-08 02:14:46,605 BAD EPOCHS (no improvement): 4
2021-10-08 02:14:46,609 ----------------------------------------------------------------------------------------------------
2021-10-08 02:17:21,237 epoch 3 - iter 313/3136 - loss 1.46143323 - samples/sec: 8.10 - lr: 0.000005
2021-10-08 02:19:54,828 epoch 3 - iter 626/3136 - loss 1.54426728 - samples/sec: 8.15 - lr: 0.000005
2021-10-08 02:22:28,860 epoch 3 - iter 939/3136 - loss 1.52144744 - samples/sec: 8.13 - lr: 0.000005
2021-10-08 02:25:05,026 epoch 3 - iter 1252/3136 - loss 1.54816096 - samples/sec: 8.02 - lr: 0.000005
2021-10-08 02:27:38,923 epoch 3 - iter 1565/3136 - loss 1.54253278 - samples/sec: 8.14 - lr: 0.000005
2021-10-08 02:30:12,772 epoch 3 - iter 1878/3136 - loss 1.51430937 - samples/sec: 8.14 - lr: 0.000005
2021-10-08 02:32:45,795 epoch 3 - iter 2191/3136 - loss 1.48103859 - samples/sec: 8.18 - lr: 0.000005
2021-10-08 02:35:19,815 epoch 3 - iter 2504/3136 - loss 1.45370631 - samples/sec: 8.13 - lr: 0.000005
2021-10-08 02:37:53,605 epoch 3 - iter 2817/3136 - loss 1.44014394 - samples/sec: 8.14 - lr: 0.000005
2021-10-08 02:40:29,013 epoch 3 - iter 3130/3136 - loss 1.42307495 - samples/sec: 8.06 - lr: 0.000005
2021-10-08 02:40:32,030 ----------------------------------------------------------------------------------------------------
2021-10-08 02:40:32,031 EPOCH 3 done: loss 1.4244 - lr 0.0000050
2021-10-08 02:42:36,294 DEV : loss 1.249068260192871 - score 0.6921
2021-10-08 02:42:36,314 BAD EPOCHS (no improvement): 4
2021-10-08 02:42:36,319 ----------------------------------------------------------------------------------------------------
2021-10-08 02:45:09,397 epoch 4 - iter 313/3136 - loss 1.27215181 - samples/sec: 8.18 - lr: 0.000005
2021-10-08 02:47:42,503 epoch 4 - iter 626/3136 - loss 1.26110429 - samples/sec: 8.18 - lr: 0.000005
2021-10-08 02:50:16,094 epoch 4 - iter 939/3136 - loss 1.22653784 - samples/sec: 8.15 - lr: 0.000005
2021-10-08 02:52:50,774 epoch 4 - iter 1252/3136 - loss 1.22515298 - samples/sec: 8.09 - lr: 0.000005
2021-10-08 02:55:24,797 epoch 4 - iter 1565/3136 - loss 1.22185813 - samples/sec: 8.13 - lr: 0.000005
2021-10-08 02:57:53,617 epoch 4 - iter 1878/3136 - loss 1.21294565 - samples/sec: 8.41 - lr: 0.000005
2021-10-08 03:00:28,904 epoch 4 - iter 2191/3136 - loss 1.20954068 - samples/sec: 8.06 - lr: 0.000005
2021-10-08 03:03:01,186 epoch 4 - iter 2504/3136 - loss 1.20386526 - samples/sec: 8.22 - lr: 0.000005
2021-10-08 03:05:33,040 epoch 4 - iter 2817/3136 - loss 1.19688159 - samples/sec: 8.25 - lr: 0.000005
2021-10-08 03:08:05,627 epoch 4 - iter 3130/3136 - loss 1.18494901 - samples/sec: 8.21 - lr: 0.000005
2021-10-08 03:08:08,589 ----------------------------------------------------------------------------------------------------
2021-10-08 03:08:08,590 EPOCH 4 done: loss 1.1850 - lr 0.0000050
2021-10-08 03:10:12,072 DEV : loss 1.1464722156524658 - score 0.7184
2021-10-08 03:10:12,091 BAD EPOCHS (no improvement): 4
2021-10-08 03:10:12,097 ----------------------------------------------------------------------------------------------------
2021-10-08 03:12:42,865 epoch 5 - iter 313/3136 - loss 0.96414293 - samples/sec: 8.31 - lr: 0.000005
2021-10-08 03:14:38,914 epoch 5 - iter 626/3136 - loss 1.06906642 - samples/sec: 10.79 - lr: 0.000005
2021-10-08 03:16:31,035 epoch 5 - iter 939/3136 - loss 1.09252393 - samples/sec: 11.17 - lr: 0.000005
2021-10-08 03:18:22,832 epoch 5 - iter 1252/3136 - loss 1.08614634 - samples/sec: 11.20 - lr: 0.000005
2021-10-08 03:20:14,337 epoch 5 - iter 1565/3136 - loss 1.06980532 - samples/sec: 11.23 - lr: 0.000005
2021-10-08 03:22:06,163 epoch 5 - iter 1878/3136 - loss 1.05699125 - samples/sec: 11.20 - lr: 0.000005
2021-10-08 03:23:58,157 epoch 5 - iter 2191/3136 - loss 1.05346660 - samples/sec: 11.18 - lr: 0.000005
2021-10-08 03:25:50,665 epoch 5 - iter 2504/3136 - loss 1.04967591 - samples/sec: 11.13 - lr: 0.000005
2021-10-08 03:27:43,868 epoch 5 - iter 2817/3136 - loss 1.04443819 - samples/sec: 11.06 - lr: 0.000005
2021-10-08 03:29:35,839 epoch 5 - iter 3130/3136 - loss 1.04678281 - samples/sec: 11.18 - lr: 0.000005
2021-10-08 03:29:37,993 ----------------------------------------------------------------------------------------------------
2021-10-08 03:29:37,993 EPOCH 5 done: loss 1.0471 - lr 0.0000050
2021-10-08 03:30:45,546 DEV : loss 1.0879895687103271 - score 0.7382
2021-10-08 03:30:45,557 BAD EPOCHS (no improvement): 4
2021-10-08 03:30:45,607 ----------------------------------------------------------------------------------------------------
2021-10-08 03:32:36,763 epoch 6 - iter 313/3136 - loss 0.97017969 - samples/sec: 11.26 - lr: 0.000005
2021-10-08 03:34:29,535 epoch 6 - iter 626/3136 - loss 0.91178889 - samples/sec: 11.10 - lr: 0.000005
2021-10-08 03:36:21,198 epoch 6 - iter 939/3136 - loss 0.89669925 - samples/sec: 11.21 - lr: 0.000005
2021-10-08 03:38:13,330 epoch 6 - iter 1252/3136 - loss 0.89309003 - samples/sec: 11.17 - lr: 0.000005
2021-10-08 03:40:04,105 epoch 6 - iter 1565/3136 - loss 0.89203828 - samples/sec: 11.30 - lr: 0.000005
2021-10-08 03:41:55,756 epoch 6 - iter 1878/3136 - loss 0.89184559 - samples/sec: 11.21 - lr: 0.000005
2021-10-08 03:43:47,397 epoch 6 - iter 2191/3136 - loss 0.88544273 - samples/sec: 11.22 - lr: 0.000005
2021-10-08 03:45:40,117 epoch 6 - iter 2504/3136 - loss 0.89314875 - samples/sec: 11.11 - lr: 0.000005
2021-10-08 03:47:33,127 epoch 6 - iter 2817/3136 - loss 0.90096315 - samples/sec: 11.08 - lr: 0.000005
2021-10-08 03:49:25,248 epoch 6 - iter 3130/3136 - loss 0.90692548 - samples/sec: 11.17 - lr: 0.000005
2021-10-08 03:49:27,308 ----------------------------------------------------------------------------------------------------
2021-10-08 03:49:27,308 EPOCH 6 done: loss 0.9074 - lr 0.0000050
2021-10-08 03:50:34,975 DEV : loss 1.0945545434951782 - score 0.7502
2021-10-08 03:50:34,987 BAD EPOCHS (no improvement): 4
2021-10-08 03:50:34,992 ----------------------------------------------------------------------------------------------------
2021-10-08 03:52:27,193 epoch 7 - iter 313/3136 - loss 0.83764920 - samples/sec: 11.16 - lr: 0.000005
2021-10-08 03:54:18,239 epoch 7 - iter 626/3136 - loss 0.81819209 - samples/sec: 11.28 - lr: 0.000005
2021-10-08 03:56:08,685 epoch 7 - iter 939/3136 - loss 0.82479112 - samples/sec: 11.34 - lr: 0.000005
2021-10-08 03:58:00,332 epoch 7 - iter 1252/3136 - loss 0.82496658 - samples/sec: 11.22 - lr: 0.000005
2021-10-08 03:59:51,981 epoch 7 - iter 1565/3136 - loss 0.82564349 - samples/sec: 11.21 - lr: 0.000005
2021-10-08 04:01:42,276 epoch 7 - iter 1878/3136 - loss 0.82451771 - samples/sec: 11.35 - lr: 0.000005
2021-10-08 04:03:32,861 epoch 7 - iter 2191/3136 - loss 0.82374588 - samples/sec: 11.32 - lr: 0.000005
2021-10-08 04:05:23,201 epoch 7 - iter 2504/3136 - loss 0.82330370 - samples/sec: 11.35 - lr: 0.000005
2021-10-08 04:07:14,392 epoch 7 - iter 2817/3136 - loss 0.82319521 - samples/sec: 11.26 - lr: 0.000005
2021-10-08 04:09:05,238 epoch 7 - iter 3130/3136 - loss 0.82935928 - samples/sec: 11.30 - lr: 0.000005
2021-10-08 04:09:07,287 ----------------------------------------------------------------------------------------------------
2021-10-08 04:09:07,288 EPOCH 7 done: loss 0.8289 - lr 0.0000050
2021-10-08 04:10:13,194 DEV : loss 1.107642650604248 - score 0.7565
2021-10-08 04:10:13,206 BAD EPOCHS (no improvement): 4
2021-10-08 04:10:13,209 ----------------------------------------------------------------------------------------------------
2021-10-08 04:12:04,224 epoch 8 - iter 313/3136 - loss 0.72364194 - samples/sec: 11.28 - lr: 0.000005
2021-10-08 04:13:54,018 epoch 8 - iter 626/3136 - loss 0.67409277 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 04:15:42,871 epoch 8 - iter 939/3136 - loss 0.71036177 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 04:17:32,120 epoch 8 - iter 1252/3136 - loss 0.72046595 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 04:19:21,473 epoch 8 - iter 1565/3136 - loss 0.72333013 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 04:21:10,115 epoch 8 - iter 1878/3136 - loss 0.73052138 - samples/sec: 11.52 - lr: 0.000005
2021-10-08 04:22:59,248 epoch 8 - iter 2191/3136 - loss 0.73483207 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 04:24:48,469 epoch 8 - iter 2504/3136 - loss 0.74476355 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 04:26:37,721 epoch 8 - iter 2817/3136 - loss 0.75145613 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 04:28:27,577 epoch 8 - iter 3130/3136 - loss 0.75654745 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 04:28:29,654 ----------------------------------------------------------------------------------------------------
2021-10-08 04:28:29,654 EPOCH 8 done: loss 0.7572 - lr 0.0000049
2021-10-08 04:29:35,063 DEV : loss 1.118957281112671 - score 0.7628
2021-10-08 04:29:35,071 BAD EPOCHS (no improvement): 4
2021-10-08 04:29:35,074 ----------------------------------------------------------------------------------------------------
2021-10-08 04:31:24,802 epoch 9 - iter 313/3136 - loss 0.68506670 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 04:33:14,469 epoch 9 - iter 626/3136 - loss 0.68548964 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 04:35:03,734 epoch 9 - iter 939/3136 - loss 0.67487346 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 04:36:52,807 epoch 9 - iter 1252/3136 - loss 0.68000803 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 04:38:41,968 epoch 9 - iter 1565/3136 - loss 0.68100684 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 04:40:30,856 epoch 9 - iter 1878/3136 - loss 0.68627525 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 04:42:19,992 epoch 9 - iter 2191/3136 - loss 0.68302365 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 04:44:09,307 epoch 9 - iter 2504/3136 - loss 0.68230097 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 04:45:57,745 epoch 9 - iter 2817/3136 - loss 0.68387847 - samples/sec: 11.55 - lr: 0.000005
2021-10-08 04:47:47,434 epoch 9 - iter 3130/3136 - loss 0.68672960 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 04:47:49,400 ----------------------------------------------------------------------------------------------------
2021-10-08 04:47:49,400 EPOCH 9 done: loss 0.6868 - lr 0.0000049
2021-10-08 04:48:54,370 DEV : loss 1.1347905397415161 - score 0.7695
2021-10-08 04:48:54,378 BAD EPOCHS (no improvement): 4
2021-10-08 04:48:54,381 ----------------------------------------------------------------------------------------------------
2021-10-08 04:50:43,969 epoch 10 - iter 313/3136 - loss 0.64032557 - samples/sec: 11.43 - lr: 0.000005
2021-10-08 04:52:33,093 epoch 10 - iter 626/3136 - loss 0.61839274 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 04:54:21,345 epoch 10 - iter 939/3136 - loss 0.61991987 - samples/sec: 11.57 - lr: 0.000005
2021-10-08 04:56:10,134 epoch 10 - iter 1252/3136 - loss 0.62499864 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 04:57:58,887 epoch 10 - iter 1565/3136 - loss 0.62598781 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 04:59:46,732 epoch 10 - iter 1878/3136 - loss 0.63239897 - samples/sec: 11.61 - lr: 0.000005
2021-10-08 05:01:35,807 epoch 10 - iter 2191/3136 - loss 0.63266405 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 05:03:24,988 epoch 10 - iter 2504/3136 - loss 0.63557234 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 05:05:15,058 epoch 10 - iter 2817/3136 - loss 0.63410926 - samples/sec: 11.38 - lr: 0.000005
2021-10-08 05:07:04,882 epoch 10 - iter 3130/3136 - loss 0.63498556 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 05:07:06,956 ----------------------------------------------------------------------------------------------------
2021-10-08 05:07:06,956 EPOCH 10 done: loss 0.6359 - lr 0.0000049
2021-10-08 05:08:12,258 DEV : loss 1.1671329736709595 - score 0.7693
2021-10-08 05:08:12,267 BAD EPOCHS (no improvement): 4
2021-10-08 05:08:12,286 ----------------------------------------------------------------------------------------------------
2021-10-08 05:10:00,718 epoch 11 - iter 313/3136 - loss 0.57500289 - samples/sec: 11.55 - lr: 0.000005
2021-10-08 05:11:49,466 epoch 11 - iter 626/3136 - loss 0.58173473 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 05:13:38,540 epoch 11 - iter 939/3136 - loss 0.59228356 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 05:15:28,319 epoch 11 - iter 1252/3136 - loss 0.59195589 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 05:17:17,667 epoch 11 - iter 1565/3136 - loss 0.58853194 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 05:19:06,840 epoch 11 - iter 1878/3136 - loss 0.58715755 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 05:20:56,896 epoch 11 - iter 2191/3136 - loss 0.59448667 - samples/sec: 11.38 - lr: 0.000005
2021-10-08 05:22:46,445 epoch 11 - iter 2504/3136 - loss 0.59141278 - samples/sec: 11.43 - lr: 0.000005
2021-10-08 05:24:34,553 epoch 11 - iter 2817/3136 - loss 0.59617823 - samples/sec: 11.58 - lr: 0.000005
2021-10-08 05:26:24,671 epoch 11 - iter 3130/3136 - loss 0.59715195 - samples/sec: 11.37 - lr: 0.000005
2021-10-08 05:26:26,644 ----------------------------------------------------------------------------------------------------
2021-10-08 05:26:26,644 EPOCH 11 done: loss 0.5975 - lr 0.0000049
2021-10-08 05:27:31,845 DEV : loss 1.1581695079803467 - score 0.7728
2021-10-08 05:27:31,854 BAD EPOCHS (no improvement): 4
2021-10-08 05:27:31,856 ----------------------------------------------------------------------------------------------------
2021-10-08 05:29:21,779 epoch 12 - iter 313/3136 - loss 0.57913968 - samples/sec: 11.39 - lr: 0.000005
2021-10-08 05:31:11,256 epoch 12 - iter 626/3136 - loss 0.56713771 - samples/sec: 11.44 - lr: 0.000005
2021-10-08 05:33:01,113 epoch 12 - iter 939/3136 - loss 0.58112832 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 05:34:49,729 epoch 12 - iter 1252/3136 - loss 0.58242533 - samples/sec: 11.53 - lr: 0.000005
2021-10-08 05:36:39,830 epoch 12 - iter 1565/3136 - loss 0.58220592 - samples/sec: 11.37 - lr: 0.000005
2021-10-08 05:38:28,513 epoch 12 - iter 1878/3136 - loss 0.57768959 - samples/sec: 11.52 - lr: 0.000005
2021-10-08 05:40:18,134 epoch 12 - iter 2191/3136 - loss 0.56545698 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 05:42:07,123 epoch 12 - iter 2504/3136 - loss 0.56642755 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 05:43:55,402 epoch 12 - iter 2817/3136 - loss 0.56622702 - samples/sec: 11.56 - lr: 0.000005
2021-10-08 05:45:44,636 epoch 12 - iter 3130/3136 - loss 0.57247996 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 05:45:46,759 ----------------------------------------------------------------------------------------------------
2021-10-08 05:45:46,759 EPOCH 12 done: loss 0.5723 - lr 0.0000049
2021-10-08 05:46:52,314 DEV : loss 1.217142939567566 - score 0.7758
2021-10-08 05:46:52,323 BAD EPOCHS (no improvement): 4
2021-10-08 05:46:52,326 ----------------------------------------------------------------------------------------------------
2021-10-08 05:48:41,080 epoch 13 - iter 313/3136 - loss 0.50966829 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 05:50:30,744 epoch 13 - iter 626/3136 - loss 0.49750639 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 05:52:21,185 epoch 13 - iter 939/3136 - loss 0.51587172 - samples/sec: 11.34 - lr: 0.000005
2021-10-08 05:54:11,010 epoch 13 - iter 1252/3136 - loss 0.52871686 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 05:56:01,034 epoch 13 - iter 1565/3136 - loss 0.53202176 - samples/sec: 11.38 - lr: 0.000005
2021-10-08 05:57:49,282 epoch 13 - iter 1878/3136 - loss 0.52631699 - samples/sec: 11.57 - lr: 0.000005
2021-10-08 05:59:38,507 epoch 13 - iter 2191/3136 - loss 0.53343585 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 06:01:27,228 epoch 13 - iter 2504/3136 - loss 0.53965941 - samples/sec: 11.52 - lr: 0.000005
2021-10-08 06:03:16,978 epoch 13 - iter 2817/3136 - loss 0.53284789 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 06:05:06,066 epoch 13 - iter 3130/3136 - loss 0.53824526 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 06:05:07,987 ----------------------------------------------------------------------------------------------------
2021-10-08 06:05:07,987 EPOCH 13 done: loss 0.5384 - lr 0.0000049
2021-10-08 06:06:13,304 DEV : loss 1.2236872911453247 - score 0.774
2021-10-08 06:06:13,313 BAD EPOCHS (no improvement): 4
2021-10-08 06:06:13,338 ----------------------------------------------------------------------------------------------------
2021-10-08 06:08:02,752 epoch 14 - iter 313/3136 - loss 0.52793776 - samples/sec: 11.44 - lr: 0.000005
2021-10-08 06:09:52,298 epoch 14 - iter 626/3136 - loss 0.47457667 - samples/sec: 11.43 - lr: 0.000005
2021-10-08 06:11:41,541 epoch 14 - iter 939/3136 - loss 0.49993053 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 06:13:30,605 epoch 14 - iter 1252/3136 - loss 0.49920445 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 06:15:19,730 epoch 14 - iter 1565/3136 - loss 0.50991583 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 06:17:08,690 epoch 14 - iter 1878/3136 - loss 0.50829353 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 06:18:57,235 epoch 14 - iter 2191/3136 - loss 0.50537895 - samples/sec: 11.54 - lr: 0.000005
2021-10-08 06:20:46,763 epoch 14 - iter 2504/3136 - loss 0.50573014 - samples/sec: 11.43 - lr: 0.000005
2021-10-08 06:22:35,614 epoch 14 - iter 2817/3136 - loss 0.50989975 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 06:24:25,328 epoch 14 - iter 3130/3136 - loss 0.51003001 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 06:24:27,294 ----------------------------------------------------------------------------------------------------
2021-10-08 06:24:27,294 EPOCH 14 done: loss 0.5096 - lr 0.0000048
2021-10-08 06:25:32,358 DEV : loss 1.239980697631836 - score 0.7791
2021-10-08 06:25:32,366 BAD EPOCHS (no improvement): 4
2021-10-08 06:25:32,369 ----------------------------------------------------------------------------------------------------
2021-10-08 06:27:21,356 epoch 15 - iter 313/3136 - loss 0.48750678 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 06:29:11,160 epoch 15 - iter 626/3136 - loss 0.48264535 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 06:31:01,505 epoch 15 - iter 939/3136 - loss 0.48570936 - samples/sec: 11.35 - lr: 0.000005
2021-10-08 06:32:50,766 epoch 15 - iter 1252/3136 - loss 0.51090383 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 06:34:40,637 epoch 15 - iter 1565/3136 - loss 0.50922282 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 06:36:30,311 epoch 15 - iter 1878/3136 - loss 0.50002768 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 06:38:19,152 epoch 15 - iter 2191/3136 - loss 0.49195057 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 06:40:08,321 epoch 15 - iter 2504/3136 - loss 0.48526832 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 06:41:57,927 epoch 15 - iter 2817/3136 - loss 0.48681152 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 06:43:47,595 epoch 15 - iter 3130/3136 - loss 0.48203427 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 06:43:49,686 ----------------------------------------------------------------------------------------------------
2021-10-08 06:43:49,686 EPOCH 15 done: loss 0.4819 - lr 0.0000048
2021-10-08 06:44:55,144 DEV : loss 1.3040558099746704 - score 0.7823
2021-10-08 06:44:55,152 BAD EPOCHS (no improvement): 4
2021-10-08 06:44:55,155 ----------------------------------------------------------------------------------------------------
2021-10-08 06:46:43,896 epoch 16 - iter 313/3136 - loss 0.43279050 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 06:48:33,811 epoch 16 - iter 626/3136 - loss 0.47606994 - samples/sec: 11.39 - lr: 0.000005
2021-10-08 06:50:22,623 epoch 16 - iter 939/3136 - loss 0.47339257 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 06:52:11,923 epoch 16 - iter 1252/3136 - loss 0.46751027 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 06:54:01,301 epoch 16 - iter 1565/3136 - loss 0.47995663 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 06:55:50,296 epoch 16 - iter 1878/3136 - loss 0.46972059 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 06:57:39,973 epoch 16 - iter 2191/3136 - loss 0.46764935 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 06:59:28,901 epoch 16 - iter 2504/3136 - loss 0.47361991 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 07:01:17,892 epoch 16 - iter 2817/3136 - loss 0.47097051 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 07:03:07,656 epoch 16 - iter 3130/3136 - loss 0.46401575 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 07:03:09,734 ----------------------------------------------------------------------------------------------------
2021-10-08 07:03:09,735 EPOCH 16 done: loss 0.4645 - lr 0.0000048
2021-10-08 07:04:15,078 DEV : loss 1.3066933155059814 - score 0.7815
2021-10-08 07:04:15,086 BAD EPOCHS (no improvement): 4
2021-10-08 07:04:15,091 ----------------------------------------------------------------------------------------------------
2021-10-08 07:06:03,740 epoch 17 - iter 313/3136 - loss 0.46635354 - samples/sec: 11.52 - lr: 0.000005
2021-10-08 07:07:52,265 epoch 17 - iter 626/3136 - loss 0.45269060 - samples/sec: 11.54 - lr: 0.000005
2021-10-08 07:09:41,265 epoch 17 - iter 939/3136 - loss 0.43170281 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 07:11:31,081 epoch 17 - iter 1252/3136 - loss 0.42691995 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 07:13:20,712 epoch 17 - iter 1565/3136 - loss 0.43021518 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 07:15:09,815 epoch 17 - iter 1878/3136 - loss 0.43859505 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 07:16:58,204 epoch 17 - iter 2191/3136 - loss 0.43817017 - samples/sec: 11.55 - lr: 0.000005
2021-10-08 07:18:47,938 epoch 17 - iter 2504/3136 - loss 0.44797237 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 07:20:37,299 epoch 17 - iter 2817/3136 - loss 0.45019620 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 07:22:27,179 epoch 17 - iter 3130/3136 - loss 0.44728014 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 07:22:29,270 ----------------------------------------------------------------------------------------------------
2021-10-08 07:22:29,270 EPOCH 17 done: loss 0.4479 - lr 0.0000048
2021-10-08 07:23:34,297 DEV : loss 1.3436864614486694 - score 0.7855
2021-10-08 07:23:34,306 BAD EPOCHS (no improvement): 4
2021-10-08 07:23:34,308 ----------------------------------------------------------------------------------------------------
2021-10-08 07:25:23,609 epoch 18 - iter 313/3136 - loss 0.39813009 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 07:27:12,995 epoch 18 - iter 626/3136 - loss 0.44809256 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 07:29:02,208 epoch 18 - iter 939/3136 - loss 0.43429150 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 07:30:51,336 epoch 18 - iter 1252/3136 - loss 0.43593564 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 07:32:39,859 epoch 18 - iter 1565/3136 - loss 0.43584472 - samples/sec: 11.54 - lr: 0.000005
2021-10-08 07:34:28,735 epoch 18 - iter 1878/3136 - loss 0.43944665 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 07:36:16,777 epoch 18 - iter 2191/3136 - loss 0.43698590 - samples/sec: 11.59 - lr: 0.000005
2021-10-08 07:38:05,950 epoch 18 - iter 2504/3136 - loss 0.43414561 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 07:39:53,977 epoch 18 - iter 2817/3136 - loss 0.43798940 - samples/sec: 11.59 - lr: 0.000005
2021-10-08 07:41:43,762 epoch 18 - iter 3130/3136 - loss 0.43820319 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 07:41:45,770 ----------------------------------------------------------------------------------------------------
2021-10-08 07:41:45,770 EPOCH 18 done: loss 0.4384 - lr 0.0000047
2021-10-08 07:42:51,398 DEV : loss 1.342809796333313 - score 0.7868
2021-10-08 07:42:51,406 BAD EPOCHS (no improvement): 4
2021-10-08 07:42:51,412 ----------------------------------------------------------------------------------------------------
2021-10-08 07:44:43,068 epoch 19 - iter 313/3136 - loss 0.45470174 - samples/sec: 11.21 - lr: 0.000005
2021-10-08 07:46:32,610 epoch 19 - iter 626/3136 - loss 0.45275270 - samples/sec: 11.43 - lr: 0.000005
2021-10-08 07:48:22,736 epoch 19 - iter 939/3136 - loss 0.43512917 - samples/sec: 11.37 - lr: 0.000005
2021-10-08 07:50:12,199 epoch 19 - iter 1252/3136 - loss 0.43396403 - samples/sec: 11.44 - lr: 0.000005
2021-10-08 07:52:01,094 epoch 19 - iter 1565/3136 - loss 0.43703757 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 07:53:52,536 epoch 19 - iter 1878/3136 - loss 0.42786599 - samples/sec: 11.24 - lr: 0.000005
2021-10-08 07:55:42,342 epoch 19 - iter 2191/3136 - loss 0.42282463 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 07:57:31,036 epoch 19 - iter 2504/3136 - loss 0.41838047 - samples/sec: 11.52 - lr: 0.000005
2021-10-08 07:59:20,176 epoch 19 - iter 2817/3136 - loss 0.41500512 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 08:01:10,425 epoch 19 - iter 3130/3136 - loss 0.41166811 - samples/sec: 11.36 - lr: 0.000005
2021-10-08 08:01:12,457 ----------------------------------------------------------------------------------------------------
2021-10-08 08:01:12,457 EPOCH 19 done: loss 0.4114 - lr 0.0000047
2021-10-08 08:02:16,728 DEV : loss 1.4082189798355103 - score 0.7869
2021-10-08 08:02:16,736 BAD EPOCHS (no improvement): 4
2021-10-08 08:02:16,758 ----------------------------------------------------------------------------------------------------
2021-10-08 08:04:06,143 epoch 20 - iter 313/3136 - loss 0.41278664 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 08:05:55,425 epoch 20 - iter 626/3136 - loss 0.40077776 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 08:07:46,135 epoch 20 - iter 939/3136 - loss 0.41098023 - samples/sec: 11.31 - lr: 0.000005
2021-10-08 08:09:35,513 epoch 20 - iter 1252/3136 - loss 0.41764944 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 08:11:24,365 epoch 20 - iter 1565/3136 - loss 0.42188469 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 08:13:12,566 epoch 20 - iter 1878/3136 - loss 0.41128987 - samples/sec: 11.57 - lr: 0.000005
2021-10-08 08:15:02,206 epoch 20 - iter 2191/3136 - loss 0.41258296 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 08:16:51,394 epoch 20 - iter 2504/3136 - loss 0.42169336 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 08:18:40,050 epoch 20 - iter 2817/3136 - loss 0.41783420 - samples/sec: 11.52 - lr: 0.000005
2021-10-08 08:20:29,442 epoch 20 - iter 3130/3136 - loss 0.41573078 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 08:20:31,419 ----------------------------------------------------------------------------------------------------
2021-10-08 08:20:31,420 EPOCH 20 done: loss 0.4166 - lr 0.0000047
2021-10-08 08:21:36,697 DEV : loss 1.4238334894180298 - score 0.7865
2021-10-08 08:21:36,706 BAD EPOCHS (no improvement): 4
2021-10-08 08:21:36,720 ----------------------------------------------------------------------------------------------------
2021-10-08 08:23:25,955 epoch 21 - iter 313/3136 - loss 0.40240706 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 08:25:14,522 epoch 21 - iter 626/3136 - loss 0.39511871 - samples/sec: 11.53 - lr: 0.000005
2021-10-08 08:27:03,021 epoch 21 - iter 939/3136 - loss 0.39431793 - samples/sec: 11.54 - lr: 0.000005
2021-10-08 08:28:51,503 epoch 21 - iter 1252/3136 - loss 0.39109346 - samples/sec: 11.54 - lr: 0.000005
2021-10-08 08:30:40,470 epoch 21 - iter 1565/3136 - loss 0.39675041 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 08:32:30,156 epoch 21 - iter 1878/3136 - loss 0.39722037 - samples/sec: 11.42 - lr: 0.000005
2021-10-08 08:34:20,219 epoch 21 - iter 2191/3136 - loss 0.40032465 - samples/sec: 11.38 - lr: 0.000005
2021-10-08 08:36:09,102 epoch 21 - iter 2504/3136 - loss 0.40155023 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 08:37:57,887 epoch 21 - iter 2817/3136 - loss 0.40317135 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 08:39:46,034 epoch 21 - iter 3130/3136 - loss 0.39855628 - samples/sec: 11.58 - lr: 0.000005
2021-10-08 08:39:47,973 ----------------------------------------------------------------------------------------------------
2021-10-08 08:39:47,973 EPOCH 21 done: loss 0.3978 - lr 0.0000046
2021-10-08 08:40:52,942 DEV : loss 1.4480031728744507 - score 0.7882
2021-10-08 08:40:52,950 BAD EPOCHS (no improvement): 4
2021-10-08 08:40:52,954 ----------------------------------------------------------------------------------------------------
2021-10-08 08:42:41,866 epoch 22 - iter 313/3136 - loss 0.43559075 - samples/sec: 11.50 - lr: 0.000005
2021-10-08 08:44:31,145 epoch 22 - iter 626/3136 - loss 0.39099293 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 08:46:21,008 epoch 22 - iter 939/3136 - loss 0.39371800 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 08:48:09,064 epoch 22 - iter 1252/3136 - loss 0.38805936 - samples/sec: 11.59 - lr: 0.000005
2021-10-08 08:49:56,870 epoch 22 - iter 1565/3136 - loss 0.38302924 - samples/sec: 11.61 - lr: 0.000005
2021-10-08 08:51:45,300 epoch 22 - iter 1878/3136 - loss 0.39199611 - samples/sec: 11.55 - lr: 0.000005
2021-10-08 08:53:33,664 epoch 22 - iter 2191/3136 - loss 0.39724670 - samples/sec: 11.55 - lr: 0.000005
2021-10-08 08:55:22,323 epoch 22 - iter 2504/3136 - loss 0.39419823 - samples/sec: 11.52 - lr: 0.000005
2021-10-08 08:57:11,689 epoch 22 - iter 2817/3136 - loss 0.39457619 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 08:59:00,790 epoch 22 - iter 3130/3136 - loss 0.39208577 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 08:59:02,630 ----------------------------------------------------------------------------------------------------
2021-10-08 08:59:02,630 EPOCH 22 done: loss 0.3939 - lr 0.0000046
2021-10-08 09:00:07,938 DEV : loss 1.5155459642410278 - score 0.7873
2021-10-08 09:00:07,947 BAD EPOCHS (no improvement): 4
2021-10-08 09:00:07,966 ----------------------------------------------------------------------------------------------------
2021-10-08 09:01:55,545 epoch 23 - iter 313/3136 - loss 0.31486830 - samples/sec: 11.64 - lr: 0.000005
2021-10-08 09:03:44,522 epoch 23 - iter 626/3136 - loss 0.34377753 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 09:05:34,429 epoch 23 - iter 939/3136 - loss 0.35987107 - samples/sec: 11.39 - lr: 0.000005
2021-10-08 09:07:23,408 epoch 23 - iter 1252/3136 - loss 0.36379059 - samples/sec: 11.49 - lr: 0.000005
2021-10-08 09:09:12,620 epoch 23 - iter 1565/3136 - loss 0.37045160 - samples/sec: 11.46 - lr: 0.000005
2021-10-08 09:11:01,678 epoch 23 - iter 1878/3136 - loss 0.37085694 - samples/sec: 11.48 - lr: 0.000005
2021-10-08 09:12:50,175 epoch 23 - iter 2191/3136 - loss 0.36668998 - samples/sec: 11.54 - lr: 0.000005
2021-10-08 09:14:39,959 epoch 23 - iter 2504/3136 - loss 0.36718363 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 09:16:29,796 epoch 23 - iter 2817/3136 - loss 0.36665782 - samples/sec: 11.40 - lr: 0.000005
2021-10-08 09:18:19,105 epoch 23 - iter 3130/3136 - loss 0.36690090 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 09:18:21,093 ----------------------------------------------------------------------------------------------------
2021-10-08 09:18:21,093 EPOCH 23 done: loss 0.3668 - lr 0.0000046
2021-10-08 09:19:26,108 DEV : loss 1.500352144241333 - score 0.7887
2021-10-08 09:19:26,116 BAD EPOCHS (no improvement): 4
2021-10-08 09:19:26,119 ----------------------------------------------------------------------------------------------------
2021-10-08 09:21:14,416 epoch 24 - iter 313/3136 - loss 0.36040100 - samples/sec: 11.56 - lr: 0.000005
2021-10-08 09:23:02,737 epoch 24 - iter 626/3136 - loss 0.35759142 - samples/sec: 11.56 - lr: 0.000005
2021-10-08 09:24:51,530 epoch 24 - iter 939/3136 - loss 0.36765414 - samples/sec: 11.51 - lr: 0.000005
2021-10-08 09:26:40,159 epoch 24 - iter 1252/3136 - loss 0.36831504 - samples/sec: 11.53 - lr: 0.000005
2021-10-08 09:28:28,449 epoch 24 - iter 1565/3136 - loss 0.36825669 - samples/sec: 11.56 - lr: 0.000005
2021-10-08 09:30:18,671 epoch 24 - iter 1878/3136 - loss 0.36723857 - samples/sec: 11.36 - lr: 0.000005
2021-10-08 09:32:07,879 epoch 24 - iter 2191/3136 - loss 0.37082841 - samples/sec: 11.47 - lr: 0.000005
2021-10-08 09:33:56,011 epoch 24 - iter 2504/3136 - loss 0.36418767 - samples/sec: 11.58 - lr: 0.000005
2021-10-08 09:35:44,395 epoch 24 - iter 2817/3136 - loss 0.36160032 - samples/sec: 11.55 - lr: 0.000005
2021-10-08 09:37:35,227 epoch 24 - iter 3130/3136 - loss 0.36083921 - samples/sec: 11.30 - lr: 0.000005
2021-10-08 09:37:37,295 ----------------------------------------------------------------------------------------------------
2021-10-08 09:37:37,295 EPOCH 24 done: loss 0.3610 - lr 0.0000045
2021-10-08 09:38:42,547 DEV : loss 1.5767343044281006 - score 0.7857
2021-10-08 09:38:42,555 BAD EPOCHS (no improvement): 4
2021-10-08 09:38:42,567 ----------------------------------------------------------------------------------------------------
2021-10-08 09:40:32,146 epoch 25 - iter 313/3136 - loss 0.36533023 - samples/sec: 11.43 - lr: 0.000005
2021-10-08 09:42:21,910 epoch 25 - iter 626/3136 - loss 0.35672270 - samples/sec: 11.41 - lr: 0.000005
2021-10-08 09:44:09,552 epoch 25 - iter 939/3136 - loss 0.35291058 - samples/sec: 11.63 - lr: 0.000005
2021-10-08 09:45:58,887 epoch 25 - iter 1252/3136 - loss 0.34390201 - samples/sec: 11.45 - lr: 0.000005
2021-10-08 09:47:49,390 epoch 25 - iter 1565/3136 - loss 0.34591991 - samples/sec: 11.33 - lr: 0.000005
2021-10-08 09:49:38,472 epoch 25 - iter 1878/3136 - loss 0.34789451 - samples/sec: 11.48 - lr: 0.000004
2021-10-08 09:51:27,545 epoch 25 - iter 2191/3136 - loss 0.34855003 - samples/sec: 11.48 - lr: 0.000004
2021-10-08 09:53:16,549 epoch 25 - iter 2504/3136 - loss 0.34727665 - samples/sec: 11.49 - lr: 0.000004
2021-10-08 09:55:04,884 epoch 25 - iter 2817/3136 - loss 0.35129801 - samples/sec: 11.56 - lr: 0.000004
2021-10-08 09:56:54,609 epoch 25 - iter 3130/3136 - loss 0.35105321 - samples/sec: 11.41 - lr: 0.000004
2021-10-08 09:56:56,612 ----------------------------------------------------------------------------------------------------
2021-10-08 09:56:56,612 EPOCH 25 done: loss 0.3507 - lr 0.0000045
2021-10-08 09:58:02,041 DEV : loss 1.5511544942855835 - score 0.7928
2021-10-08 09:58:02,050 BAD EPOCHS (no improvement): 4
2021-10-08 09:58:02,062 ----------------------------------------------------------------------------------------------------
2021-10-08 09:59:50,471 epoch 26 - iter 313/3136 - loss 0.38618970 - samples/sec: 11.55 - lr: 0.000004
2021-10-08 10:01:38,720 epoch 26 - iter 626/3136 - loss 0.35005474 - samples/sec: 11.57 - lr: 0.000004
2021-10-08 10:03:28,050 epoch 26 - iter 939/3136 - loss 0.33172202 - samples/sec: 11.45 - lr: 0.000004
2021-10-08 10:05:16,627 epoch 26 - iter 1252/3136 - loss 0.32822645 - samples/sec: 11.53 - lr: 0.000004
2021-10-08 10:07:05,630 epoch 26 - iter 1565/3136 - loss 0.33849554 - samples/sec: 11.49 - lr: 0.000004
2021-10-08 10:08:55,140 epoch 26 - iter 1878/3136 - loss 0.34579331 - samples/sec: 11.43 - lr: 0.000004
2021-10-08 10:10:44,371 epoch 26 - iter 2191/3136 - loss 0.34443610 - samples/sec: 11.46 - lr: 0.000004
2021-10-08 10:12:33,298 epoch 26 - iter 2504/3136 - loss 0.34461049 - samples/sec: 11.49 - lr: 0.000004
2021-10-08 10:14:21,027 epoch 26 - iter 2817/3136 - loss 0.34648168 - samples/sec: 11.62 - lr: 0.000004
2021-10-08 10:16:09,732 epoch 26 - iter 3130/3136 - loss 0.35025001 - samples/sec: 11.52 - lr: 0.000004
2021-10-08 10:16:11,828 ----------------------------------------------------------------------------------------------------
2021-10-08 10:16:11,828 EPOCH 26 done: loss 0.3505 - lr 0.0000044
2021-10-08 10:17:17,321 DEV : loss 1.5472676753997803 - score 0.7896
2021-10-08 10:17:17,329 BAD EPOCHS (no improvement): 4
2021-10-08 10:17:17,333 ----------------------------------------------------------------------------------------------------
2021-10-08 10:19:06,333 epoch 27 - iter 313/3136 - loss 0.33663705 - samples/sec: 11.49 - lr: 0.000004
2021-10-08 10:20:55,100 epoch 27 - iter 626/3136 - loss 0.33702515 - samples/sec: 11.51 - lr: 0.000004
2021-10-08 10:22:42,882 epoch 27 - iter 939/3136 - loss 0.33653477 - samples/sec: 11.62 - lr: 0.000004
2021-10-08 10:24:30,379 epoch 27 - iter 1252/3136 - loss 0.34225791 - samples/sec: 11.65 - lr: 0.000004
2021-10-08 10:26:18,538 epoch 27 - iter 1565/3136 - loss 0.34358626 - samples/sec: 11.58 - lr: 0.000004
2021-10-08 10:28:08,280 epoch 27 - iter 1878/3136 - loss 0.34602219 - samples/sec: 11.41 - lr: 0.000004
2021-10-08 10:29:57,382 epoch 27 - iter 2191/3136 - loss 0.34648664 - samples/sec: 11.48 - lr: 0.000004
2021-10-08 10:31:46,144 epoch 27 - iter 2504/3136 - loss 0.34776293 - samples/sec: 11.51 - lr: 0.000004
2021-10-08 10:33:34,817 epoch 27 - iter 2817/3136 - loss 0.34750995 - samples/sec: 11.52 - lr: 0.000004
2021-10-08 10:35:24,651 epoch 27 - iter 3130/3136 - loss 0.34581553 - samples/sec: 11.40 - lr: 0.000004
2021-10-08 10:35:26,511 ----------------------------------------------------------------------------------------------------
2021-10-08 10:35:26,511 EPOCH 27 done: loss 0.3456 - lr 0.0000044
2021-10-08 10:36:32,202 DEV : loss 1.592006802558899 - score 0.7915
2021-10-08 10:36:32,211 BAD EPOCHS (no improvement): 4
2021-10-08 10:36:32,215 ----------------------------------------------------------------------------------------------------
2021-10-08 10:38:20,827 epoch 28 - iter 313/3136 - loss 0.33408901 - samples/sec: 11.53 - lr: 0.000004
2021-10-08 10:40:09,638 epoch 28 - iter 626/3136 - loss 0.34682950 - samples/sec: 11.51 - lr: 0.000004
2021-10-08 10:41:57,542 epoch 28 - iter 939/3136 - loss 0.33907392 - samples/sec: 11.60 - lr: 0.000004
2021-10-08 10:43:46,490 epoch 28 - iter 1252/3136 - loss 0.34626394 - samples/sec: 11.49 - lr: 0.000004
2021-10-08 10:45:36,521 epoch 28 - iter 1565/3136 - loss 0.33769186 - samples/sec: 11.38 - lr: 0.000004
2021-10-08 10:47:25,259 epoch 28 - iter 1878/3136 - loss 0.33283411 - samples/sec: 11.51 - lr: 0.000004
2021-10-08 10:49:13,692 epoch 28 - iter 2191/3136 - loss 0.33211924 - samples/sec: 11.55 - lr: 0.000004
2021-10-08 10:51:02,748 epoch 28 - iter 2504/3136 - loss 0.33087584 - samples/sec: 11.48 - lr: 0.000004
2021-10-08 10:52:51,443 epoch 28 - iter 2817/3136 - loss 0.33543046 - samples/sec: 11.52 - lr: 0.000004
2021-10-08 10:54:39,344 epoch 28 - iter 3130/3136 - loss 0.33654469 - samples/sec: 11.60 - lr: 0.000004
2021-10-08 10:54:41,380 ----------------------------------------------------------------------------------------------------
2021-10-08 10:54:41,380 EPOCH 28 done: loss 0.3362 - lr 0.0000044
2021-10-08 10:55:46,211 DEV : loss 1.5796531438827515 - score 0.7936
2021-10-08 10:55:46,219 BAD EPOCHS (no improvement): 4
2021-10-08 10:55:46,224 ----------------------------------------------------------------------------------------------------
2021-10-08 10:57:34,260 epoch 29 - iter 313/3136 - loss 0.34687024 - samples/sec: 11.59 - lr: 0.000004
2021-10-08 10:59:23,376 epoch 29 - iter 626/3136 - loss 0.34098650 - samples/sec: 11.47 - lr: 0.000004
2021-10-08 11:01:12,606 epoch 29 - iter 939/3136 - loss 0.33221271 - samples/sec: 11.46 - lr: 0.000004
2021-10-08 11:03:01,626 epoch 29 - iter 1252/3136 - loss 0.34946377 - samples/sec: 11.49 - lr: 0.000004
2021-10-08 11:04:48,709 epoch 29 - iter 1565/3136 - loss 0.35489742 - samples/sec: 11.69 - lr: 0.000004
2021-10-08 11:06:37,749 epoch 29 - iter 1878/3136 - loss 0.34713808 - samples/sec: 11.48 - lr: 0.000004
2021-10-08 11:08:27,620 epoch 29 - iter 2191/3136 - loss 0.35075785 - samples/sec: 11.40 - lr: 0.000004
2021-10-08 11:10:15,485 epoch 29 - iter 2504/3136 - loss 0.34595553 - samples/sec: 11.61 - lr: 0.000004
2021-10-08 11:12:04,919 epoch 29 - iter 2817/3136 - loss 0.34714127 - samples/sec: 11.44 - lr: 0.000004
2021-10-08 11:13:54,413 epoch 29 - iter 3130/3136 - loss 0.34406755 - samples/sec: 11.44 - lr: 0.000004
2021-10-08 11:13:56,490 ----------------------------------------------------------------------------------------------------
2021-10-08 11:13:56,491 EPOCH 29 done: loss 0.3442 - lr 0.0000043
2021-10-08 11:15:01,691 DEV : loss 1.611224889755249 - score 0.7922
2021-10-08 11:15:01,699 BAD EPOCHS (no improvement): 4
2021-10-08 11:15:01,702 ----------------------------------------------------------------------------------------------------
2021-10-08 11:16:50,230 epoch 30 - iter 313/3136 - loss 0.34686401 - samples/sec: 11.54 - lr: 0.000004
2021-10-08 11:18:39,660 epoch 30 - iter 626/3136 - loss 0.36453829 - samples/sec: 11.44 - lr: 0.000004
2021-10-08 11:20:27,023 epoch 30 - iter 939/3136 - loss 0.36528454 - samples/sec: 11.66 - lr: 0.000004
2021-10-08 11:22:15,192 epoch 30 - iter 1252/3136 - loss 0.36022447 - samples/sec: 11.58 - lr: 0.000004
2021-10-08 11:24:04,086 epoch 30 - iter 1565/3136 - loss 0.34986958 - samples/sec: 11.50 - lr: 0.000004
2021-10-08 11:25:52,067 epoch 30 - iter 1878/3136 - loss 0.35124174 - samples/sec: 11.60 - lr: 0.000004
2021-10-08 11:27:41,606 epoch 30 - iter 2191/3136 - loss 0.35410033 - samples/sec: 11.43 - lr: 0.000004
2021-10-08 11:29:29,994 epoch 30 - iter 2504/3136 - loss 0.35147643 - samples/sec: 11.55 - lr: 0.000004
2021-10-08 11:31:18,765 epoch 30 - iter 2817/3136 - loss 0.34600033 - samples/sec: 11.51 - lr: 0.000004
2021-10-08 11:33:06,618 epoch 30 - iter 3130/3136 - loss 0.34511292 - samples/sec: 11.61 - lr: 0.000004
2021-10-08 11:33:08,714 ----------------------------------------------------------------------------------------------------
2021-10-08 11:33:08,714 EPOCH 30 done: loss 0.3452 - lr 0.0000043
2021-10-08 11:34:13,546 DEV : loss 1.6024011373519897 - score 0.7908
2021-10-08 11:34:13,555 BAD EPOCHS (no improvement): 4
2021-10-08 11:34:13,558 ----------------------------------------------------------------------------------------------------
2021-10-08 11:36:01,479 epoch 31 - iter 313/3136 - loss 0.34806541 - samples/sec: 11.60 - lr: 0.000004
2021-10-08 11:37:50,428 epoch 31 - iter 626/3136 - loss 0.35529842 - samples/sec: 11.49 - lr: 0.000004
2021-10-08 11:39:40,088 epoch 31 - iter 939/3136 - loss 0.34186284 - samples/sec: 11.42 - lr: 0.000004
2021-10-08 11:41:28,281 epoch 31 - iter 1252/3136 - loss 0.34525109 - samples/sec: 11.57 - lr: 0.000004
2021-10-08 11:43:17,771 epoch 31 - iter 1565/3136 - loss 0.34389946 - samples/sec: 11.44 - lr: 0.000004
2021-10-08 11:45:06,071 epoch 31 - iter 1878/3136 - loss 0.33796762 - samples/sec: 11.56 - lr: 0.000004
2021-10-08 11:46:55,362 epoch 31 - iter 2191/3136 - loss 0.33930723 - samples/sec: 11.46 - lr: 0.000004
2021-10-08 11:48:43,568 epoch 31 - iter 2504/3136 - loss 0.33275661 - samples/sec: 11.57 - lr: 0.000004
2021-10-08 11:50:32,708 epoch 31 - iter 2817/3136 - loss 0.33648906 - samples/sec: 11.47 - lr: 0.000004
2021-10-08 11:52:22,171 epoch 31 - iter 3130/3136 - loss 0.33846519 - samples/sec: 11.44 - lr: 0.000004
2021-10-08 11:52:24,182 ----------------------------------------------------------------------------------------------------
2021-10-08 11:52:24,183 EPOCH 31 done: loss 0.3383 - lr 0.0000042
2021-10-08 11:53:29,385 DEV : loss 1.6685919761657715 - score 0.7914
2021-10-08 11:53:29,394 BAD EPOCHS (no improvement): 4
2021-10-08 11:53:29,397 ----------------------------------------------------------------------------------------------------
2021-10-08 11:55:18,896 epoch 32 - iter 313/3136 - loss 0.30893502 - samples/sec: 11.43 - lr: 0.000004
2021-10-08 11:57:08,162 epoch 32 - iter 626/3136 - loss 0.32866876 - samples/sec: 11.46 - lr: 0.000004
2021-10-08 11:58:56,272 epoch 32 - iter 939/3136 - loss 0.35360599 - samples/sec: 11.58 - lr: 0.000004
2021-10-08 12:00:44,025 epoch 32 - iter 1252/3136 - loss 0.34810021 - samples/sec: 11.62 - lr: 0.000004
2021-10-08 12:02:32,422 epoch 32 - iter 1565/3136 - loss 0.34490519 - samples/sec: 11.55 - lr: 0.000004
2021-10-08 12:03:26,992 ----------------------------------------------------------------------------------------------------
2021-10-08 12:03:26,992 Exiting from training early.
2021-10-08 12:03:26,992 Saving model ...
2021-10-08 12:04:05,775 Done.
2021-10-08 12:04:05,775 ----------------------------------------------------------------------------------------------------
2021-10-08 12:04:05,775 Testing using best model ...
2021-10-08 12:05:13,168 	0.7877
2021-10-08 12:05:13,168 
Results:
- F-score (micro): 0.7877
- F-score (macro): 0.318
- Accuracy (incl. no class): 0.7877

By class:
                    precision    recall  f1-score   support

            1,NOUN     0.9157    0.9159    0.9158      3497
   1,NOUN,ARGM-ADJ     0.7647    0.7006    0.7312       167
        1,AUX,ARG1     0.9093    0.9093    0.9093       397
             2,ADJ     0.4286    0.1304    0.2000        23
             1,ADJ     0.8532    0.8693    0.8612       528
    1,ADJ,ARGM-EXT     0.7183    0.8793    0.7907        58
       -1,AUX,ARG2     0.9039    0.9207    0.9122       429
            -1,ADJ     0.7542    0.6818    0.7162       396
           -1,ROOT     0.8969    0.8707    0.8836      1709
          -1,PUNCT     0.8748    0.8727    0.8737      2081
       1,VERB,ARG1     0.7723    0.8072    0.7894       332
           1,PROPN     0.8293    0.7465    0.7857       501
      -1,VERB,ARG2     0.7624    0.7174    0.7392       407
       1,VERB,ARG0     0.9424    0.9530    0.9477      1064
            4,NOUN     0.9375    0.5556    0.6977        27
            3,NOUN     0.5941    0.6593    0.6250        91
            2,NOUN     0.7421    0.8135    0.7761       520
           -1,NOUN     0.7589    0.7635    0.7612      1810
      -1,VERB,ARG1     0.9063    0.9341    0.9200      1563
      -1,VERB,ARG4     0.7368    0.7636    0.7500        55
          -5,PUNCT     0.2754    0.2836    0.2794        67
          -4,PUNCT     0.4025    0.5766    0.4741       111
          -2,PUNCT     0.7391    0.8257    0.7800       614
    1,AUX,ARGM-DIS     0.3636    0.4706    0.4103        17
    1,AUX,ARGM-ADV     0.7000    0.6364    0.6667        22
       -1,AUX,ARG1     0.8800    0.8627    0.8713        51
           -1,PRON     0.7398    0.6894    0.7137       132
    1,ADJ,ARGM-ADV     0.2000    0.5000    0.2857         6
    1,AUX,ARGM-TMP     0.8824    0.7895    0.8333        19
  1,AUX,R-ARGM-TMP     1.0000    0.0000    0.0000         1
           -3,NOUN     0.5448    0.3687    0.4398       198
            -3,ADJ     0.2326    0.3226    0.2703        31
       1,NOUN,ARG2     0.2778    0.2500    0.2632        20
       1,NOUN,ARG0     0.7632    0.7768    0.7699       112
            1,VERB     0.8797    0.8936    0.8866       761
       1,NOUN,ARG1     0.6321    0.6321    0.6321       106
          -1,PROPN     0.7862    0.6881    0.7339       497
      -1,NOUN,ARG2     0.4889    0.7333    0.5867        30
   1,VERB,ARGM-MOD     0.9620    0.9838    0.9728       309
           -1,VERB     0.5640    0.6230    0.5920       191
   1,VERB,ARGM-ADV     0.8354    0.7287    0.7784       188
  -1,VERB,ARGM-ADV     0.4757    0.5052    0.4900        97
  -1,VERB,ARGM-TMP     0.8583    0.8862    0.8720       246
   -1,AUX,ARGM-ADV     0.6000    0.6154    0.6076        39
   -1,AUX,ARGM-LOC     0.6000    0.5000    0.5455         6
      -2,VERB,ARG2     0.7500    0.5000    0.6000        12
            1,PRON     0.9083    0.9252    0.9167       107
            -1,ADV     0.6601    0.6012    0.6293       168
      -1,VERB,ARG3     0.6182    0.6800    0.6476        50
     1,VERB,R-ARG0     0.9231    0.9600    0.9412        50
   1,VERB,ARGM-NEG     0.9514    0.9716    0.9614       141
      -2,VERB,ARG1     0.6667    0.6667    0.6667        36
         -1,VERB,V     0.2500    0.3333    0.2857         3
             1,ADV     0.7544    0.8350    0.7926       103
          -2,PROPN     0.6859    0.6009    0.6406       218
   2,NOUN,ARGM-ADJ     0.5556    0.8824    0.6818        17
       1,NOUN,ARG3     0.0833    0.2500    0.1250         4
   1,NOUN,ARGM-MNR     0.5000    0.4516    0.4746        31
      -1,NOUN,ARG1     0.8125    0.6691    0.7339       136
           -2,PRON     0.4500    0.7500    0.5625        36
    -1,VERB,C-ARG1     0.6923    0.6136    0.6506        44
  -2,VERB,ARGM-LOC     1.0000    0.5000    0.6667         8
  -1,VERB,ARGM-GOL     0.2500    0.3077    0.2759        13
  -1,VERB,ARGM-PRP     0.7451    0.7037    0.7238        54
   1,NOUN,ARGM-LVB     0.7619    0.7619    0.7619        42
  -1,VERB,ARGM-PRR     0.7460    0.7581    0.7520        62
            -1,NUM     0.8205    0.5590    0.6649       229
     1,VERB,R-ARG1     0.8182    0.9000    0.8571        30
  -1,VERB,ARGM-LOC     0.6875    0.6929    0.6902       127
   1,NOUN,ARGM-LOC     0.0000    0.0000    0.0000         3
         <UNKNOWN>     0.0000    1.0000    0.0000         0
      -2,NOUN,ARG1     0.5652    0.7222    0.6341        18
           -2,VERB     0.3654    0.3585    0.3619        53
             1,NUM     0.8382    0.8028    0.8201        71
       2,VERB,ARG0     0.7083    0.8095    0.7556        21
  -1,NOUN,ARGM-PRD     0.3750    0.3750    0.3750         8
  -1,NOUN,ARGM-LVB     0.3333    0.5000    0.4000         4
           2,PROPN     0.7050    0.7717    0.7368       127
      -1,VERB,ARG0     0.7755    0.7600    0.7677        50
       3,VERB,ARG0     1.0000    0.0000    0.0000         5
           -2,NOUN     0.5514    0.5438    0.5476       434
     3,VERB,R-ARG0     1.0000    0.0000    0.0000         1
   2,VERB,ARGM-ADV     0.5000    0.5455    0.5217        11
           -4,NOUN     0.4691    0.3654    0.4108       104
   -1,AUX,ARGM-NEG     0.9524    0.8696    0.9091        23
       2,NOUN,ARG1     0.2222    0.2857    0.2500         7
       -1,ADJ,ARG1     0.5862    0.6538    0.6182        26
       -2,ADJ,ARG1     0.5000    0.3333    0.4000         3
          -3,PUNCT     0.6129    0.6502    0.6310       263
  -1,VERB,ARGM-PRD     0.2857    0.2500    0.2667        16
          -6,PUNCT     0.3143    0.3548    0.3333        31
  -1,NOUN,ARGM-ADJ     0.5000    0.5000    0.5000        30
      -1,NOUN,ARG0     0.4545    0.6250    0.5263        16
       -1,ADJ,ARG0     0.6154    0.5333    0.5714        15
   1,VERB,ARGM-TMP     0.8228    0.9774    0.8935       133
          -8,PUNCT     0.5000    0.0714    0.1250        14
          -9,PUNCT     0.2222    0.3333    0.2667         6
         -10,PUNCT     1.0000    0.0000    0.0000         4
         -11,PUNCT     1.0000    0.0000    0.0000         4
  -1,VERB,ARGM-MNR     0.5429    0.6333    0.5846        60
   1,VERB,ARGM-DIS     0.7395    0.8980    0.8111        98
   1,NOUN,ARGM-NEG     0.7222    0.7647    0.7429        17
        2,AUX,ARG1     0.8831    0.9444    0.9128        72
           4,PROPN     0.7692    0.7692    0.7692        13
           3,PROPN     0.6176    0.5833    0.6000        36
       -1,ADJ,ARG2     0.7812    0.6757    0.7246        37
      -2,NOUN,ARG2     0.2000    0.3333    0.2500         3
            -2,ADJ     0.5044    0.5135    0.5089       111
  -2,VERB,ARGM-ADV     0.1304    0.2308    0.1667        13
       2,VERB,ARG1     0.6111    0.6111    0.6111        18
  -1,VERB,ARGM-COM     0.7273    0.7273    0.7273        11
   1,NOUN,ARGM-TMP     0.6538    0.6071    0.6296        28
  -1,VERB,ARGM-DIR     0.5517    0.4103    0.4706        39
            2,VERB     0.4000    0.1111    0.1739        18
  -2,VERB,ARGM-TMP     0.3333    0.0714    0.1176        14
  -2,NOUN,ARGM-PRD     0.5000    1.0000    0.6667         1
           -3,VERB     0.2609    0.4000    0.3158        15
  -1,NOUN,ARGM-TMP     0.5000    0.6923    0.5806        13
          -7,PUNCT     0.2941    0.4167    0.3448        24
  -1,NOUN,ARGM-LOC     0.4545    0.5556    0.5000        18
  -1,VERB,ARGM-CAU     0.6000    0.8571    0.7059        14
       -2,ADJ,ARG2     0.5000    0.5000    0.5000         2
  -2,VERB,ARGM-MNR     0.0000    0.0000    0.0000         2
  -4,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
      1,AUX,R-ARG1     0.6667    0.6000    0.6316        10
        1,ADJ,ARG2     1.0000    1.0000    1.0000         1
  -2,NOUN,ARGM-ADJ     0.0000    0.0000    0.0000         3
          -6,PROPN     0.0645    0.2000    0.0976        10
           -6,NOUN     0.2308    0.3000    0.2609        20
 1,VERB,R-ARGM-MNR     1.0000    0.0000    0.0000         8
     1,VERB,R-ARG2     0.0000    0.0000    0.0000         1
          -3,PROPN     0.4940    0.5899    0.5377       139
          -4,PROPN     0.3929    0.5867    0.4706        75
      -4,NOUN,ARG1     1.0000    0.0000    0.0000         1
           -7,NOUN     0.2308    0.4286    0.3000         7
   1,VERB,ARGM-MNR     0.6000    0.7241    0.6562        29
         -19,PUNCT     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-ADV     0.0000    1.0000    0.0000         0
  -1,VERB,ARGM-EXT     0.5455    0.6667    0.6000         9
             1,DET     0.8462    0.6875    0.7586        16
            -1,DET     0.9318    0.5942    0.7257        69
       2,NOUN,ARG0     0.5385    0.7000    0.6087        10
           -5,NOUN     0.3056    0.2500    0.2750        44
   1,VERB,ARGM-LOC     0.7333    0.5238    0.6111        21
 1,VERB,R-ARGM-LOC     1.0000    1.0000    1.0000         7
           -9,NOUN     1.0000    0.0000    0.0000         8
       -3,AUX,ARG1     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-DIR     0.5000    0.7500    0.6000         4
            -6,ADJ     1.0000    0.0000    0.0000         3
            -4,ADJ     0.1875    0.2727    0.2222        11
        1,AUX,ARG2     0.7188    0.8214    0.7667        28
   1,VERB,ARGM-PRR     0.5000    0.6667    0.5714         3
  -1,NOUN,ARGM-PRP     0.3333    0.3333    0.3333         3
   -1,AUX,ARGM-PRD     1.0000    0.0000    0.0000         1
   1,NOUN,ARGM-PRP     0.0000    0.0000    0.0000         2
  -3,VERB,ARGM-ADV     1.0000    0.0000    0.0000         2
       1,VERB,ARG2     0.5455    0.4615    0.5000        26
  -2,NOUN,ARGM-TMP     1.0000    0.0000    0.0000         3
      -2,NOUN,ARG0     1.0000    0.0000    0.0000         4
             1,SYM     0.8462    0.8800    0.8627        25
            -1,SYM     0.6567    0.7213    0.6875        61
   1,VERB,ARGM-EXT     0.7619    0.6667    0.7111        24
     -1,ADJ,C-ARG1     0.7500    1.0000    0.8571         3
    1,AUX,ARGM-MOD     0.9559    0.9559    0.9559        68
    2,AUX,ARGM-ADV     0.7500    0.7500    0.7500        12
       -1,VERB,C-V     0.7857    0.7333    0.7586        15
   1,NOUN,ARGM-PRD     0.6364    0.7000    0.6667        10
   1,NOUN,ARGM-EXT     0.3333    0.4000    0.3636         5
      -3,NOUN,ARG3     1.0000    0.0000    0.0000         2
             2,NUM     0.0000    0.0000    0.0000         4
   3,VERB,ARGM-TMP     1.0000    0.0000    0.0000         2
   3,VERB,ARGM-ADV     0.0000    0.0000    0.0000         3
      -3,NOUN,ARG0     1.0000    0.0000    0.0000         1
      -3,NOUN,ARG1     0.2857    0.4000    0.3333         5
       4,VERB,ARG0     1.0000    0.0000    0.0000         1
            3,VERB     0.5000    0.2500    0.3333         4
   1,VERB,ARGM-CAU     0.5385    0.6364    0.5833        11
    -1,NOUN,R-ARG1     1.0000    0.0000    0.0000         1
      -3,VERB,ARG1     0.5000    0.3333    0.4000         6
  -1,VERB,ARGM-DIS     0.4545    0.3846    0.4167        13
           -5,VERB     1.0000    0.0000    0.0000         4
   -1,AUX,ARGM-TMP     0.7097    0.9565    0.8148        23
       5,VERB,ARG0     1.0000    0.0000    0.0000         1
      -3,VERB,ARG2     1.0000    0.0000    0.0000         3
       4,VERB,ARG1     1.0000    0.0000    0.0000         2
          -5,PROPN     0.0476    0.0667    0.0556        45
   3,VERB,ARGM-DIS     0.5000    0.5000    0.5000         2
   2,VERB,ARGM-DIS     0.5000    0.3333    0.4000         3
  -1,VERB,ARGM-NEG     0.2000    0.5000    0.2857         2
   1,VERB,ARGM-PRD     0.0000    0.0000    0.0000         1
       -1,ADJ,ARG3     1.0000    0.2857    0.4444         7
          -7,PROPN     0.0000    0.0000    0.0000        11
   2,NOUN,ARGM-GOL     1.0000    0.0000    0.0000         2
    2,AUX,ARGM-DIS     0.2500    0.3333    0.2857         3
   2,VERB,ARGM-CAU     1.0000    0.0000    0.0000         3
    1,ADJ,ARGM-CXN     0.7143    1.0000    0.8333         5
             2,AUX     1.0000    0.0000    0.0000         1
             1,AUX     0.0000    0.0000    0.0000         1
 -1,ADJ,C-ARGM-CXN     0.5000    0.6000    0.5455         5
   1,NOUN,ARGM-DIS     0.3333    0.6667    0.4444         3
   -1,AUX,ARGM-MNR     0.0000    0.0000    0.0000         2
   1,NOUN,ARGM-ADV     0.5000    0.5000    0.5000         2
  -2,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         5
   -1,AUX,ARGM-EXT     1.0000    0.0000    0.0000         3
   2,NOUN,ARGM-LOC     1.0000    0.0000    0.0000         2
 1,NOUN,R-ARGM-ADJ     1.0000    0.0000    0.0000         1
            1,INTJ     1.0000    0.2500    0.4000         4
           -1,INTJ     1.0000    0.0000    0.0000        10
   4,VERB,ARGM-ADV     1.0000    0.0000    0.0000         3
            2,PRON     0.5000    0.3333    0.4000         3
      -5,NOUN,ARG1     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
  -4,NOUN,ARGM-ADJ     0.0000    1.0000    0.0000         0
   2,NOUN,ARGM-DIS     1.0000    0.0000    0.0000         2
       2,VERB,ARG2     1.0000    0.0000    0.0000         2
    -3,VERB,C-ARG1     1.0000    0.0000    0.0000         1
    -1,NOUN,C-ARG3     1.0000    0.0000    0.0000         1
    1,AUX,ARGM-NEG     1.0000    1.0000    1.0000         8
   -1,AUX,ARGM-CAU     1.0000    0.0000    0.0000         1
   -2,AUX,ARGM-DIS     1.0000    0.0000    0.0000         1
            -2,NUM     0.6200    0.4366    0.5124        71
  -2,VERB,ARGM-DIS     1.0000    0.0000    0.0000         3
              -1,X     0.5563    0.8587    0.6752       184
      -1,NOUN,ARG3     0.2000    0.5000    0.2857         2
            -2,ADV     0.6667    0.2000    0.3077        20
    -2,VERB,C-ARG1     0.0000    0.0000    0.0000         1
   -2,AUX,ARGM-ADV     0.1429    0.3333    0.2000         3
  -4,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
              -2,X     0.5000    0.0968    0.1622        31
            -2,SYM     0.0000    0.0000    0.0000        14
            -3,SYM     1.0000    0.0000    0.0000         1
            -3,NUM     0.2162    0.3333    0.2623        24
              -3,X     0.0000    1.0000    0.0000         0
   -1,AUX,ARGM-DIS     1.0000    0.3333    0.5000         6
   2,VERB,ARGM-MNR     1.0000    0.0000    0.0000         1
            -4,NUM     0.0000    0.0000    0.0000         7
            -5,NUM     1.0000    0.0000    0.0000         2
            -6,NUM     1.0000    0.0000    0.0000         4
            -7,NUM     1.0000    0.0000    0.0000         2
  -2,NOUN,ARGM-PRP     0.0000    0.0000    0.0000         1
             2,ADV     0.5000    0.2000    0.2857         5
        2,AUX,ARG2     1.0000    0.0000    0.0000         2
    2,AUX,ARGM-TMP     0.0000    0.0000    0.0000         2
   2,NOUN,ARGM-TMP     0.4000    0.4000    0.4000         5
  -1,NOUN,ARGM-EXT     1.0000    0.5000    0.6667         2
  -1,NOUN,ARGM-DIS     0.3571    1.0000    0.5263         5
      -1,VERB,ARG5     1.0000    0.0000    0.0000         1
               1,X     0.1111    0.2500    0.1538         4
   2,NOUN,ARGM-ADV     0.5000    0.2500    0.3333         4
       3,NOUN,ARG0     0.0000    0.0000    0.0000         2
               2,X     1.0000    0.0000    0.0000         2
   3,NOUN,ARGM-TMP     1.0000    0.0000    0.0000         2
   -1,ADJ,ARGM-TMP     1.0000    0.0000    0.0000         1
   1,NOUN,ARGM-MOD     1.0000    0.4286    0.6000         7
      -4,NOUN,ARG4     1.0000    0.0000    0.0000         1
    -1,VERB,C-ARG2     0.6667    0.6667    0.6667         6
            -2,DET     0.9091    0.7692    0.8333        13
            -1,ADP     1.0000    0.0000    0.0000         7
            -2,ADP     1.0000    0.0000    0.0000         3
  -2,VERB,ARGM-GOL     1.0000    0.0000    0.0000         1
      -2,VERB,ARG0     0.0000    0.0000    0.0000         1
         -12,PUNCT     1.0000    0.0000    0.0000         1
      -2,NOUN,ARG3     0.0000    0.0000    0.0000         1
           -3,PRON     0.0000    0.0000    0.0000         7
   2,VERB,ARGM-PRR     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-MOD     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-LVB     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-PRD     0.5000    1.0000    0.6667         1
  -1,NOUN,ARGM-ADV     0.7500    0.3750    0.5000         8
           7,PROPN     1.0000    0.0000    0.0000         2
           6,PROPN     1.0000    0.0000    0.0000         1
           5,PROPN     1.0000    0.0000    0.0000         1
       2,NOUN,ARG2     0.0000    1.0000    0.0000         0
  -3,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         1
      1,ADJ,R-ARG1     1.0000    0.0000    0.0000         1
    1,ADJ,ARGM-MOD     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-MOD     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-GOL     1.0000    0.0000    0.0000         2
          -8,PROPN     1.0000    0.0000    0.0000         2
          -9,PROPN     1.0000    0.0000    0.0000         6
 3,VERB,R-ARGM-ADV     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-COM     1.0000    0.0000    0.0000         2
   3,NOUN,ARGM-ADJ     1.0000    0.0000    0.0000         5
       3,VERB,ARG1     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-DIR     1.0000    0.0000    0.0000         1
    2,AUX,ARGM-CAU     1.0000    0.0000    0.0000         2
  -3,VERB,ARGM-PRP     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         2
          -1,SCONJ     1.0000    0.0000    0.0000         1
      2,AUX,R-ARG1     0.0000    1.0000    0.0000         0
   -1,AUX,ARGM-PRP     1.0000    0.5000    0.6667         4
  -2,VERB,ARGM-PRP     0.0000    0.0000    0.0000         1
    -1,VERB,C-ARG3     0.3333    1.0000    0.5000         1
           -4,VERB     1.0000    0.0000    0.0000         2
        1,ADJ,ARG1     0.0000    0.0000    0.0000         2
         -1,NOUN,V     1.0000    0.0000    0.0000         1
           -8,NOUN     1.0000    0.0000    0.0000         8
   2,VERB,ARGM-TMP     1.0000    0.3333    0.5000         3
    -1,VERB,C-ARG0     1.0000    0.0000    0.0000         2
         -10,PROPN     1.0000    0.0000    0.0000         2
   2,NOUN,ARGM-LVB     1.0000    0.0000    0.0000         5
            -5,ADJ     1.0000    0.0000    0.0000         4
      -3,NOUN,ARG2     0.0000    1.0000    0.0000         0
    1,ADJ,ARGM-ADJ     1.0000    0.0000    0.0000         2
  -3,VERB,ARGM-LOC     1.0000    0.0000    0.0000         1
   5,VERB,ARGM-ADV     1.0000    0.0000    0.0000         1
            -1,AUX     0.0000    0.0000    0.0000         3
            -2,AUX     1.0000    0.0000    0.0000         1
   9,NOUN,ARGM-ADV     1.0000    0.0000    0.0000         1
       3,NOUN,ARG1     1.0000    0.0000    0.0000         1
  -1,NOUN,ARGM-MNR     0.0000    0.0000    0.0000         1
     1,NOUN,R-ARG0     1.0000    0.0000    0.0000         2
       -2,AUX,ARG1     0.0000    0.0000    0.0000         2
            5,NOUN     1.0000    0.0000    0.0000         4
      -5,VERB,ARG1     1.0000    0.0000    0.0000         1
   1,VERB,ARG1-DSP     1.0000    0.0000    0.0000         4
-1,VERB,C-ARG1-DSP     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-NEG     1.0000    0.0000    0.0000         1
      -4,VERB,ARG1     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-GOL     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-PRP     1.0000    0.0000    0.0000         2
       -3,ADJ,ARG0     1.0000    0.0000    0.0000         1
   -2,AUX,ARGM-CAU     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-CAU     0.3333    0.6667    0.4444         3
   -1,ADJ,ARGM-EXT     1.0000    0.0000    0.0000         1
   -1,ADJ,ARGM-CXN     0.6250    0.8333    0.7143         6
    2,AUX,ARGM-LOC     1.0000    0.0000    0.0000         1
    1,AUX,ARGM-LOC     0.0000    1.0000    0.0000         0
   -3,ADJ,ARGM-CXN     1.0000    0.0000    0.0000         1
   2,NOUN,ARGM-NEG     1.0000    0.0000    0.0000         1
   1,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
             3,ADV     1.0000    0.0000    0.0000         1
   1,NOUN,ARGM-GOL     0.0000    1.0000    0.0000         0
      -2,VERB,ARG3     1.0000    0.0000    0.0000         2
             2,DET     1.0000    0.0000    0.0000         2
   4,NOUN,ARGM-CAU     1.0000    0.0000    0.0000         1
  -2,VERB,ARGM-DIR     1.0000    0.0000    0.0000         1
            -3,ADV     1.0000    0.0000    0.0000         1
    1,AUX,ARGM-PRP     0.0000    1.0000    0.0000         0
-1,VERB,C-ARGM-LOC     1.0000    1.0000    1.0000         1
       1,VERB,ARGA     1.0000    0.0000    0.0000         2
  -3,VERB,ARGM-CAU     1.0000    0.0000    0.0000         1
  -3,VERB,ARGM-DIS     1.0000    0.0000    0.0000         1
            1,PART     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-PRP     1.0000    0.0000    0.0000         3
       1,VERB,ARG3     0.5000    0.5000    0.5000         2
   -1,AUX,ARGM-GOL     0.5000    1.0000    0.6667         2
    1,ADJ,ARGM-LVB     1.0000    0.0000    0.0000         1
  -2,NOUN,ARGM-MNR     1.0000    0.0000    0.0000         1
 1,VERB,R-ARGM-TMP     1.0000    1.0000    1.0000         1
             1,ADP     1.0000    0.0000    0.0000         4
     -1,AUX,C-ARG2     0.0000    0.0000    0.0000         1
    1,ADJ,ARGM-NEG     0.5000    1.0000    0.6667         1
   2,NOUN,ARGM-EXT     0.0000    1.0000    0.0000         0
    1,ADJ,ARGM-TMP     0.0000    1.0000    0.0000         0
   2,NOUN,ARGM-DIR     1.0000    0.0000    0.0000         1
       -2,ADJ,ARG0     1.0000    1.0000    1.0000         2
    -1,NOUN,C-ARG1     1.0000    0.0000    0.0000         1
       -2,AUX,ARG2     1.0000    0.0000    0.0000         1
   1,VERB,ARGM-DIR     1.0000    0.0000    0.0000         1
     -2,ADJ,C-ARG1     1.0000    0.0000    0.0000         1
     2,VERB,R-ARG1     0.0000    1.0000    0.0000         0
  -2,VERB,ARGM-PRR     1.0000    0.0000    0.0000         1
        3,AUX,ARG1     1.0000    1.0000    1.0000         1
    2,AUX,ARGM-MOD     1.0000    1.0000    1.0000         1
    2,AUX,ARGM-NEG     1.0000    0.0000    0.0000         1

          accuracy                         0.7877     25096
         macro avg     0.6860    0.3657    0.3180     25096
      weighted avg     0.7990    0.7877    0.7852     25096

2021-10-08 12:05:13,170 ----------------------------------------------------------------------------------------------------
